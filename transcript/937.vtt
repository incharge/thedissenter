WEBVTT

1
00:00:00.009 --> 00:00:02.829
Hello everybody. Welcome to a new episode of the

2
00:00:02.910 --> 00:00:05.719
Center. I'm your host as always Ricard Lob and

3
00:00:05.730 --> 00:00:09.189
to the MG by Doctor Oliver Oliver Tralee. He

4
00:00:09.199 --> 00:00:12.619
is a John and D Bery postdoctoral research fellow

5
00:00:12.630 --> 00:00:16.184
at the James Medicine program at Princeton in University.

6
00:00:16.204 --> 00:00:20.565
He studies epistemology with the focus on the epistemology

7
00:00:20.575 --> 00:00:23.555
of politics. And today we're talking about his book,

8
00:00:23.565 --> 00:00:28.485
Political Beliefs, uh Philosophical Introduction. So Oliver, welcome to

9
00:00:28.495 --> 00:00:30.174
the show. It's a pleasure to everyone.

10
00:00:30.594 --> 00:00:32.205
Thank you. It's a pleasure to be here. Thank

11
00:00:32.215 --> 00:00:33.115
you so much for having me.

12
00:00:33.740 --> 00:00:39.470
So tell us first, perhaps what is political epistemology

13
00:00:39.479 --> 00:00:43.209
and how did you get interested in that specific?

14
00:00:43.500 --> 00:00:48.680
Uh I mean, sub discipline of epistemology, I guess.

15
00:00:49.290 --> 00:00:51.979
Yeah. So um the, the second question is easier

16
00:00:51.990 --> 00:00:54.500
than the first one A as I was applying

17
00:00:54.509 --> 00:01:01.529
to, to phd programs. Uh IN 2017, it basically

18
00:01:01.540 --> 00:01:03.540
was in the early days of the, the Trump

19
00:01:03.549 --> 00:01:05.080
era, right, of the early days of the Trump

20
00:01:05.089 --> 00:01:08.220
presidency and a lot of a lot of public

21
00:01:08.230 --> 00:01:12.830
discussion, not discussion in academia but public discussion um

22
00:01:12.839 --> 00:01:17.050
about the Trump presidency uh and the Trump campaign.

23
00:01:17.080 --> 00:01:21.550
Uh AND American politics around then was about epistemology

24
00:01:21.559 --> 00:01:23.660
in a sense, right? So you had terms like

25
00:01:23.669 --> 00:01:28.470
fake news, alternative facts being thrown, thrown around. You

26
00:01:28.480 --> 00:01:32.510
had these discussions of um you know, information bubbles,

27
00:01:32.519 --> 00:01:36.910
information cascades. Um And you had other words, which

28
00:01:36.919 --> 00:01:39.379
to me felt a little epistemological even though nobody

29
00:01:39.389 --> 00:01:42.129
else had, had given an account of them uh

30
00:01:42.139 --> 00:01:46.400
in terms like normalization, um you had talked conspiracy

31
00:01:46.410 --> 00:01:49.730
theories, things like that. So political epistemology was very

32
00:01:49.739 --> 00:01:53.129
much in the air um in the early days

33
00:01:53.139 --> 00:01:56.199
and it, and it kind of related back to

34
00:01:56.209 --> 00:01:59.160
some things I'd studied as an undergraduate as an

35
00:01:59.169 --> 00:02:01.639
undergraduate. I was a classics major with an interest

36
00:02:01.650 --> 00:02:03.529
in politics. I ended up going to law school

37
00:02:03.540 --> 00:02:05.790
for a year after that and I worked on

38
00:02:05.800 --> 00:02:08.399
a political campaign in 2008 and things like that.

39
00:02:08.839 --> 00:02:13.419
Um And I had studied polarization in college. Uh

40
00:02:13.429 --> 00:02:16.660
And I was familiar with CASS Sunstein account of

41
00:02:16.669 --> 00:02:20.559
uh belief polarization and going to extremes and uh

42
00:02:20.570 --> 00:02:22.470
his account of the way the internet related to

43
00:02:22.479 --> 00:02:25.520
that by, by putting people in these bubbles. There

44
00:02:25.529 --> 00:02:29.490
was a great article by Ty Gin um in

45
00:02:29.500 --> 00:02:33.600
a eon called a, a public article about uh

46
00:02:33.610 --> 00:02:36.220
well, what he calls uh epistemic bubbles and echo

47
00:02:36.229 --> 00:02:39.380
chambers. Um THAT also there was also an academic

48
00:02:39.389 --> 00:02:42.570
article in epistemic um on the same topic that

49
00:02:42.580 --> 00:02:45.830
he wrote. Uh So I was basically seeing all

50
00:02:45.839 --> 00:02:49.880
this stuff um about the Trump presidency, but also

51
00:02:50.149 --> 00:02:52.789
I was having my own doubts about, you know,

52
00:02:52.800 --> 00:02:54.619
having been sort of a political progress in my

53
00:02:54.630 --> 00:02:58.279
whole life. I started having these questions about, you

54
00:02:58.289 --> 00:03:01.740
know, why do I have so little contact um,

55
00:03:01.750 --> 00:03:05.979
with this whole other group of people, uh, who,

56
00:03:05.990 --> 00:03:07.880
who are voting in ways that, you know, nobody,

57
00:03:07.889 --> 00:03:12.360
nobody who I'm around ever understands. Um, WHERE do

58
00:03:12.369 --> 00:03:14.380
their beliefs come from? Are their beliefs based on

59
00:03:14.389 --> 00:03:17.710
reasons, um, and talking to them improve my own

60
00:03:17.720 --> 00:03:22.029
beliefs. Uh This was also, uh you know, it's

61
00:03:22.039 --> 00:03:23.960
funny that these things happened at the same time,

62
00:03:23.970 --> 00:03:28.119
but the term campaign was happening around the same

63
00:03:28.130 --> 00:03:30.080
time as a lot of the uh the Open

64
00:03:30.089 --> 00:03:34.199
Science movement um and Heterodox Academy and efforts like

65
00:03:34.210 --> 00:03:38.360
that, that were about, on the one hand, um

66
00:03:38.820 --> 00:03:43.850
you know, eee even things like statistical practices or

67
00:03:43.860 --> 00:03:48.279
peer review in the Science sciences preprints, um avoiding

68
00:03:48.289 --> 00:03:52.820
p hacking, avoiding um fraud and science. Um But

69
00:03:52.830 --> 00:03:54.429
on the other hand about the, you know, the

70
00:03:54.440 --> 00:03:58.669
political skew of the Academy, um and where, where

71
00:03:58.679 --> 00:04:02.570
our supposedly expert beliefs come from, uh and things

72
00:04:02.580 --> 00:04:05.009
like that. So it was basically a moment where

73
00:04:05.550 --> 00:04:07.740
there was all this talk of political epistemology and

74
00:04:07.750 --> 00:04:09.889
I wasn't sure that it was being directed in,

75
00:04:09.899 --> 00:04:12.279
in the right way. Uh It felt to me

76
00:04:12.289 --> 00:04:15.229
like, maybe, maybe there was this sort of expert

77
00:04:15.240 --> 00:04:18.269
community that was actually the one that should be,

78
00:04:18.279 --> 00:04:19.709
you know, where, where we should be putting a

79
00:04:19.720 --> 00:04:23.760
lot of uh critical attention. Um So, OK, that's

80
00:04:23.769 --> 00:04:26.130
sort of my personal story. Your question, what is

81
00:04:26.140 --> 00:04:29.730
political epistemology? Um Well, so we can, we can

82
00:04:29.739 --> 00:04:33.570
start with, you know, traditional epistemology. We're, we're just

83
00:04:33.579 --> 00:04:37.429
thinking about, you know, how can we justify our

84
00:04:37.440 --> 00:04:39.950
beliefs, how can we possibly have knowledge? Um What

85
00:04:39.959 --> 00:04:41.549
do you know, what sources of evidence do we

86
00:04:41.559 --> 00:04:44.350
have? Um And are they good sources of evidence,

87
00:04:44.359 --> 00:04:48.130
things like, uh you know, mathematical intuition or, or

88
00:04:48.140 --> 00:04:52.500
perception? Um Those are the sorts of things that

89
00:04:52.510 --> 00:04:55.929
Descartes, for instance, might have been interested in um

90
00:04:56.910 --> 00:05:01.899
uh deductive versus inductive reasoning, you know, human skepticism

91
00:05:01.910 --> 00:05:07.589
about causation, things like that. Then social epistemology is

92
00:05:07.600 --> 00:05:11.109
concerned about a subset of those sources, the ones

93
00:05:11.119 --> 00:05:14.230
where the information comes from other people. Um So

94
00:05:14.239 --> 00:05:17.359
questions about how can we tell when somebody is

95
00:05:17.369 --> 00:05:19.359
an expert, what should we do when somebody disagrees

96
00:05:19.369 --> 00:05:24.119
with us? Um You know, what justifies you, you

97
00:05:24.130 --> 00:05:25.559
know, I was in Europe a couple of months

98
00:05:25.570 --> 00:05:28.269
ago, which I don't travel that much. Um And

99
00:05:28.279 --> 00:05:29.600
uh so it was a big deal for me.

100
00:05:30.239 --> 00:05:33.269
Uh You're on the street. I mean, among other

101
00:05:33.279 --> 00:05:35.709
things, I didn't realize everybody speaks English. So I

102
00:05:35.720 --> 00:05:37.089
can just go up to somebody and ask them

103
00:05:37.100 --> 00:05:39.440
in English even if it's, you know, you know,

104
00:05:39.940 --> 00:05:42.269
in Europe uh in a non English speaking country.

105
00:05:42.399 --> 00:05:43.910
So I go up and ask, you know, where's

106
00:05:43.920 --> 00:05:46.709
the museum? You know, where's the the modern art

107
00:05:46.720 --> 00:05:49.779
museum in this town. Um And they give me

108
00:05:49.790 --> 00:05:52.839
an answer, why do I trust that answer? What,

109
00:05:52.850 --> 00:05:56.480
like what licenses me in trusting that answer. Um

110
00:05:56.690 --> 00:05:58.489
So the nature of trust, the nature of other

111
00:05:58.500 --> 00:06:01.239
people's testimony. Uh So that's a source of my

112
00:06:01.250 --> 00:06:03.179
belief about where the museum is. That's a social

113
00:06:03.190 --> 00:06:05.549
source. So that then we get to social epistemology,

114
00:06:05.959 --> 00:06:09.829
then political epistemology um sort of means a few

115
00:06:09.839 --> 00:06:12.510
different things at once. Uh One of them is

116
00:06:12.519 --> 00:06:16.510
just uh the epistemology of political beliefs and how

117
00:06:16.519 --> 00:06:20.309
we answer political questions and come to our beliefs

118
00:06:20.320 --> 00:06:25.299
about political questions. Um BUT that's very broad, right?

119
00:06:25.309 --> 00:06:29.329
So like, for example, um if I've proven uh

120
00:06:29.339 --> 00:06:34.170
that, you know, one policy will result in, you

121
00:06:34.179 --> 00:06:39.880
know, a low number, yy, you know, say five

122
00:06:39.890 --> 00:06:43.390
injuries at my workplace and another policy will result

123
00:06:43.399 --> 00:06:46.859
in 5000 injuries that were at my workplace. Um

124
00:06:47.220 --> 00:06:49.399
Obviously, I need to do a little math to

125
00:06:49.410 --> 00:06:52.519
know that 5000 is greater than five. But math,

126
00:06:52.529 --> 00:06:55.100
you know, mathematical epistemology is not generally thought of

127
00:06:55.109 --> 00:06:56.489
as we just take that sort of stuff for

128
00:06:56.500 --> 00:06:58.899
granted once we're in the political realm, right? So

129
00:06:58.910 --> 00:07:01.690
the most characteristic part of political epistemology is the

130
00:07:01.700 --> 00:07:05.910
part that's sort of still social. Um So questions

131
00:07:05.920 --> 00:07:09.359
about political disagreement, what, what do we do when

132
00:07:09.369 --> 00:07:12.790
we disagree with others about politics? Um Questions about,

133
00:07:12.799 --> 00:07:15.730
should we trust people in our political party more

134
00:07:15.739 --> 00:07:18.429
than people in another political party. Are there experts

135
00:07:18.440 --> 00:07:22.899
in politics? Um So some of these, some of

136
00:07:22.910 --> 00:07:25.420
these questions come from the nature of politics that's

137
00:07:25.429 --> 00:07:28.450
involving groups, right? So that there are different groups

138
00:07:28.459 --> 00:07:32.230
in politics. And there's questions about OK, social epistemology.

139
00:07:32.239 --> 00:07:35.059
I'm thinking about how does my epistemology change based

140
00:07:35.070 --> 00:07:37.880
on the awareness that there are multiple people and

141
00:07:37.890 --> 00:07:40.250
political epistemology in one sense is how did my,

142
00:07:40.260 --> 00:07:42.410
how does my epistemology change based on the awareness

143
00:07:42.420 --> 00:07:45.279
that there are multiple groups? Uh But an another

144
00:07:45.290 --> 00:07:47.779
aspect of political epistemology is just that there are

145
00:07:47.790 --> 00:07:51.790
these phenomena that happened in politics. Um And we

146
00:07:51.799 --> 00:07:55.179
want to investigate kind of all the possible epistemological

147
00:07:55.190 --> 00:07:59.420
contours of them uh phenomena that could be types

148
00:07:59.429 --> 00:08:03.380
of beliefs or types of causes of beliefs. Um

149
00:08:04.089 --> 00:08:08.190
So, for example, belief polarization uh seems to be

150
00:08:08.200 --> 00:08:10.750
something that causes our beliefs to get into sets,

151
00:08:10.760 --> 00:08:14.040
you know, sort of party platforms and also seems

152
00:08:14.049 --> 00:08:16.079
to be something that pushes maybe people's beliefs away

153
00:08:16.089 --> 00:08:19.559
from each other. Um uh So it's what I

154
00:08:19.570 --> 00:08:22.660
call in the book sorting and extremism. On the

155
00:08:22.670 --> 00:08:24.739
other hand, the cons conspiracy theories seem to be

156
00:08:24.750 --> 00:08:27.869
types of beliefs. Um And so we can ask,

157
00:08:27.880 --> 00:08:30.410
is there anything general we can say about this

158
00:08:30.420 --> 00:08:34.000
type of belief? Um But though they both fall

159
00:08:34.010 --> 00:08:36.750
under political epistemology because they're, they're both related in

160
00:08:36.760 --> 00:08:38.770
some way to po politics. So it's, you know,

161
00:08:38.780 --> 00:08:42.380
it's not a well defined term. Um But it

162
00:08:42.390 --> 00:08:45.650
you know, it refers to the, the broad fact

163
00:08:45.659 --> 00:08:47.349
that, that there's a lot of interesting stuff going

164
00:08:47.359 --> 00:08:48.450
on here that a lot of people want to

165
00:08:48.460 --> 00:08:51.950
study basically. Um And, you know, just one other

166
00:08:51.960 --> 00:08:55.090
thing to say is that there's, there's a, you

167
00:08:55.099 --> 00:08:58.450
know, there's this other tradition of political epistemology um

168
00:08:58.460 --> 00:09:00.919
in Marxist thought, um and radical thought which has

169
00:09:00.929 --> 00:09:05.619
to do with the notion of ideology. Um And

170
00:09:05.630 --> 00:09:08.789
uh there's sort of questions about that because, you

171
00:09:08.799 --> 00:09:10.900
know, it's not clear that if you're a Marxist

172
00:09:10.909 --> 00:09:14.140
materialist, that you should think that the epistemology matters

173
00:09:14.150 --> 00:09:15.679
that much, you should probably just think that like

174
00:09:15.690 --> 00:09:19.030
the class relations and the material relations are, are

175
00:09:19.500 --> 00:09:22.380
um what, what causes everything. So there's questions about

176
00:09:22.390 --> 00:09:24.299
kind of the, the structure of the theory there.

177
00:09:24.650 --> 00:09:26.969
Um But so there's, you know, so yeah, there's

178
00:09:26.979 --> 00:09:28.909
a history of political epistemology and a lot of

179
00:09:28.919 --> 00:09:31.710
different political and philosophical traditions.

180
00:09:32.799 --> 00:09:37.840
But what actually counts as political because people might

181
00:09:37.849 --> 00:09:41.700
say that they have, for example, a political disagreement

182
00:09:41.710 --> 00:09:45.390
that they're having a political debate that when they're

183
00:09:45.400 --> 00:09:50.799
discussing a particular subject, they're talking politics there. And

184
00:09:50.809 --> 00:09:53.539
there are people that even claim and we can

185
00:09:53.549 --> 00:09:56.635
get a little bit more into that, that everything

186
00:09:56.645 --> 00:10:01.875
is actually political. So, but, but what does really

187
00:10:01.885 --> 00:10:04.875
count as something political? This is

188
00:10:04.885 --> 00:10:06.994
a question um This is the question that I

189
00:10:07.005 --> 00:10:09.125
start my book with and I, I've, I've talked

190
00:10:09.135 --> 00:10:10.395
about this with a lot of people and I

191
00:10:10.405 --> 00:10:12.775
haven't, I haven't been super satisfied by any of

192
00:10:12.784 --> 00:10:15.614
the answers. There are some things that seem obviously

193
00:10:15.625 --> 00:10:18.864
political. You know, if, if I, you know, if

194
00:10:18.875 --> 00:10:22.075
you go to Washington DC and you see people

195
00:10:22.710 --> 00:10:24.739
yelling at each other about what the tax rate

196
00:10:24.750 --> 00:10:27.820
should be and then people maneuver and there's sort

197
00:10:27.830 --> 00:10:30.869
of like, you know, some compromise or they're trading

198
00:10:30.880 --> 00:10:32.809
favors. If you vote on this bill this way,

199
00:10:32.820 --> 00:10:35.750
then I'll vote on that bill that way. Um

200
00:10:35.840 --> 00:10:37.640
Or if they, or if they go to war

201
00:10:37.650 --> 00:10:39.530
or try to win an election against each other

202
00:10:39.539 --> 00:10:41.109
or try to get a judge appointed or something

203
00:10:41.119 --> 00:10:45.299
like that, these things seem obviously political. Um So

204
00:10:45.309 --> 00:10:47.929
there are questions about what makes these things, um,

205
00:10:48.340 --> 00:10:51.520
political, one aspect is that there seems to be

206
00:10:51.530 --> 00:10:53.130
some kind of struggle in a lot of them,

207
00:10:53.140 --> 00:10:56.260
right? Um So there's some sort of dispute uh

208
00:10:56.270 --> 00:11:00.179
at the beginning and resolving that dispute or kind

209
00:11:00.190 --> 00:11:03.539
of, you know, failing to resolve that dispute. Um

210
00:11:03.549 --> 00:11:06.140
WHETHER you resolve it or not, uh That seems

211
00:11:06.150 --> 00:11:08.159
to be part of the politics of what's going

212
00:11:08.169 --> 00:11:11.369
on. Another aspect is that sort of the dispute

213
00:11:11.380 --> 00:11:15.969
is resolved by the goal of the dispute is

214
00:11:15.979 --> 00:11:19.770
to sort of, that's something up in society in

215
00:11:19.780 --> 00:11:22.960
a certain way, right? Um Whether it's the tax

216
00:11:22.969 --> 00:11:25.020
rate or the, you know, a question of whether

217
00:11:25.030 --> 00:11:27.010
we'll go to war or not. So these are

218
00:11:27.020 --> 00:11:28.780
what I call conflict and order in the book.

219
00:11:28.789 --> 00:11:31.690
So uh you start with conflict, you wanna set

220
00:11:31.700 --> 00:11:35.750
something in order. Um And uh then, then there's

221
00:11:35.760 --> 00:11:37.809
another notion which I've never been quite as clear

222
00:11:37.820 --> 00:11:41.590
on, um which is the notion of power. Uh

223
00:11:42.190 --> 00:11:44.830
So a lot of people say, and this has

224
00:11:44.840 --> 00:11:47.070
to be included just because, you know, this is

225
00:11:47.080 --> 00:11:49.280
a book that's academic facing, but also public facing.

226
00:11:49.289 --> 00:11:50.869
And in public, a lot of people will just

227
00:11:50.880 --> 00:11:54.750
say the phrase politics is about power. Um So

228
00:11:54.760 --> 00:11:55.869
I don't really know what any of the words

229
00:11:55.880 --> 00:11:59.299
in that sentence mean. Um You know, we're trying

230
00:11:59.309 --> 00:12:01.719
to define politics, trying to define power. The word

231
00:12:01.729 --> 00:12:04.599
about is like one of these insanely vague words

232
00:12:04.609 --> 00:12:08.000
that can mean completely different things in different contexts.

233
00:12:08.010 --> 00:12:12.960
Um uh But it seems like here, the power

234
00:12:12.969 --> 00:12:15.119
has to do with the, the setting in order.

235
00:12:15.130 --> 00:12:18.000
So it's about the power to determine how things

236
00:12:18.010 --> 00:12:21.460
will go. Um And uh it does seem like

237
00:12:21.469 --> 00:12:23.489
even if you're not setting something specific in order,

238
00:12:23.500 --> 00:12:26.359
if you're trying to gain power for some future

239
00:12:26.369 --> 00:12:28.270
goal of setting things in order to resolve a

240
00:12:28.280 --> 00:12:30.840
conflict or to win in a conflict, then then

241
00:12:30.849 --> 00:12:34.630
you're doing politics as well. Um The, unfortunately, it

242
00:12:34.640 --> 00:12:37.580
just seems that there's also, you know, we can

243
00:12:37.590 --> 00:12:39.789
come up with counter examples to, to all of

244
00:12:39.799 --> 00:12:42.929
these, you know, power conflict in order. There's, there's

245
00:12:42.940 --> 00:12:46.210
counterexamples where OK, you're using power, but it's not

246
00:12:46.219 --> 00:12:48.090
political power, there's a conflict, but it's not a

247
00:12:48.099 --> 00:12:51.010
political conflict. You made some arrangement, but it's not

248
00:12:51.020 --> 00:12:52.679
a political arrangement and you can even have all

249
00:12:52.690 --> 00:12:55.010
three at the same time. So it doesn't seem

250
00:12:55.020 --> 00:12:57.239
necessarily the case that there, there seems to be

251
00:12:57.250 --> 00:13:01.270
maybe some background notion of political um where we,

252
00:13:01.280 --> 00:13:02.789
we have an idea of what is a political

253
00:13:02.799 --> 00:13:04.989
conflict and a non-political conflict. We have an idea

254
00:13:05.000 --> 00:13:07.099
of what is political power and non-political power. We

255
00:13:07.109 --> 00:13:09.429
have an idea of what is a political arrangement

256
00:13:09.440 --> 00:13:13.900
and the non-political arrangement. Um So that's, you know,

257
00:13:13.909 --> 00:13:17.890
that whatever that background idea is, um that, that's

258
00:13:17.900 --> 00:13:18.979
sort of what we want to figure out and

259
00:13:18.989 --> 00:13:20.650
I don't think I made any progress on figuring

260
00:13:20.659 --> 00:13:23.599
that out. Um I hope somebody else does um

261
00:13:24.330 --> 00:13:27.929
regarding this question of whether everything is political, um

262
00:13:28.239 --> 00:13:29.580
you know, the first thing to say, which I

263
00:13:29.590 --> 00:13:31.190
don't think I, I even mentioned in the book,

264
00:13:31.200 --> 00:13:32.400
but one of the first things you see when

265
00:13:32.409 --> 00:13:35.590
people talk about this online is basically like, you

266
00:13:35.599 --> 00:13:39.489
know, any word that actually applies to everything, you

267
00:13:39.500 --> 00:13:43.369
know, ceases to be meaningful, right? Um You know,

268
00:13:43.390 --> 00:13:46.820
if, if you think extensionally about definitions, right? A

269
00:13:46.830 --> 00:13:48.570
definition is meant to pick some things out and

270
00:13:48.580 --> 00:13:51.169
exclude other things. That's how you communicate information, right?

271
00:13:51.690 --> 00:13:54.679
Um So if somebody says, well, I'm gonna get

272
00:13:54.690 --> 00:13:57.409
a little political here, you have like a rough

273
00:13:57.419 --> 00:13:59.390
idea of the sorts of topics they might discuss.

274
00:13:59.400 --> 00:14:01.179
Right. They're probably not going to talk about the

275
00:14:01.190 --> 00:14:04.489
weather unless they're talking about climate change. Right. Um,

276
00:14:04.919 --> 00:14:07.130
THEY might not talk about television unless they're talking

277
00:14:07.140 --> 00:14:09.070
about the political content of some show or if

278
00:14:09.080 --> 00:14:11.969
they're talking about, you know, what is the government

279
00:14:11.979 --> 00:14:14.309
body that regulates what's on television or something like

280
00:14:14.320 --> 00:14:16.799
that? That would be political. Um, BUT saying what

281
00:14:16.809 --> 00:14:18.969
happened on the soap opera yesterday, that doesn't seem

282
00:14:18.979 --> 00:14:22.650
political. Right. Um So basically, first of all, it

283
00:14:22.659 --> 00:14:24.219
seems like the word political has to, it has

284
00:14:24.229 --> 00:14:26.349
some information in it and if everything is political,

285
00:14:26.359 --> 00:14:30.890
then I can't really have any information. Um But

286
00:14:30.900 --> 00:14:34.289
people also, you know, people give arguments uh to

287
00:14:34.299 --> 00:14:36.869
the effect that everything is political. Um And I

288
00:14:36.880 --> 00:14:38.340
also, you know, I just in general think that

289
00:14:38.349 --> 00:14:40.950
they're not very good arguments. Um One of the

290
00:14:40.960 --> 00:14:43.820
arguments that I sort of make fun of the

291
00:14:43.830 --> 00:14:48.109
most is that, you know, everything has some, you

292
00:14:48.119 --> 00:14:51.929
know, everything that that exists now is the way

293
00:14:51.940 --> 00:14:53.880
that it is because of the past, right? Because

294
00:14:53.890 --> 00:14:57.840
of history and that history involves political history, right?

295
00:14:58.309 --> 00:15:01.760
Um uh But at the same time, you know,

296
00:15:02.049 --> 00:15:04.150
the history also involves a lot of other things

297
00:15:04.159 --> 00:15:06.969
and generally we don't say like, you know, um

298
00:15:08.010 --> 00:15:11.010
that, that, that something's history makes it what it

299
00:15:11.020 --> 00:15:16.239
is, right? Um If, uh you know, I'm sitting

300
00:15:16.250 --> 00:15:18.260
in a certain kind of chair, maybe the person

301
00:15:18.270 --> 00:15:21.549
got inspired to make this chair because uh he

302
00:15:21.559 --> 00:15:24.500
saw a sculpture, but that doesn't make the chair

303
00:15:24.510 --> 00:15:26.239
a sculpture, right? That's just sort of like a,

304
00:15:26.250 --> 00:15:28.000
a random fact about the history of the chair,

305
00:15:28.010 --> 00:15:30.369
right? So you have these random facts about the

306
00:15:30.380 --> 00:15:34.419
histories of objects and situations and people um that

307
00:15:34.429 --> 00:15:36.690
can involve political facts, but that doesn't, you know,

308
00:15:36.700 --> 00:15:37.890
this is just not the way that we use

309
00:15:37.900 --> 00:15:41.119
words, right? Um So I've been, I've always been

310
00:15:41.130 --> 00:15:43.489
very confused by the notion that everything is political.

311
00:15:43.859 --> 00:15:47.460
Um I have some suspicions about what people's motivations

312
00:15:47.469 --> 00:15:50.739
are in saying that. Um, BUT one way in

313
00:15:50.750 --> 00:15:53.760
which I do think it's important to have a

314
00:15:53.770 --> 00:15:58.669
somewhat expanded concept of the political uh is, you

315
00:15:58.679 --> 00:16:00.200
know, just comes from the sorts of things I

316
00:16:00.210 --> 00:16:02.929
argue about and in, in my public work and,

317
00:16:02.940 --> 00:16:06.039
and with a lot of other people. Um So

318
00:16:06.049 --> 00:16:08.039
I don't wanna say that the political is just

319
00:16:08.049 --> 00:16:11.429
like passing laws going to war, things like that,

320
00:16:11.440 --> 00:16:16.250
right? Um Because there are these chains of reasoning

321
00:16:17.409 --> 00:16:19.960
that justify or at least when it comes to

322
00:16:19.969 --> 00:16:22.119
political belief, right? There are these changes of reasoning

323
00:16:22.130 --> 00:16:24.400
that justify those actions going to war, passing a

324
00:16:24.409 --> 00:16:29.549
law. Um And it seems like our disputes about

325
00:16:29.559 --> 00:16:32.669
items of those changes of reasoning can themselves be

326
00:16:32.679 --> 00:16:36.700
political and the the beliefs we have that go

327
00:16:36.710 --> 00:16:38.960
into those change of reasoning can, can themselves be

328
00:16:38.969 --> 00:16:43.479
political. So if I believe, you know, somebody says

329
00:16:43.489 --> 00:16:47.280
we should go to war because uh this, this

330
00:16:47.289 --> 00:16:52.609
dictator is oppressing his own people. Um uh So,

331
00:16:53.590 --> 00:16:56.210
well, my, you know, my belief about whether the

332
00:16:56.219 --> 00:16:58.809
dictator is oppressing his own people, that seems like

333
00:16:58.820 --> 00:17:02.900
a political belief um in part because it affects

334
00:17:03.210 --> 00:17:05.250
political decision about whether or not to go to

335
00:17:05.260 --> 00:17:07.170
war. Right? So in the book, I think of

336
00:17:07.180 --> 00:17:10.410
political beliefs as being the things that affect political

337
00:17:10.420 --> 00:17:13.348
decisions, the things that, that can, that are involved

338
00:17:13.358 --> 00:17:16.689
in public disputes that, that, that relate to political

339
00:17:16.699 --> 00:17:20.108
actions. Um Now you ask, what is a political

340
00:17:20.118 --> 00:17:23.098
action? I'm gonna table that, right? I'll let somebody

341
00:17:23.108 --> 00:17:23.839
else figure that out.

342
00:17:25.260 --> 00:17:28.098
OK. So, but, but since we're talking here about

343
00:17:28.108 --> 00:17:32.839
political beliefs, what are political beliefs really about? I

344
00:17:32.849 --> 00:17:36.689
mean, there's a proposal that some people have that

345
00:17:37.109 --> 00:17:40.489
what they, they are really about what we ought

346
00:17:40.500 --> 00:17:43.979
to do and if that's the case, then they

347
00:17:43.989 --> 00:17:48.530
are mostly moral beliefs. Uh What do you think

348
00:17:48.540 --> 00:17:52.000
about that? And if you disagree, uh I mean,

349
00:17:52.010 --> 00:17:55.270
what's your take on what the political belief is?

350
00:17:55.719 --> 00:17:56.209
Yeah.

351
00:17:56.219 --> 00:17:58.560
So, yeah, so like what I, like I was

352
00:17:58.569 --> 00:18:02.609
saying, I think political beliefs um are belief that

353
00:18:03.630 --> 00:18:06.500
are involved in these chains of reasoning. At least

354
00:18:06.510 --> 00:18:07.930
the theory I give in the book, I'm sure

355
00:18:07.939 --> 00:18:09.989
somebody will prove it wrong. But the theory that

356
00:18:10.000 --> 00:18:13.250
I give in the book is political beliefs. Well,

357
00:18:13.260 --> 00:18:16.750
there's two types I mentioned before how politics, a

358
00:18:16.760 --> 00:18:18.920
lot of politics is about being in groups, right?

359
00:18:19.329 --> 00:18:22.189
So one type of political belief is a belief

360
00:18:22.199 --> 00:18:24.060
that sort of says I'm in this group, you're

361
00:18:24.069 --> 00:18:27.739
in that group. Um And one issue in American

362
00:18:27.750 --> 00:18:30.020
politics, at least is that we seem to have

363
00:18:30.030 --> 00:18:32.439
a lot of beliefs like this right now. Um

364
00:18:32.449 --> 00:18:36.099
This is what Robert Tali calls political saturation. Um

365
00:18:36.479 --> 00:18:39.489
So non Americans are often surprised when I explain

366
00:18:39.500 --> 00:18:44.680
this. But um so like Republicans tend to drink

367
00:18:44.689 --> 00:18:47.729
coffee from one coffee company, and Democrats tend to

368
00:18:47.739 --> 00:18:50.359
drink coffee from another coffee company. So you might

369
00:18:50.369 --> 00:18:52.750
say our beliefs about which coffee is better or

370
00:18:52.760 --> 00:18:56.040
political beliefs because they, they sort of, you know,

371
00:18:56.050 --> 00:18:58.969
they, they separate one group for another. Um I

372
00:18:58.979 --> 00:19:00.969
don't know, I don't know. That's, again, that's something,

373
00:19:00.979 --> 00:19:03.109
you know, I'll, I'll let other people decide. I,

374
00:19:03.119 --> 00:19:04.630
I more want to bridge the issue on that

375
00:19:04.640 --> 00:19:07.250
kind of topic. The other kind, which is maybe

376
00:19:07.260 --> 00:19:11.449
a little bit more central. Uh IS that when

377
00:19:11.459 --> 00:19:13.290
we have chains of reasoning about what to do

378
00:19:13.300 --> 00:19:16.270
in politics, what political actions to take? It seems

379
00:19:16.280 --> 00:19:18.699
to me that whenever items in those chains are

380
00:19:18.709 --> 00:19:22.729
in dispute or in conflict, um our beliefs about

381
00:19:22.739 --> 00:19:26.719
them are political. Um So of course, it's a

382
00:19:26.729 --> 00:19:28.609
poli a political belief whether or not we should

383
00:19:28.619 --> 00:19:33.530
raise the tax rate. Um But I, you know,

384
00:19:33.540 --> 00:19:37.430
one item in that chain of belief might be

385
00:19:37.920 --> 00:19:47.160
uh does raising the tax rate. I disincentivize people

386
00:19:47.170 --> 00:19:50.109
from starting businesses for example, right? Some economists might

387
00:19:50.119 --> 00:19:53.000
think, yes, some economists might think. No, that's an,

388
00:19:53.010 --> 00:19:56.410
that's a question in economics, but my belief about

389
00:19:56.420 --> 00:19:59.000
it, I'm calling a political belief because the dispute

390
00:19:59.010 --> 00:20:02.140
about it could be a dispute that, you know,

391
00:20:02.150 --> 00:20:04.680
the resolution of which would resolve the question about

392
00:20:04.689 --> 00:20:09.020
the political action if that makes sense. Um So

393
00:20:09.030 --> 00:20:12.479
basically, whenever anything in one of these kind of

394
00:20:12.550 --> 00:20:15.530
publicly controversial change of reasoning is in dispute, I'm

395
00:20:15.540 --> 00:20:17.930
gonna call it a political dispute and I'm gonna

396
00:20:17.939 --> 00:20:20.449
call the beliefs people have about it, political beliefs,

397
00:20:20.459 --> 00:20:23.920
that's my approach at the moment must be I

398
00:20:23.930 --> 00:20:24.849
end up with in the book,

399
00:20:25.319 --> 00:20:29.939
right? But when there's a political disagreement or political

400
00:20:29.949 --> 00:20:35.099
disputes, where does the disagreement stem from? Is it

401
00:20:35.109 --> 00:20:40.380
uh because people disagree over facts or values because

402
00:20:40.390 --> 00:20:45.275
they just use words differently because they struggle over

403
00:20:45.285 --> 00:20:49.814
particular kinds of issues like finite resources or something

404
00:20:49.824 --> 00:20:53.114
like that? I mean, and is it usually just

405
00:20:53.344 --> 00:20:56.015
one single issue? I mean, would there be a,

406
00:20:56.155 --> 00:21:01.515
an overarching theory of political conflict here or not?

407
00:21:01.755 --> 00:21:06.469
Yeah. So I'm a, unfortunately, I'm sort of boring

408
00:21:06.479 --> 00:21:08.790
about this issue. Um I call, I call myself

409
00:21:08.800 --> 00:21:10.930
boring whenever I don't have like a big theory

410
00:21:10.939 --> 00:21:13.030
and I say there's like different types, you know,

411
00:21:13.579 --> 00:21:17.300
so there are people who have big theories about

412
00:21:17.310 --> 00:21:20.670
this. So, um in particular, you have these political

413
00:21:20.680 --> 00:21:23.900
realists, some of whom seem to say, well, there's

414
00:21:23.910 --> 00:21:26.900
all, all the apparent disagreement is kind of like

415
00:21:26.910 --> 00:21:31.739
papering over the, like, hobby and struggle for, for

416
00:21:31.750 --> 00:21:35.199
resources. Right? Just I want something you want the

417
00:21:35.209 --> 00:21:38.270
same thing. Maybe we come up with these pretend

418
00:21:38.280 --> 00:21:40.790
reasons why I should get it and you should

419
00:21:40.800 --> 00:21:43.260
not get it. But those are all the basic

420
00:21:43.270 --> 00:21:45.170
fact is that we want the same thing. Right?

421
00:21:45.180 --> 00:21:47.979
And, and the reasons are just us trying to

422
00:21:47.989 --> 00:21:53.680
justify and us trying to, to rationalize getting what

423
00:21:53.689 --> 00:21:57.540
we want. That's not what I believe. Um I

424
00:21:57.550 --> 00:22:00.300
believe the these four sources of conflict you talk

425
00:22:00.310 --> 00:22:04.819
about are all present in politics. Um And I

426
00:22:04.829 --> 00:22:08.939
think it's really important not to. Um THERE'S an

427
00:22:08.949 --> 00:22:11.839
old blog post by a blogger called Scott Alexander

428
00:22:11.849 --> 00:22:14.040
where he talks about, he gives sort of like

429
00:22:14.050 --> 00:22:16.040
a sim a simpler and like a little bit

430
00:22:16.050 --> 00:22:20.510
less clear um version of this question of where

431
00:22:20.520 --> 00:22:24.589
does political conflict come from. Um And yeah, so

432
00:22:24.599 --> 00:22:26.640
he calls one type of theory, mistake theory and

433
00:22:26.650 --> 00:22:30.329
another type of theory, conflict theory um where the

434
00:22:30.339 --> 00:22:33.359
mistake theory is. Yeah, but there's political disagreements because,

435
00:22:33.369 --> 00:22:35.839
you know, and political conflict because some people make

436
00:22:35.849 --> 00:22:38.160
mistakes, right? Just like you might, if you're, you

437
00:22:38.170 --> 00:22:39.729
know, if you're doing the math, you come up

438
00:22:39.739 --> 00:22:42.109
with the wrong answer. So we have different answers

439
00:22:42.119 --> 00:22:44.369
and then other people think, oh yeah, we're, we're,

440
00:22:44.380 --> 00:22:46.910
we have different values or we're arguing over resources.

441
00:22:46.920 --> 00:22:49.170
Um Well, I just think sometimes each of those

442
00:22:49.180 --> 00:22:51.089
things can happen you know, it's just a complicated

443
00:22:51.099 --> 00:22:53.530
world, different sorts of things happen at different times.

444
00:22:53.900 --> 00:22:55.369
Um So I don't, I, I don't, I'm not

445
00:22:55.380 --> 00:22:58.500
really, I'm not motivated to say any of these

446
00:22:58.510 --> 00:23:00.479
things as sort of like the central or univocal

447
00:23:00.489 --> 00:23:03.739
cause, um, or unique cause I guess is the

448
00:23:03.750 --> 00:23:07.760
right word of a political conflict. I certainly think

449
00:23:07.770 --> 00:23:12.420
that, you know, drilling down one thing I talk

450
00:23:12.430 --> 00:23:14.339
about when, when I talk with, uh, you know,

451
00:23:14.349 --> 00:23:18.459
state college students about, I how to argue with

452
00:23:18.469 --> 00:23:20.589
people about politics and how to make those conversations

453
00:23:20.599 --> 00:23:25.329
productive. Um, IS sort of like, yeah, you wanna

454
00:23:25.339 --> 00:23:27.349
drill down and get at what is the root

455
00:23:27.359 --> 00:23:30.239
of the, the conflict, right? What is really driving

456
00:23:30.250 --> 00:23:33.349
this conflict? Um And sometimes it can be different

457
00:23:33.359 --> 00:23:37.719
values, right? Like, you know, why does one person

458
00:23:37.739 --> 00:23:40.250
support gun rights and the other one doesn't, well,

459
00:23:40.810 --> 00:23:44.109
maybe the first person just thinks self reliance is

460
00:23:44.119 --> 00:23:45.780
just so important. People should be able to be

461
00:23:45.790 --> 00:23:49.510
self reliant even if, even if there's a, it

462
00:23:49.520 --> 00:23:52.849
raises the chance that they accidentally shoot themselves, right?

463
00:23:53.130 --> 00:23:55.300
It's the price we pay, right. Safety is not

464
00:23:55.310 --> 00:23:57.739
as important as self reliance. Other people say no

465
00:23:57.750 --> 00:23:59.869
safety, safety is like one of my most important

466
00:23:59.880 --> 00:24:02.530
values. I don't want people to be injuring themselves

467
00:24:02.540 --> 00:24:06.160
with guns. Right. So that seems like, um, you

468
00:24:06.170 --> 00:24:07.989
know, and that's a pretty tame way maybe of

469
00:24:08.000 --> 00:24:10.180
putting that disagreement, but it seems that we can

470
00:24:10.189 --> 00:24:13.579
have those disagreements where there are these principles or

471
00:24:13.589 --> 00:24:18.180
values and people just weigh them differently. Um, AND,

472
00:24:18.199 --> 00:24:20.310
uh, you know, some of the psychological theories of

473
00:24:20.319 --> 00:24:22.839
political beliefs seem to be about that seem to

474
00:24:22.849 --> 00:24:25.310
be about how do we, how do we weigh

475
00:24:25.319 --> 00:24:27.060
these values? And are there different types of people

476
00:24:27.069 --> 00:24:29.589
who weigh values in different ways? Um, ON the

477
00:24:29.599 --> 00:24:32.579
other hand, you know, some people could also just

478
00:24:32.589 --> 00:24:35.819
say like, well, I looked at the statistics uh

479
00:24:36.390 --> 00:24:39.459
and there just are so many, you know, self

480
00:24:39.469 --> 00:24:43.290
inflicted injuries from guns that even if you only

481
00:24:43.300 --> 00:24:44.949
weigh safety a little bit, you should be against

482
00:24:44.959 --> 00:24:47.780
them or you might say, well, actually there are

483
00:24:47.790 --> 00:24:50.280
basically none. So even if you weigh safety a

484
00:24:50.290 --> 00:24:53.219
lot, you shouldn't, it shouldn't worry you that much,

485
00:24:53.229 --> 00:24:55.800
right? So the empirical question of like how many

486
00:24:55.810 --> 00:25:00.479
injuries actually are there, um if say different people

487
00:25:00.489 --> 00:25:03.800
are studying this issue with different methodologies or if

488
00:25:03.810 --> 00:25:06.079
the data is unclear or fuzzy or something like

489
00:25:06.089 --> 00:25:08.660
that, um which is the case in studying so

490
00:25:08.670 --> 00:25:11.569
many political issues. Um It's just very hard to

491
00:25:11.579 --> 00:25:15.069
get the the the right answers. And um you

492
00:25:15.079 --> 00:25:17.030
know, especially for somebody like me with a kind

493
00:25:17.040 --> 00:25:21.150
of broad skepticism about uh kind of like the

494
00:25:21.160 --> 00:25:25.790
current knowledge production, uh industry and expertise then yeah,

495
00:25:25.800 --> 00:25:27.900
you can have a disagreement which is over empirical

496
00:25:27.910 --> 00:25:31.030
facts and that can generate a political disagreement, right?

497
00:25:31.729 --> 00:25:32.869
And then you can have, you know, you can

498
00:25:32.880 --> 00:25:35.349
have these verbal disagreements. Uh, WELL, you know, sometimes

499
00:25:35.359 --> 00:25:36.910
you think you're disagreeing with somebody and it just

500
00:25:36.920 --> 00:25:41.130
turns out, you know, uh, you know, if somebody

501
00:25:41.140 --> 00:25:43.939
says Trump is a fascist and I say, I

502
00:25:43.949 --> 00:25:45.780
don't think Trump is a fascist and they think,

503
00:25:45.790 --> 00:25:51.119
oh, this guy loves Trump. Um, AND, uh, then

504
00:25:51.130 --> 00:25:52.869
we start arguing and then it comes out of

505
00:25:52.880 --> 00:25:54.310
the argument. That's just like, oh, no, we just

506
00:25:54.319 --> 00:25:56.469
think about fascism, you know, like I'm just using

507
00:25:56.479 --> 00:25:59.439
this term, you know, in a slightly different way

508
00:25:59.449 --> 00:26:01.459
than you are. And, you know, we're, we're both

509
00:26:01.469 --> 00:26:03.300
on the same side of the politics of it.

510
00:26:03.729 --> 00:26:06.140
Um So we're really disagreeing about the word fascism

511
00:26:06.150 --> 00:26:09.000
rather than about Trump or about politics. Now, it

512
00:26:09.010 --> 00:26:11.979
could still be that, you know, whether a verbal

513
00:26:11.989 --> 00:26:14.300
dispute, verbal disputes can still be important in various

514
00:26:14.310 --> 00:26:17.729
ways. Um But I feel like it's important to

515
00:26:17.739 --> 00:26:19.930
know when, when, when that's what you're arguing about

516
00:26:19.939 --> 00:26:21.849
rather than arguing about the world or arguing about

517
00:26:21.859 --> 00:26:25.150
what's important or valuable. Um And then for this

518
00:26:25.160 --> 00:26:27.680
last type of conflict, this sort of raw conflict,

519
00:26:27.689 --> 00:26:29.689
I think that is a type of conflict that

520
00:26:29.699 --> 00:26:32.609
is really important to, you know, take in, in

521
00:26:32.619 --> 00:26:34.709
political theory. And this is an area where I

522
00:26:34.719 --> 00:26:36.439
don't know much, you know, like, I haven't read,

523
00:26:37.300 --> 00:26:40.479
you know, a lot of like Hobbes and Rousseau

524
00:26:40.489 --> 00:26:42.680
and people like that are not, I'm reading a

525
00:26:42.689 --> 00:26:44.920
lot of contemporary epistemology books of the time, right.

526
00:26:45.209 --> 00:26:46.609
But I know that in Hobbs and Rousseau, there

527
00:26:46.619 --> 00:26:50.770
are these questions of, you know, does the world

528
00:26:50.780 --> 00:26:53.709
start with conflict or not? You know, is there

529
00:26:53.719 --> 00:26:57.949
if you go back before political institutions, to what

530
00:26:57.959 --> 00:27:00.010
extent do we have conflict? Maybe, maybe it's just

531
00:27:00.020 --> 00:27:03.270
a state of peer conflict, right? Um, WITHOUT anything

532
00:27:03.280 --> 00:27:05.630
to resolve that conflict, without the politics to resolve

533
00:27:05.640 --> 00:27:11.770
that conflict. And so these peer conflicts in our

534
00:27:11.780 --> 00:27:15.140
sort of modern society where we're always arguing about

535
00:27:15.150 --> 00:27:18.739
politics and giving reasons and things like that, the

536
00:27:18.750 --> 00:27:22.410
peer conflict might start to dissipate in our, in

537
00:27:22.420 --> 00:27:25.770
our thinking. Um But it also may be that

538
00:27:25.780 --> 00:27:29.780
this is sort of like, you know, pretty fundamental

539
00:27:29.790 --> 00:27:31.569
to why we even have politics to begin with.

540
00:27:31.579 --> 00:27:34.550
Um So I think, I think it's important to

541
00:27:34.560 --> 00:27:38.040
keep the possibility of just pure conflict over resources,

542
00:27:38.050 --> 00:27:41.099
you know, pure struggle. Um Me wanting one thing

543
00:27:41.109 --> 00:27:42.709
and you wanting another, it's really important to keep

544
00:27:42.719 --> 00:27:45.489
that in mind. Um uh It's, it's like a

545
00:27:45.500 --> 00:27:48.550
fact of it's an in inimitable fact of human

546
00:27:48.560 --> 00:27:48.890
life

547
00:27:50.229 --> 00:27:53.069
and another very important thing here for us to

548
00:27:53.079 --> 00:27:55.739
try to understand these or, or at least that

549
00:27:55.750 --> 00:28:00.069
people try to explain is where political beliefs come

550
00:28:00.079 --> 00:28:03.930
from, where they stem from. There are different theories

551
00:28:03.939 --> 00:28:06.400
of political beliefs out there and you go, we

552
00:28:06.469 --> 00:28:09.859
list through the main ones in your book. So,

553
00:28:10.060 --> 00:28:12.900
uh I would like to ask you about each

554
00:28:12.910 --> 00:28:16.150
particular one of them to tell us about what

555
00:28:16.160 --> 00:28:19.859
you think are their strengths and limitations. So to

556
00:28:19.869 --> 00:28:24.050
start with the theories based on uh type or

557
00:28:24.060 --> 00:28:28.839
personality we have, for example, moral foundations, a theory

558
00:28:28.849 --> 00:28:32.670
by Frank and he uh theories based on particular

559
00:28:32.680 --> 00:28:36.819
personality traits, like authoritarian is more, for example, people

560
00:28:36.829 --> 00:28:40.270
that do work based on the big five personality

561
00:28:40.280 --> 00:28:44.849
traits, sometimes talk about openness to experience conscientiousness and

562
00:28:44.859 --> 00:28:47.530
other kinds of personality traits. So what do you

563
00:28:47.540 --> 00:28:49.849
make of those types of theories?

564
00:28:50.380 --> 00:28:53.359
Yeah, I think in general, I think there's two

565
00:28:53.369 --> 00:28:54.969
things for me to say one is sort of

566
00:28:54.979 --> 00:28:57.699
like my background intuition just based on my experiences

567
00:28:57.709 --> 00:28:59.829
in the world. And the other is sort of

568
00:28:59.839 --> 00:29:03.949
my evaluation of the ability of these theories. Once

569
00:29:03.959 --> 00:29:07.099
I looked at them sort of academically to explain

570
00:29:07.109 --> 00:29:09.900
political beliefs. So my experience of the world is

571
00:29:09.910 --> 00:29:14.290
really that you have in all different political groups,

572
00:29:14.300 --> 00:29:18.390
you have different types of people. Um AND in

573
00:29:18.400 --> 00:29:21.810
each political group, you have leaders and followers in

574
00:29:21.819 --> 00:29:25.939
each political group, you have moralists and pragmatists and

575
00:29:25.949 --> 00:29:29.040
idealists, you know, ideologies need to be able to

576
00:29:29.050 --> 00:29:30.640
bring in all sorts of people. You have true

577
00:29:30.650 --> 00:29:35.670
believers and grifters, you know, um take, you know,

578
00:29:35.680 --> 00:29:38.790
online, I participate in this sort of like social

579
00:29:38.800 --> 00:29:42.270
justice and antisocial justice, you know, wars, culture, wars,

580
00:29:42.280 --> 00:29:45.040
right? And on, on each side, you have people

581
00:29:45.520 --> 00:29:47.640
who really believe what they're saying. And you also

582
00:29:47.650 --> 00:29:49.750
have people who are you know, trying to sell

583
00:29:49.760 --> 00:29:50.989
something, you know, who are kind of like trying

584
00:29:51.000 --> 00:29:53.130
to make a buck by becoming like an influencer

585
00:29:53.140 --> 00:29:57.170
or something like that. Right? Um So it's my

586
00:29:57.180 --> 00:30:00.829
impression is just that politics always involves at least

587
00:30:00.839 --> 00:30:04.369
in terms of political actors, there's all different types

588
00:30:04.380 --> 00:30:07.349
of people and each group will have, you know,

589
00:30:08.479 --> 00:30:11.420
different types of people in different ways. The, the,

590
00:30:11.430 --> 00:30:13.140
the other thing to say is sort of like

591
00:30:13.150 --> 00:30:16.369
examining these theories, you, you start to have questions

592
00:30:16.380 --> 00:30:19.199
about what the measures of personality are and just

593
00:30:19.209 --> 00:30:23.020
how correlated they are with politics. So for example,

594
00:30:24.130 --> 00:30:26.969
looking at the, the Publications and Moral Foundations Theory,

595
00:30:26.979 --> 00:30:28.880
which has been pretty influential and in my circles

596
00:30:28.890 --> 00:30:31.569
have been pretty influential, right? Because like I said,

597
00:30:31.579 --> 00:30:35.479
at the beginning, my circles are, these are, or

598
00:30:35.500 --> 00:30:37.109
at least where I started out in my public

599
00:30:37.119 --> 00:30:40.599
writing and thinking about these issues, um were sort

600
00:30:40.609 --> 00:30:42.650
of circles with these liberals who were sort of

601
00:30:42.660 --> 00:30:45.199
like, well, the we don't understand conservatives and maybe

602
00:30:45.209 --> 00:30:48.920
if we try to understand conservatives better, um we'll

603
00:30:48.930 --> 00:30:51.920
understand politics better, we'll understand American politics better, we'll

604
00:30:51.930 --> 00:30:55.680
understand why people have reasons for their beliefs. Um

605
00:30:55.699 --> 00:30:57.329
AND we'll be able to improve our own beliefs,

606
00:30:57.339 --> 00:31:00.540
right? That's the dream in a sense, the epistemological

607
00:31:00.550 --> 00:31:04.569
dream, right? Improving our beliefs now. So the idea

608
00:31:04.579 --> 00:31:06.500
of one of the motive, you know, one of

609
00:31:06.510 --> 00:31:09.849
the attractions of moral foundations theory was that it

610
00:31:09.859 --> 00:31:12.239
gave us a, a clean way of saying what

611
00:31:12.250 --> 00:31:15.589
are we not appreciating about conservatives? Right. So we

612
00:31:15.599 --> 00:31:19.680
weren't appreciating that there were, there were these, uh

613
00:31:20.410 --> 00:31:22.420
so just to give an overview of moral foundations

614
00:31:22.430 --> 00:31:25.430
theory, moral foundations theory is, is the idea that

615
00:31:25.439 --> 00:31:29.939
there are these um that there, there's a set

616
00:31:29.949 --> 00:31:35.030
of kind of basic moral principles or ways of

617
00:31:35.040 --> 00:31:39.250
looking at the world or perspectives. Um A set

618
00:31:39.260 --> 00:31:42.260
of five or six of these, uh some of

619
00:31:42.270 --> 00:31:47.560
which are based individualistically and some of which are

620
00:31:47.599 --> 00:31:52.550
not solely about individuals. And the claim of moral

621
00:31:52.560 --> 00:31:56.939
foundations theory is that liberals have liberals see the

622
00:31:56.949 --> 00:32:02.000
world morally in terms of these individualistic foundations, but

623
00:32:02.010 --> 00:32:05.380
not the other ones. Whereas conservatives have both the

624
00:32:05.390 --> 00:32:10.430
individualistic and the sort of group based or purity

625
00:32:10.439 --> 00:32:14.859
based or loyalty based foundations that aren't about individuals

626
00:32:14.869 --> 00:32:18.660
per se. Um You know, individualistic ideas are sort

627
00:32:18.670 --> 00:32:23.900
of like safety care, liberty, um respect dignity, all

628
00:32:23.910 --> 00:32:26.459
things that you could say, I respect a person,

629
00:32:26.469 --> 00:32:28.939
a person should be safe, you know, the human

630
00:32:28.949 --> 00:32:32.689
dignity, things like that. Um And these, these other,

631
00:32:32.910 --> 00:32:36.900
these other values are more about hierarchy and community

632
00:32:37.400 --> 00:32:39.699
and the sanctity of symbols and things like that.

633
00:32:39.709 --> 00:32:41.439
And the idea is that those are the conservative

634
00:32:41.449 --> 00:32:45.920
ones. So the notion was if we liberals start

635
00:32:45.930 --> 00:32:48.619
thinking in terms of these other things, then, you

636
00:32:48.630 --> 00:32:50.829
know, maybe we'll appreciate where conservatives are coming from,

637
00:32:50.839 --> 00:32:54.660
at least, right? But it turns out if you

638
00:32:54.670 --> 00:32:56.170
look at the, you know, if you look at

639
00:32:56.180 --> 00:32:59.449
the actual data in the papers on moral foundations

640
00:32:59.459 --> 00:33:05.739
theory, the sort of correlations of these so called

641
00:33:05.750 --> 00:33:09.469
foundations showing up in the research. It's not like,

642
00:33:10.010 --> 00:33:11.780
you know, it's not a chart that is like

643
00:33:11.790 --> 00:33:14.439
this, this incredibly big line, right? It's actually a

644
00:33:14.449 --> 00:33:18.500
very smooth, you know, is a very smooth and

645
00:33:18.510 --> 00:33:22.130
gentle change from becoming liberal to becoming conservative, right?

646
00:33:22.459 --> 00:33:28.239
Um And it doesn't make it scientifically uninteresting, but

647
00:33:28.250 --> 00:33:30.900
it makes it a little less clear how we

648
00:33:30.910 --> 00:33:33.660
can turn it into a theory of politics. Um

649
00:33:33.670 --> 00:33:39.189
And a political belief, another issue is that all

650
00:33:39.199 --> 00:33:45.079
of these foundations and kind of attached to different

651
00:33:45.089 --> 00:33:48.780
things. And it started to seem to me and

652
00:33:48.790 --> 00:33:50.439
I don't, I don't think I really explained this

653
00:33:50.449 --> 00:33:52.280
in the book as I was going through this

654
00:33:52.290 --> 00:33:53.939
theory, but it started to seem to me that

655
00:33:55.150 --> 00:33:57.290
or, you know, what political group we're a part

656
00:33:57.300 --> 00:34:01.800
of depends much more on what these notions attached

657
00:34:01.810 --> 00:34:03.650
to rather than whether we have these notions at

658
00:34:03.660 --> 00:34:11.060
all. Right? Um So I, you know, one thing,

659
00:34:11.418 --> 00:34:14.188
one thing a lot of people said in say

660
00:34:14.197 --> 00:34:18.188
2017, 2018 when they were reading moral foundations theory,

661
00:34:18.418 --> 00:34:21.608
this notion of purity, um which was supposed to

662
00:34:21.618 --> 00:34:23.608
be a solely conservative notion. A lot of people

663
00:34:23.618 --> 00:34:26.820
said, well, there are left wing notions of, you

664
00:34:26.830 --> 00:34:29.219
know, purity trying to make sure you're not involved

665
00:34:29.228 --> 00:34:31.708
in anything problematic, trying to make sure you don't

666
00:34:31.918 --> 00:34:34.100
use the wrong words, trying to make sure that

667
00:34:34.110 --> 00:34:36.418
you, you don't use the wrong symbols. That seems

668
00:34:36.429 --> 00:34:39.310
like a kind of notion of purity, right? Um

669
00:34:39.438 --> 00:34:42.899
And similarly, like, take the, this so called individualizing

670
00:34:42.909 --> 00:34:46.549
foundation of care. Um Well, there's a question of

671
00:34:46.559 --> 00:34:48.217
who do you care about, right? You're gonna care

672
00:34:48.228 --> 00:34:50.138
more about some people than others. And that seems

673
00:34:50.148 --> 00:34:53.569
like a political question as well. Um So it

674
00:34:53.579 --> 00:34:54.967
seemed to me that it started to seem to

675
00:34:54.978 --> 00:34:58.799
me that most of politics is not in these

676
00:34:58.809 --> 00:35:01.069
foundations but is in sort of like, what are

677
00:35:01.079 --> 00:35:02.789
the, what are the real world dynamics of how

678
00:35:02.799 --> 00:35:06.839
people apply these foundations? Um And how they apply

679
00:35:06.849 --> 00:35:09.850
them selectively. And that is, yes,

680
00:35:09.929 --> 00:35:11.219
go, go ahead. Sorry,

681
00:35:11.229 --> 00:35:12.899
sorry, I was gonna say that is, you know,

682
00:35:12.909 --> 00:35:15.879
moral foundations theory is probably the best of those

683
00:35:15.889 --> 00:35:17.979
theories. A lot of the other theories that are

684
00:35:17.989 --> 00:35:23.659
based on personality um are, are, are much more,

685
00:35:24.600 --> 00:35:28.149
they just start from the perspective that, you know,

686
00:35:28.159 --> 00:35:30.459
he at least started from the Jonathan height, the

687
00:35:30.520 --> 00:35:32.850
sort of the guy behind moral foundation, he at

688
00:35:32.929 --> 00:35:36.239
least started from the perspective of, well, let's try

689
00:35:36.250 --> 00:35:40.129
and understand every, you know, let's try and understand

690
00:35:40.139 --> 00:35:42.199
people and see what we can appreciate about different

691
00:35:42.209 --> 00:35:45.040
views. Um A lot of the personality theories and

692
00:35:45.050 --> 00:35:46.919
really a lot of the psychological theories of political

693
00:35:46.929 --> 00:35:49.810
belief overall, they start from the perspective of, you

694
00:35:49.820 --> 00:35:53.580
know, I know what's right and wrong in politics.

695
00:35:53.959 --> 00:35:56.600
And so I'm gonna, you know, I'm gonna explain

696
00:35:56.610 --> 00:35:59.699
the deviancy of the people who disagree with me

697
00:35:59.850 --> 00:36:02.090
in some time or another. So the idea of

698
00:36:02.100 --> 00:36:04.719
the authoritarian personality is a great example of this

699
00:36:04.729 --> 00:36:06.790
or the idea of the rigidity of the right,

700
00:36:07.129 --> 00:36:09.409
that's a model under which right wingers are just

701
00:36:09.419 --> 00:36:13.239
sort of like cognitively inflexible. Um But you also

702
00:36:13.250 --> 00:36:15.590
see this stuff from centrists where they, they kind

703
00:36:15.600 --> 00:36:17.379
of take these sorts of models and just apply

704
00:36:17.389 --> 00:36:19.070
them to everybody on the extreme. So if you're,

705
00:36:19.219 --> 00:36:21.100
if you're an extremist of some sort, you must,

706
00:36:21.110 --> 00:36:23.370
you know, it's almost a way of saying if

707
00:36:23.379 --> 00:36:25.449
you're an extremist, you have a personality disorder, right?

708
00:36:25.459 --> 00:36:30.350
And that's the explanation for political beliefs. Um OR,

709
00:36:30.360 --> 00:36:34.080
or, you know, correlations with dark triad traits. Um

710
00:36:34.090 --> 00:36:37.350
So the dark triad of uh Machiavellian is and

711
00:36:37.360 --> 00:36:43.020
narcissism and psycho psychopathy um or sociopathy, uh some

712
00:36:43.030 --> 00:36:45.219
studies have suggested there are correlations with that and

713
00:36:45.229 --> 00:36:49.290
political extremism. So I, I think, you know, II

714
00:36:49.300 --> 00:36:52.030
I find flaws in all these things, but I

715
00:36:52.040 --> 00:36:54.889
also have just sort of like this background view

716
00:36:54.899 --> 00:36:57.719
of like this isn't the way we should proceed

717
00:36:57.729 --> 00:37:02.070
in political epistemology. We shouldn't start by, you know,

718
00:37:02.080 --> 00:37:05.830
sort of like hating people we disagree with and

719
00:37:05.840 --> 00:37:08.790
trying to justify it basically, you know, um I

720
00:37:08.800 --> 00:37:11.149
don't, I don't think that's the right way and

721
00:37:11.159 --> 00:37:14.419
it's not, um it's not an approach that is

722
00:37:14.429 --> 00:37:19.590
consistent with the epistemological perspective, which is one which

723
00:37:19.600 --> 00:37:23.159
entertains the possibility of error and doubt and skepticism.

724
00:37:23.169 --> 00:37:25.739
Um And has since Descartes, if not before, you

725
00:37:25.750 --> 00:37:25.879
know,

726
00:37:27.010 --> 00:37:31.500
so another set of theories is based on ideology.

727
00:37:31.510 --> 00:37:33.860
What do you make of them and what do

728
00:37:33.870 --> 00:37:36.219
you think, or how do you look at the

729
00:37:36.229 --> 00:37:40.379
role that ideology might play in political beliefs?

730
00:37:41.280 --> 00:37:43.500
Yeah. So I, I think I ideology is a

731
00:37:43.510 --> 00:37:50.139
really interesting notion. Um And I have certain kinds

732
00:37:50.149 --> 00:37:57.520
of skepticism about uh whether people really have ideologies.

733
00:37:57.530 --> 00:38:04.800
Um So ideological theories try to explain, at least,

734
00:38:04.810 --> 00:38:08.209
at least in the most classical Marxist sense, ideological

735
00:38:08.219 --> 00:38:11.239
theories. I'm probably gonna make some mistakes here. Ideological

736
00:38:11.250 --> 00:38:19.729
theories try to explain why society is stable using

737
00:38:21.149 --> 00:38:26.090
uh posits about the beliefs that people have. So,

738
00:38:27.000 --> 00:38:29.260
you know, if you start from the perspective of

739
00:38:29.270 --> 00:38:31.899
we live in a super unjust society that doesn't

740
00:38:31.909 --> 00:38:36.239
benefit the people in it then or, or most

741
00:38:36.250 --> 00:38:37.729
of the people in it or something like that,

742
00:38:37.949 --> 00:38:40.979
then you have to give an explanation of why

743
00:38:40.989 --> 00:38:43.270
are these, the people in this society? Why are

744
00:38:43.280 --> 00:38:47.219
they not changing things up? Why are they not

745
00:38:47.229 --> 00:38:49.889
having a revolution or why are, why are not,

746
00:38:49.899 --> 00:38:51.929
why are less of them going on strike or

747
00:38:51.939 --> 00:38:57.790
something like that? And you posit ideology uh to

748
00:38:57.800 --> 00:39:01.290
say, well, they've been there, these beliefs have been

749
00:39:01.300 --> 00:39:03.750
sort of put in their heads which tell them

750
00:39:03.760 --> 00:39:07.850
you shouldn't have a revolution, right? It's ok, it

751
00:39:07.860 --> 00:39:11.300
stabilizes the society if they say to themselves, it's

752
00:39:11.310 --> 00:39:12.899
ok that I have less money than the other

753
00:39:12.909 --> 00:39:15.860
person because I don't deserve it. Right. Um, SO

754
00:39:15.870 --> 00:39:18.909
that's ideology in the Marxist sense is when, when

755
00:39:18.919 --> 00:39:21.969
people say, well, I have this background sense of

756
00:39:21.979 --> 00:39:24.320
how things ought to be and that makes it

757
00:39:24.330 --> 00:39:26.030
ok, that society is the way that it is

758
00:39:26.040 --> 00:39:29.129
and that stabilizes society. So that's why a lot

759
00:39:29.139 --> 00:39:31.879
of theories of ideology talk about it as having

760
00:39:31.889 --> 00:39:35.860
a functional role. Um And that also means that

761
00:39:35.870 --> 00:39:38.679
the concept of ideology has an explanatory role when

762
00:39:38.689 --> 00:39:41.899
it comes to uh the stability of society. And

763
00:39:41.909 --> 00:39:44.149
the fact that there aren't revolutions, the fact that

764
00:39:44.399 --> 00:39:48.510
we're still in a capitalist system, um capitalism, they're

765
00:39:48.520 --> 00:39:51.050
not meaning an ideology of capitalism, but just capitalism

766
00:39:51.060 --> 00:39:54.010
as a, you know, certain social structure. Um So

767
00:39:54.020 --> 00:39:57.300
this narrative has been extended past the Marxist. Um

768
00:39:58.310 --> 00:40:00.889
YOU know, one thing that has happened, uh you

769
00:40:00.899 --> 00:40:03.729
know, since the sixties uh in academia is that

770
00:40:03.909 --> 00:40:06.489
different sorts of Marxist theories have been applied to

771
00:40:06.500 --> 00:40:10.399
categories like race and gender. Um And so you

772
00:40:10.409 --> 00:40:15.449
have uh basically theories of ideology explaining, you know,

773
00:40:16.429 --> 00:40:21.489
you know, why, why are, you know, why would

774
00:40:21.500 --> 00:40:24.030
a, a black person accept racial inequality or why

775
00:40:24.040 --> 00:40:27.090
would a woman accept sex inequality? Well, it's because

776
00:40:27.100 --> 00:40:31.449
there's this ideology of white supremacy or this ideology

777
00:40:31.459 --> 00:40:35.479
of patriarchy um which to an extent has been

778
00:40:35.760 --> 00:40:39.000
internalized. So these ideology theories are often theories of

779
00:40:39.310 --> 00:40:44.889
uh the internalization and legitimation and moralization of unjust

780
00:40:45.290 --> 00:40:49.550
um social structures. Um Now, what do I think

781
00:40:49.560 --> 00:40:52.500
of ideology theory? This is a good question. One

782
00:40:52.510 --> 00:40:55.330
thing to say about ideology theories is that this

783
00:40:55.340 --> 00:41:00.860
functional role aspect um is something, is something that

784
00:41:00.870 --> 00:41:05.209
I have some skepticism about. Um I think that

785
00:41:05.379 --> 00:41:07.870
if you, if you look at people's political beliefs

786
00:41:07.879 --> 00:41:12.510
and say, well, this is sort of, this is

787
00:41:12.520 --> 00:41:15.939
sort of coming at them from above. Um FOR

788
00:41:15.949 --> 00:41:19.560
this uh nefarious reason. Um I sort of start

789
00:41:19.570 --> 00:41:22.760
out with a lot of skepticism towards that idea

790
00:41:22.770 --> 00:41:24.840
of how people get their, their beliefs to begin

791
00:41:24.850 --> 00:41:28.280
with, right? Um I tend to think most people

792
00:41:28.290 --> 00:41:31.520
get their beliefs, including their beliefs about politics from

793
00:41:31.530 --> 00:41:34.649
having some sort of contact with reality. Um And

794
00:41:34.659 --> 00:41:38.500
in fact, in the Marxist theory, there's the ideology

795
00:41:38.510 --> 00:41:42.610
theory is sort of combined with the opposite theory,

796
00:41:42.620 --> 00:41:46.350
which is class consciousness, which says yes, from being

797
00:41:46.360 --> 00:41:48.520
in, in the lower class, you do have an

798
00:41:48.530 --> 00:41:51.669
enhanced contact with reality, right? Um You actually have

799
00:41:51.679 --> 00:41:57.729
an epistemic advantage. Um uh THE class disadvantage in,

800
00:41:57.739 --> 00:42:00.469
in Marxism leads in at least some part, you

801
00:42:00.479 --> 00:42:03.929
know, some cir Marxist circles leads to an epistemic

802
00:42:03.939 --> 00:42:08.919
advantage. Um I also don't, so if we think

803
00:42:08.929 --> 00:42:11.770
of an ideology as sort of comprehensive or sort

804
00:42:11.780 --> 00:42:13.649
of big in some way, right? It structures the

805
00:42:13.659 --> 00:42:16.000
way people look at the world, which is one

806
00:42:16.010 --> 00:42:20.610
way people think about ideology. Um, IT'S not clear

807
00:42:20.620 --> 00:42:23.919
to me so it's really easy for somebody like

808
00:42:23.929 --> 00:42:26.840
a philosopher to say, well, of course, people have

809
00:42:26.989 --> 00:42:29.110
all these ideas in their heads and they're trying

810
00:42:29.120 --> 00:42:32.040
to make them consistent and they're making inferences from

811
00:42:32.050 --> 00:42:36.399
them and they're constantly, like, cogitating and intellectualizing everything

812
00:42:36.409 --> 00:42:40.010
they experience. Um, IT'S not clear to me that

813
00:42:40.020 --> 00:42:43.840
most people, you know, that is like a sort

814
00:42:43.850 --> 00:42:46.310
of anxious response, you know, and it's sort of

815
00:42:46.320 --> 00:42:49.989
overthinking that we find maybe intellectually productive use for

816
00:42:50.000 --> 00:42:54.370
in academic philosophy. Um But it, it doesn't seem

817
00:42:54.379 --> 00:42:59.889
to me that I, most people have such structured

818
00:42:59.899 --> 00:43:03.030
ways of thinking about the world. And in fact,

819
00:43:03.040 --> 00:43:05.709
there's classic work by the political science scientist, Philip

820
00:43:05.719 --> 00:43:09.169
Con, um which suggests, yeah, most people don't seem

821
00:43:09.179 --> 00:43:13.370
to have ideologies and I, you know, there's work

822
00:43:13.379 --> 00:43:16.770
by people like Michael Hannon, early femi time, Taiwo

823
00:43:17.810 --> 00:43:20.709
uh about what whether, you know, whether people have

824
00:43:20.719 --> 00:43:23.350
these political beliefs to begin with, uh whether people

825
00:43:23.360 --> 00:43:25.929
really have any political beliefs at all. So I

826
00:43:25.939 --> 00:43:28.080
think, I think I have some skepticism that these,

827
00:43:28.090 --> 00:43:31.590
that these big structures which are really um sort

828
00:43:31.600 --> 00:43:35.159
of intellectual organizations, uh whether they really can play

829
00:43:35.169 --> 00:43:37.139
any causal role when it comes to thinking about

830
00:43:37.149 --> 00:43:41.209
people's individual political beliefs. Um Now, that doesn't mean

831
00:43:41.219 --> 00:43:43.639
the ideology, it could be, it could be that

832
00:43:44.439 --> 00:43:48.209
the notion of ideology still has, you know, can

833
00:43:48.219 --> 00:43:52.429
still be an important notion in political theorizing. Um

834
00:43:52.790 --> 00:43:56.550
But that would be different than saying that individuals

835
00:43:56.560 --> 00:44:00.179
have ideologies, right? Um So for example, here's one

836
00:44:00.189 --> 00:44:02.389
way that here's one way that you could have

837
00:44:02.399 --> 00:44:04.860
an ideology at the level of society without individuals

838
00:44:04.870 --> 00:44:08.229
having an ideology, right? It could be that, you

839
00:44:08.239 --> 00:44:10.550
know, different people have different pieces of the ideology

840
00:44:11.020 --> 00:44:14.600
and for various reasons, you know, the person with

841
00:44:14.610 --> 00:44:17.449
this piece of the ideology gets to say at

842
00:44:17.459 --> 00:44:19.399
this moment and the person with that piece of

843
00:44:19.409 --> 00:44:22.790
the ideology gets to say at that moment. Um

844
00:44:23.159 --> 00:44:24.899
And so that's a way that you could have

845
00:44:24.909 --> 00:44:28.020
without individuals having an ideology, you could still have

846
00:44:28.030 --> 00:44:33.729
some sort of society wide ideology. Um Now, would

847
00:44:33.739 --> 00:44:35.649
it then be best to think of it as

848
00:44:35.659 --> 00:44:39.870
a set of political beliefs? Well, probably not exactly,

849
00:44:39.879 --> 00:44:41.370
it would be better to think of it as

850
00:44:41.379 --> 00:44:44.050
sort of like a, a set of social incentives

851
00:44:44.060 --> 00:44:48.530
and a set of um dynamics in which in

852
00:44:48.600 --> 00:44:50.689
which you had different people are brought forward at

853
00:44:50.699 --> 00:44:53.590
different times or have, have power in different contexts.

854
00:44:54.020 --> 00:44:56.229
Um And this has to do with how conducive

855
00:44:56.959 --> 00:45:00.000
those people's views are to, to some, I don't

856
00:45:00.010 --> 00:45:01.739
know, some power structure or whatever you would want

857
00:45:01.750 --> 00:45:05.909
to call it. Um So you can still have,

858
00:45:05.919 --> 00:45:07.949
maybe the notion of ideology can still be useful

859
00:45:07.959 --> 00:45:10.090
even if you don't think people have ideologies or

860
00:45:10.100 --> 00:45:14.959
individuals have ideologies. Um I also, you know, in

861
00:45:14.969 --> 00:45:17.179
the book, I make this quick note which I've

862
00:45:17.189 --> 00:45:21.290
considered coming back to. Um, SO, and maybe we'll

863
00:45:21.300 --> 00:45:23.929
talk about this later. Uh There's this notion of

864
00:45:23.939 --> 00:45:29.149
conspiracy theory, um, uh, which is a big, uh,

865
00:45:29.159 --> 00:45:32.209
a big topic in political epistemology. Um, JUST because

866
00:45:32.219 --> 00:45:34.250
conspiracy theorizing seems a lot of people to be

867
00:45:34.260 --> 00:45:36.790
very bad, it's bad for society and also seems

868
00:45:36.800 --> 00:45:40.870
to people to be bad reasoning. Um, BUT a

869
00:45:40.879 --> 00:45:42.770
lot of theories of ideology, sort of, kind of

870
00:45:42.780 --> 00:45:44.949
sound like conspiracy theories, right? They sort of sound

871
00:45:44.959 --> 00:45:47.290
like, well, there's this group of people who are

872
00:45:47.300 --> 00:45:51.250
advantaged in society and they sort of put a

873
00:45:51.260 --> 00:45:53.820
lot of ideas in people's heads to pull the

874
00:45:53.830 --> 00:45:55.909
war over their eyes about what's really going on,

875
00:45:55.919 --> 00:46:00.120
right? Um In order to keep their power. Um

876
00:46:01.120 --> 00:46:03.000
And uh yeah, so I have a lot of

877
00:46:03.010 --> 00:46:06.899
skepticism about narratives like that. Um Not because I

878
00:46:06.909 --> 00:46:10.620
think so, I've skepticism about, about that because I

879
00:46:10.629 --> 00:46:12.189
don't think it's so easy to pull the wall

880
00:46:12.199 --> 00:46:13.929
over people's eyes. I do think people want to

881
00:46:13.939 --> 00:46:15.459
pull the wall over each other's eyes, right? Like

882
00:46:15.469 --> 00:46:18.989
people aren't, people aren't so great. Um But I,

883
00:46:19.000 --> 00:46:20.879
but I also don't think people are that gullible

884
00:46:20.889 --> 00:46:21.179
either.

885
00:46:21.250 --> 00:46:25.689
What about group membership? What do you make of

886
00:46:25.699 --> 00:46:28.989
the theories that try to explain political beliefs on

887
00:46:29.000 --> 00:46:31.300
the basis of a group membership?

888
00:46:32.320 --> 00:46:35.100
Yeah. So, yeah, so this is sort of like

889
00:46:35.110 --> 00:46:36.939
the other side of the coin that I was

890
00:46:36.949 --> 00:46:39.729
talking about where, you know, in the Marxist conception,

891
00:46:39.739 --> 00:46:44.149
the relevant groups are classes, right? Um OR economic

892
00:46:44.159 --> 00:46:48.419
strata or different relations to the uh to production

893
00:46:48.429 --> 00:46:54.040
groups with different relations to production. Now, these, these

894
00:46:54.050 --> 00:46:57.679
theories can be very useful uh especially in American

895
00:46:57.689 --> 00:47:00.629
politics, right? A lot of American political commentary is,

896
00:47:00.639 --> 00:47:05.600
you know, well, you know, turns out actually, you

897
00:47:05.610 --> 00:47:07.639
know, there's more, you do a poll and there's

898
00:47:07.649 --> 00:47:10.350
more Hispanic support for Donald Trump this time than

899
00:47:10.360 --> 00:47:12.840
there was last time. So then you're like, wow,

900
00:47:12.850 --> 00:47:15.320
what are, you know, what is this? You know,

901
00:47:15.330 --> 00:47:17.310
why do Hispanics like Trump more? And you have

902
00:47:17.320 --> 00:47:19.520
this whole and uh there do seem to be

903
00:47:19.530 --> 00:47:23.439
correlations where, you know, men vote Republican more often

904
00:47:23.449 --> 00:47:25.409
and women vote Democrat more often, white people vote

905
00:47:25.419 --> 00:47:27.530
Republican more often and black people vote Democrat more

906
00:47:27.550 --> 00:47:29.860
often. So in the US, it seems really useful.

907
00:47:29.870 --> 00:47:35.020
But on the other hand, um these, you know,

908
00:47:35.030 --> 00:47:38.379
these correlations are not actually that strong either, right?

909
00:47:38.830 --> 00:47:43.250
Um So for example, Hispanics, uh these days, it's

910
00:47:43.260 --> 00:47:45.919
around maybe two thirds vote for Democrats and one

911
00:47:45.929 --> 00:47:49.699
third vote for Republicans. Um So to me that's

912
00:47:49.709 --> 00:47:53.250
not gonna be, that doesn't seem like we're gonna

913
00:47:53.260 --> 00:47:55.300
get a great background view of, of where people's

914
00:47:55.310 --> 00:47:58.189
political beliefs come from. Um And in fact, all

915
00:47:58.199 --> 00:47:59.870
these theories, we were just talking about theories of

916
00:47:59.879 --> 00:48:03.090
ideology, they're there precisely to explain why the correlations

917
00:48:03.100 --> 00:48:07.620
aren't that strong, right? The notion is precisely, well,

918
00:48:07.629 --> 00:48:10.669
actually the lower classes, some of them believe in

919
00:48:10.679 --> 00:48:13.310
cal, right? Like some of them don't, some of

920
00:48:13.320 --> 00:48:15.330
them aren't Marxists. We have to explain that. So

921
00:48:15.340 --> 00:48:17.310
we, we, we throw in this notion of ideology,

922
00:48:17.320 --> 00:48:22.919
right? Um So in general, I think that uh

923
00:48:23.340 --> 00:48:26.760
theories of group membership, uh they're just not really

924
00:48:26.770 --> 00:48:31.560
full theories of political belief. Um BECAUSE everybody acknowledges

925
00:48:31.570 --> 00:48:35.729
that, you know, any given member of any demographic

926
00:48:35.739 --> 00:48:40.040
or economic group could support, you know, could support

927
00:48:40.050 --> 00:48:42.040
the, you know, the exact opposite of what everybody

928
00:48:42.050 --> 00:48:46.620
else does, right? And yeah, so there are they

929
00:48:47.540 --> 00:48:50.449
the source of, and I don't know how good

930
00:48:50.459 --> 00:48:52.469
a job I did explaining this in the book.

931
00:48:53.370 --> 00:48:57.129
Um Thinking back now, probably not that great. The

932
00:48:57.139 --> 00:49:01.419
source of in Marxism, the, the source of the

933
00:49:01.429 --> 00:49:03.969
group or the class, political belief has something to

934
00:49:03.979 --> 00:49:06.550
do with the interests of the class, right? So

935
00:49:06.560 --> 00:49:09.669
it's sort of like, you know, the, the proletariat

936
00:49:09.679 --> 00:49:12.860
class interests will determine the proletariat beliefs and the

937
00:49:12.870 --> 00:49:15.489
capitalist class interests will determine the capitalist beliefs in

938
00:49:15.500 --> 00:49:18.179
some sense or another. It's important to distinguish this

939
00:49:18.189 --> 00:49:23.770
from theories of individual interest. Um So that's another

940
00:49:23.780 --> 00:49:28.159
kind of mysterious question of how do I sort

941
00:49:28.169 --> 00:49:31.959
of conceive of myself as a member of a

942
00:49:31.969 --> 00:49:34.929
class or group rather than an individual and sort

943
00:49:34.939 --> 00:49:37.969
of magically get the class interest into my head.

944
00:49:38.219 --> 00:49:40.360
There's obvious reasons why, you know, we have a

945
00:49:40.370 --> 00:49:42.340
sense of our own individual interest because we have

946
00:49:42.350 --> 00:49:47.860
a sense of our desires, right? Um It's not

947
00:49:47.870 --> 00:49:50.969
clear, this is another instance, the notion of class

948
00:49:50.979 --> 00:49:54.110
interest seems very intellectual and abstract. It's not clear

949
00:49:54.120 --> 00:49:57.850
to me that most humans are gonna are gonna

950
00:49:57.860 --> 00:50:00.250
be thinking about politics in this way, um whether

951
00:50:00.260 --> 00:50:01.620
explicitly or implicitly.

952
00:50:01.729 --> 00:50:04.250
So another set of theories has to do with

953
00:50:04.260 --> 00:50:08.540
social isolation. So what are the claims there?

954
00:50:09.850 --> 00:50:13.219
Yeah. So these are, these are the that um

955
00:50:14.770 --> 00:50:17.510
fit really well with contemporary American politics and of

956
00:50:17.520 --> 00:50:19.989
course, with all these things, there's a difficulty of,

957
00:50:21.030 --> 00:50:23.040
you know, this book is a very American book,

958
00:50:23.050 --> 00:50:24.669
right? I'm, I'm writing about dynamics that seem to

959
00:50:24.679 --> 00:50:28.770
be present in American politics right now, right? Um

960
00:50:29.330 --> 00:50:33.239
So the, these, these theories about social groups uh

961
00:50:34.419 --> 00:50:38.149
are they're different from the group membership theories um

962
00:50:38.959 --> 00:50:41.780
because they have to do with the incentives people

963
00:50:41.790 --> 00:50:49.090
have uh to sort of not avoid going against

964
00:50:49.100 --> 00:50:55.629
their group, right? Um So these incentives could be

965
00:50:55.639 --> 00:50:58.669
kind of internal or external, right? Um So the

966
00:50:58.679 --> 00:51:02.189
external incentive is like, if I'm a democrat and

967
00:51:02.199 --> 00:51:06.070
I say taxes should be lower and everybody should

968
00:51:06.080 --> 00:51:08.870
own a gun, then maybe my friends aren't friends

969
00:51:08.879 --> 00:51:12.300
with me anymore, right? Maybe I get socially um

970
00:51:12.310 --> 00:51:15.449
ostracized or something like that. The internal one is

971
00:51:15.459 --> 00:51:18.129
like, and this is the one that is really,

972
00:51:18.300 --> 00:51:21.449
really keyed to this current moment in American politics.

973
00:51:21.810 --> 00:51:27.419
The internal one is about maybe my sense of

974
00:51:27.429 --> 00:51:31.370
myself is tied up in a certain way with

975
00:51:31.379 --> 00:51:33.610
being a good Democrat or being a good Republican

976
00:51:33.620 --> 00:51:35.159
or being a good progressive or being a good

977
00:51:35.169 --> 00:51:39.080
conservative or whatever it is. And that means if

978
00:51:39.090 --> 00:51:42.330
I go against the group on a political issue

979
00:51:43.510 --> 00:51:46.469
internally, I feel a struggle internally. I start saying

980
00:51:46.479 --> 00:51:49.060
who am I, I start worrying that I can't

981
00:51:49.070 --> 00:51:53.300
make sense of my own identity, right? Um And

982
00:51:53.310 --> 00:51:56.830
so you have these two sources of pressure from

983
00:51:56.840 --> 00:52:00.010
the notion of political identity. And so, yeah, the

984
00:52:00.020 --> 00:52:04.419
theory basically says our political beliefs exist in part

985
00:52:04.429 --> 00:52:06.429
or we have the political beliefs we do in

986
00:52:06.439 --> 00:52:12.540
part reserve the external and internal appearance of having

987
00:52:12.550 --> 00:52:16.280
some kind of identity that corresponds with the political

988
00:52:16.290 --> 00:52:21.310
group. Um And yeah, so this is, it's very,

989
00:52:21.320 --> 00:52:24.030
it's still a notion that it's based on the

990
00:52:24.040 --> 00:52:27.260
fact that there are political groups. Um But the

991
00:52:27.270 --> 00:52:31.010
groups in question are different uh than in, than

992
00:52:31.020 --> 00:52:35.379
in the group membership. The, and uh the relationship

993
00:52:35.389 --> 00:52:39.010
is also different. Um uh It's much more a

994
00:52:39.020 --> 00:52:42.320
relationship based on psychological incentives and things like that.

995
00:52:43.060 --> 00:52:46.459
Um And one thing to say about these identity

996
00:52:46.469 --> 00:52:49.780
and partisanship type theories is that they're not theories

997
00:52:49.790 --> 00:52:52.189
on which our political beliefs come out looking very

998
00:52:52.199 --> 00:52:56.939
rational. Um These are theories on which, you know,

999
00:52:57.080 --> 00:53:01.909
we're doing motivated reasoning to avoid upsetting our sense,

1000
00:53:01.919 --> 00:53:04.350
upsetting our social circle or upsetting our sense of

1001
00:53:04.360 --> 00:53:09.409
who we are, you know. Um And uh that's

1002
00:53:09.419 --> 00:53:15.199
not, you know, that's not epistemological, secure right. That's

1003
00:53:15.209 --> 00:53:17.510
not looking at the evidence and making a recent

1004
00:53:17.520 --> 00:53:21.790
judgment. Um It's sort of uh you know, reacting

1005
00:53:21.800 --> 00:53:26.530
on instinct, um reacting out of a sense of

1006
00:53:26.540 --> 00:53:29.280
threat or something like that, a social or psychological

1007
00:53:29.290 --> 00:53:32.389
threat. So let me ask you about one final

1008
00:53:32.399 --> 00:53:35.120
set of theories before we move on to another

1009
00:53:35.129 --> 00:53:40.750
topic, you also cover theories based on cognitive heuristics.

1010
00:53:40.760 --> 00:53:43.449
So what are these about?

1011
00:53:43.939 --> 00:53:48.540
Yeah. So this is, this is um this is

1012
00:53:48.550 --> 00:53:51.550
really just one theory from the late Jeffrey Friedman,

1013
00:53:51.560 --> 00:53:54.239
who was a, I wouldn't say a mentor but

1014
00:53:54.250 --> 00:53:56.169
invited me to a thing at Berkeley. And we

1015
00:53:56.179 --> 00:53:58.370
had a lot of talks about political epistemology when

1016
00:53:58.379 --> 00:54:01.280
I was just starting out in grad school. Um

1017
00:54:02.340 --> 00:54:06.899
So he has basically a theory. Uh IT takes

1018
00:54:06.909 --> 00:54:08.840
as its starting point, sort of like the complexity

1019
00:54:08.850 --> 00:54:12.969
of the modern world. Um And Friedman Friedman's notion

1020
00:54:12.979 --> 00:54:18.169
is that we have to do something to cognitively

1021
00:54:18.179 --> 00:54:20.370
to simplify the modern world. We have to be

1022
00:54:20.379 --> 00:54:22.540
able to understand the modern world in some way.

1023
00:54:22.830 --> 00:54:25.530
And I don't, I forget if he uses the

1024
00:54:25.540 --> 00:54:27.330
term cognitive heuristics, it might be the term I

1025
00:54:27.340 --> 00:54:30.010
use. Basically, the notion is we need a way

1026
00:54:30.020 --> 00:54:32.389
to categorize all the different inputs we're we're getting

1027
00:54:32.899 --> 00:54:36.040
um maybe we need some information is more important

1028
00:54:36.050 --> 00:54:37.590
than others, or some people can be trusted and

1029
00:54:37.600 --> 00:54:41.229
other people can't um or some phenomena we reach

1030
00:54:41.239 --> 00:54:44.080
for certain sorts of explanations, right? Rather than others

1031
00:54:44.469 --> 00:54:47.090
because if we couldn't do that, then, you know,

1032
00:54:48.149 --> 00:54:49.899
you try to make you just, just think of

1033
00:54:49.909 --> 00:54:51.229
how many things are happening in the world. And

1034
00:54:51.239 --> 00:54:52.620
if you're on, you're on Twitter like me, I

1035
00:54:52.629 --> 00:54:55.219
think how, just how many world events you see,

1036
00:54:55.659 --> 00:54:58.929
you, you're, you, you're familiar with the internal politics

1037
00:54:58.939 --> 00:55:00.919
of all these different countries. You know, whenever I

1038
00:55:00.929 --> 00:55:03.500
wake up early I see all these tweets from

1039
00:55:03.510 --> 00:55:06.159
my British friends about all, all everything that's happening

1040
00:55:06.169 --> 00:55:09.239
in British politics. Um And then the Americans wake

1041
00:55:09.250 --> 00:55:11.090
up and I hear about American politics and then,

1042
00:55:11.469 --> 00:55:13.939
you know, there's Asian politics and it's sort of

1043
00:55:13.949 --> 00:55:17.489
like, you know, if I want to make sense

1044
00:55:17.500 --> 00:55:22.389
of this. Um AND avoid it being uh you

1045
00:55:22.399 --> 00:55:25.830
know, Friedman is working from sort of American pragmatist

1046
00:55:25.840 --> 00:55:27.919
tradition. I think it's William James who he, he

1047
00:55:27.929 --> 00:55:31.840
quotes. That's maybe, maybe someone else. But there's this

1048
00:55:31.850 --> 00:55:34.929
phrase uh about the uh I think about babies

1049
00:55:34.939 --> 00:55:38.320
where before they have categories for their experience, they

1050
00:55:38.330 --> 00:55:41.590
experience the world as a blooming buzzing confusion. Um

1051
00:55:41.600 --> 00:55:43.629
IS the phrase, right? So the idea is if

1052
00:55:43.639 --> 00:55:45.060
we don't want the world to just be like

1053
00:55:45.070 --> 00:55:48.239
a confusing assortment of signs and symbols, um we

1054
00:55:48.250 --> 00:55:51.919
need boxes to put the things in. Um And

1055
00:55:51.929 --> 00:55:56.550
Friedman basically said that there's no rational way to

1056
00:55:56.560 --> 00:56:00.750
pick one kind of categorization or over another one

1057
00:56:00.760 --> 00:56:03.959
set of heuristics over another. Um Because you're sort

1058
00:56:03.969 --> 00:56:07.080
of pre rational the, the, the way you organize

1059
00:56:07.090 --> 00:56:10.149
your information is prior to the rationality of, of,

1060
00:56:10.159 --> 00:56:12.659
of digesting it um and making inferences based on

1061
00:56:12.669 --> 00:56:17.439
it. Um And so he basically thinks all of

1062
00:56:17.449 --> 00:56:19.739
your politics will be determined by the set of

1063
00:56:19.750 --> 00:56:22.899
cognitive heuristics that you're working with to simplify uh

1064
00:56:22.909 --> 00:56:26.560
the informational sphere. Um So an example that he,

1065
00:56:26.570 --> 00:56:28.580
I think, and his life sort of went back

1066
00:56:28.590 --> 00:56:32.149
and forth on maybe in some sense, uh is

1067
00:56:32.159 --> 00:56:36.600
the notion that people respond to incentives, which obviously

1068
00:56:36.610 --> 00:56:39.899
is an assumption that economists make and political libertarians

1069
00:56:39.909 --> 00:56:43.179
and other sorts of, you know, economic determinist and

1070
00:56:43.189 --> 00:56:48.510
materialists, um uh and reduction, you know, economic reductionist

1071
00:56:48.520 --> 00:56:53.270
make, class reductionist and Marxism make um the notion

1072
00:56:53.280 --> 00:56:55.639
that people are gonna respond to incentives uh also

1073
00:56:56.120 --> 00:56:58.949
helps us explain a lot of different things. Um

1074
00:56:59.739 --> 00:57:01.840
Yeah, but according to Friedman, it's one of these,

1075
00:57:01.850 --> 00:57:06.050
like, you, you can't really get behind it, right?

1076
00:57:06.060 --> 00:57:07.949
It's not something that you can assess. It's just

1077
00:57:07.959 --> 00:57:12.179
sort of like a way of um a way

1078
00:57:12.189 --> 00:57:15.850
of making sense of the world. Um AAA kind

1079
00:57:15.860 --> 00:57:18.520
of heuristic or, or, or, or even a kind

1080
00:57:18.530 --> 00:57:22.489
of bias. Um So that's, so that's Friedman's notion.

1081
00:57:22.500 --> 00:57:24.850
It's, it's an interesting notion. I, I don't, I

1082
00:57:24.860 --> 00:57:27.959
don't know if to me that there's, it seems

1083
00:57:27.969 --> 00:57:31.060
a little um I don't know if I really

1084
00:57:31.070 --> 00:57:32.790
believe in the hierarchy of there are these different

1085
00:57:32.800 --> 00:57:36.280
kinds of things by one of which is a

1086
00:57:36.290 --> 00:57:39.330
heuristic and then there's the perception and the heuristic

1087
00:57:39.340 --> 00:57:42.189
and they're like completely different types of, you know,

1088
00:57:42.199 --> 00:57:43.899
maybe he doesn't need to make such a strong

1089
00:57:43.909 --> 00:57:46.899
assumption. Um But to me, it's always seemed that

1090
00:57:47.110 --> 00:57:49.459
we can, we can re evaluate our heuristics. Um

1091
00:57:49.469 --> 00:57:52.189
If they, if they, you know, if they lead

1092
00:57:52.199 --> 00:57:55.169
us astray, um, it can be very difficult obviously

1093
00:57:55.179 --> 00:57:56.949
and we can get stuck in them, especially as

1094
00:57:56.959 --> 00:58:01.580
we get older, less flexible. Um, BUT I'm not,

1095
00:58:01.590 --> 00:58:03.540
I'm, I don't think we get stuck in quite

1096
00:58:03.550 --> 00:58:06.729
the way. Um I call it tunneling. It's like

1097
00:58:06.739 --> 00:58:08.340
tunnel vision you get right when you have these

1098
00:58:08.350 --> 00:58:10.689
year, I don't think, I don't think it's necessary

1099
00:58:10.699 --> 00:58:12.800
that we get tunnel vision in this way. Um

1100
00:58:12.810 --> 00:58:16.699
I think a lot of people do um but

1101
00:58:16.709 --> 00:58:20.070
that's sort of out of, you know, the explanation

1102
00:58:20.080 --> 00:58:22.429
for that is not gonna be from like first

1103
00:58:22.439 --> 00:58:24.110
principle. So he thinks this is like a first

1104
00:58:24.120 --> 00:58:28.000
principle. It's just the way perception and complexity work

1105
00:58:28.010 --> 00:58:30.449
that we have to have these simplifications. I think

1106
00:58:30.459 --> 00:58:32.510
it's more about like people are a little lazy

1107
00:58:32.520 --> 00:58:33.070
and stuff like

1108
00:58:33.080 --> 00:58:36.530
that. So uh let me ask you now about

1109
00:58:36.540 --> 00:58:41.189
the epistemology of democracy, which is also something that

1110
00:58:41.199 --> 00:58:45.060
you cover in the book. So what is this

1111
00:58:45.070 --> 00:58:47.179
about? Exactly? I mean, what are the kinds of

1112
00:58:47.189 --> 00:58:50.399
questions that people raise when they talk about the

1113
00:58:50.409 --> 00:58:55.050
epistemology of democracy? And since we're talking about epistemology

1114
00:58:55.060 --> 00:58:59.899
here, should we trust what the majority beliefs?

1115
00:59:00.229 --> 00:59:03.219
Yeah. Good question. Um So the notion of the

1116
00:59:03.229 --> 00:59:07.290
epistemology of democracy, yes, basically this question that, that,

1117
00:59:07.300 --> 00:59:09.320
that you just asked, uh should we trust the

1118
00:59:09.330 --> 00:59:13.550
majority um in the la sorry. In the late

1119
00:59:13.560 --> 00:59:18.179
17 hundreds, um people started being interested in sort

1120
00:59:18.189 --> 00:59:21.780
of a, you know, statistical and mathematical justifications for

1121
00:59:21.790 --> 00:59:27.020
democracy. Um And one of them uh from the

1122
00:59:27.030 --> 00:59:31.050
Marquis de Conor site, very, very simple notion is

1123
00:59:31.060 --> 00:59:36.419
just about uh how majority votes go if the

1124
00:59:36.429 --> 00:59:40.969
voters are sort of independent. Um And it turns

1125
00:59:40.979 --> 00:59:42.929
out that, uh you know, the more people you

1126
00:59:42.939 --> 00:59:46.830
have voting, as long as they're reliable and reliable,

1127
00:59:46.840 --> 00:59:48.750
you know, to a very small degree, just like

1128
00:59:48.760 --> 00:59:50.709
better than chance, right, better than a coin flip

1129
00:59:50.820 --> 00:59:53.530
to the extent that people are better than a

1130
00:59:53.540 --> 00:59:56.969
coin flip as you add, people who are in

1131
00:59:56.979 --> 01:00:00.239
some sense are correlated with one another. Um You

1132
01:00:00.250 --> 01:00:03.870
actually get a completely reliable um majority vote um

1133
01:00:03.879 --> 01:00:06.090
if the number of voters is high enough. Um

1134
01:00:06.100 --> 01:00:08.229
So this is like a ma uh a mathematical

1135
01:00:08.239 --> 01:00:14.239
fact. Um And so the epistemology of democracy um

1136
01:00:15.020 --> 01:00:17.580
or democratic epistemology is sort of a research program

1137
01:00:17.590 --> 01:00:21.030
where people debate. What did these, what does this

1138
01:00:21.040 --> 01:00:24.340
mathematical fact have to do with real world democracy?

1139
01:00:24.350 --> 01:00:26.949
Um IN a sense. Well, this is, this is

1140
01:00:26.959 --> 01:00:29.280
one side of the epistemology of democracy, the other

1141
01:00:29.290 --> 01:00:31.270
sense that that's what I call aggregate, right? Where

1142
01:00:31.280 --> 01:00:34.800
you're aggregating, aggregating a lot of different people um

1143
01:00:35.209 --> 01:00:37.439
the other side of the epistemology of democracy uh

1144
01:00:37.449 --> 01:00:41.300
is about deliberation and the notion that, yeah, if

1145
01:00:41.310 --> 01:00:43.149
you have a group of people talking something over,

1146
01:00:43.570 --> 01:00:45.050
just think about when you go to, you know,

1147
01:00:45.139 --> 01:00:46.550
when you have a hard decision, you ask other

1148
01:00:46.560 --> 01:00:49.639
people for advice, right? Um It just seems that

1149
01:00:49.649 --> 01:00:51.199
if we deliberate in a group, we get more

1150
01:00:51.209 --> 01:00:53.429
reasons, we have more minds, right? We have more

1151
01:00:53.439 --> 01:00:56.459
people helping to figure something out. Um It seems

1152
01:00:56.469 --> 01:00:58.719
like bringing in other people, other smart people always

1153
01:00:58.729 --> 01:01:01.989
helps us make decisions. That's what we do. Um

1154
01:01:03.030 --> 01:01:05.159
Very, very common practice for people to ask and

1155
01:01:05.169 --> 01:01:07.129
advice in this way. And so you have these

1156
01:01:07.139 --> 01:01:11.639
two sides, the aggregation and the deliberation. Um And

1157
01:01:13.179 --> 01:01:16.169
in political theory, you would be using these to

1158
01:01:16.179 --> 01:01:18.719
say, well, this is why democracy is good, right?

1159
01:01:19.159 --> 01:01:23.189
Um That's not exactly my goal is the epistemological

1160
01:01:23.199 --> 01:01:24.949
goal of the individual, which is sort of like,

1161
01:01:25.560 --> 01:01:27.159
say, I say, I know that a group of

1162
01:01:27.169 --> 01:01:30.649
people deliberated and came to a certain conclusion. Uh

1163
01:01:30.659 --> 01:01:33.149
Should I trust that conclusion or like under what

1164
01:01:33.159 --> 01:01:35.929
conditions should I trust that conclusion or say, I

1165
01:01:35.939 --> 01:01:38.010
see that there's a majority vote under what conditions

1166
01:01:38.020 --> 01:01:41.560
should I trust that majority vote? And so just

1167
01:01:41.570 --> 01:01:46.250
taking these one by one in the deliberating case,

1168
01:01:46.260 --> 01:01:48.110
one thing we might want to say is, well,

1169
01:01:48.120 --> 01:01:50.540
it depends on the internal dynamics of the deliberating

1170
01:01:50.550 --> 01:01:54.709
group, right? So if there are sort of, you

1171
01:01:54.719 --> 01:01:59.889
know, uh, I think Helen Land more or somebody

1172
01:01:59.899 --> 01:02:04.139
calls it reputational influences. Right. Um, IF there are,

1173
01:02:04.149 --> 01:02:06.520
or maybe it's what K Sunstein calls it, if

1174
01:02:06.530 --> 01:02:08.760
there are these reputational influences where there, there's one

1175
01:02:08.770 --> 01:02:11.580
person in there who everybody likes, you know, um,

1176
01:02:11.590 --> 01:02:14.219
or maybe he's, like, really physically attractive or something

1177
01:02:14.489 --> 01:02:17.169
and nobody wants to, you know, nobody wants to

1178
01:02:17.179 --> 01:02:19.439
disagree with them. Right. So, they're just gonna say

1179
01:02:19.449 --> 01:02:21.449
what they want and everybody's just gonna go along

1180
01:02:21.459 --> 01:02:24.169
with them. That's not a group, I'm gonna trust,

1181
01:02:24.179 --> 01:02:27.389
right? Unless I trust that one person, everybody's going

1182
01:02:27.399 --> 01:02:28.699
along with the one person I trust. But then

1183
01:02:28.709 --> 01:02:31.409
it's basically just the equivalent of only trusting one

1184
01:02:31.419 --> 01:02:34.879
person, right? Did everybody feel free to speak things

1185
01:02:34.889 --> 01:02:37.570
like that? That'll affect whether I trust, whether I

1186
01:02:37.580 --> 01:02:39.219
trust the group. What are the, basically what are

1187
01:02:39.229 --> 01:02:42.459
the incentives for giving good reasons or giving bad

1188
01:02:42.469 --> 01:02:48.250
reasons? Um In the aggregate case, I, you wanna

1189
01:02:48.260 --> 01:02:51.280
know sort of like how good, how good are

1190
01:02:51.290 --> 01:02:53.560
the people who I'm aggregating and making decisions, their

1191
01:02:53.570 --> 01:02:56.590
competency. And if, if it turns out that most

1192
01:02:56.600 --> 01:02:58.750
people who are voting get things wrong most of

1193
01:02:58.760 --> 01:03:01.360
the time, then you're not gonna trust the vote,

1194
01:03:01.370 --> 01:03:02.790
right? But if it turns out that most people,

1195
01:03:02.800 --> 01:03:04.389
most of the people voting get things right? Most

1196
01:03:04.399 --> 01:03:05.449
of the time, then you are gonna trust the

1197
01:03:05.459 --> 01:03:07.919
vote, the other issue which is very dear to

1198
01:03:07.929 --> 01:03:10.340
my heart. Um Probably to me, one of the

1199
01:03:10.350 --> 01:03:11.870
most important things in the book, one of the

1200
01:03:11.879 --> 01:03:14.790
most important notions of the book is this notion

1201
01:03:14.800 --> 01:03:17.719
that if people are making up their minds independently,

1202
01:03:18.500 --> 01:03:21.139
the group, the majority vote of the group will

1203
01:03:21.149 --> 01:03:25.669
be, will be more reliable. Um And this, yeah,

1204
01:03:25.679 --> 01:03:27.899
this is just what Connor says, jury theorem says

1205
01:03:27.909 --> 01:03:31.409
um and can be seen uh you know, mathematically.

1206
01:03:31.770 --> 01:03:34.360
So in, in the book, I have two diagrams,

1207
01:03:34.370 --> 01:03:38.739
you know, of if you have three people and

1208
01:03:38.750 --> 01:03:42.010
three issues and each, each person is right about

1209
01:03:42.020 --> 01:03:44.350
two out of the three issues and wrong about

1210
01:03:44.360 --> 01:03:46.840
one, well, if they're wrong all wrong about the

1211
01:03:46.850 --> 01:03:49.030
same issue, then the majority vote will get that

1212
01:03:49.040 --> 01:03:52.250
wrong, right? But if they're all wrong about different

1213
01:03:52.260 --> 01:03:54.520
issues, then the majority vote will get every issue.

1214
01:03:54.530 --> 01:03:58.620
Right. Right. Um So it's a way of, you

1215
01:03:58.629 --> 01:04:00.479
know, if, if you think about it's almost like

1216
01:04:00.489 --> 01:04:02.979
investments, it's a way of like diversifying risk or

1217
01:04:02.989 --> 01:04:04.909
something like that, right? Like it's a way of,

1218
01:04:05.379 --> 01:04:08.100
it's a way of unc correlating the risk making

1219
01:04:08.110 --> 01:04:10.149
sure that mistakes don't happen at the same time

1220
01:04:10.159 --> 01:04:14.250
if people are thinking independently. Um NOW there's a

1221
01:04:14.260 --> 01:04:18.169
sort of paradox here which is like this is

1222
01:04:18.179 --> 01:04:23.020
a reason to trust majority votes or to trust

1223
01:04:23.030 --> 01:04:29.820
deliberation. But in trusting these democratic mechanisms, I reduce

1224
01:04:29.830 --> 01:04:32.860
the independence of my own decision making, right? By

1225
01:04:32.870 --> 01:04:35.709
looking to them as a source for my beliefs.

1226
01:04:36.060 --> 01:04:38.149
And that makes me actually worse at contributing to

1227
01:04:38.159 --> 01:04:41.189
them. Right? So there's a sense in which they

1228
01:04:41.399 --> 01:04:46.139
are epistemic responsibility in terms of our own beliefs

1229
01:04:46.149 --> 01:04:50.209
may not be the same as our epistemic responsibility

1230
01:04:50.219 --> 01:04:53.040
and some broader sense of contributing to a group

1231
01:04:53.050 --> 01:04:56.639
if that makes sense. Um I don't know if

1232
01:04:56.649 --> 01:04:57.500
that does make sense.

1233
01:04:58.260 --> 01:05:01.840
Y yes, it does. And, but then an alternative

1234
01:05:01.850 --> 01:05:06.360
to the epistemology of democracy would be to trust

1235
01:05:06.370 --> 01:05:09.469
the experts. Uh What do you make of that?

1236
01:05:09.929 --> 01:05:10.439
Yeah,

1237
01:05:10.449 --> 01:05:14.179
I think that so in the, in the book,

1238
01:05:14.189 --> 01:05:16.290
I, I lay out, I think seven different questions

1239
01:05:16.300 --> 01:05:20.139
about the nature of expertise. Um But the, you

1240
01:05:20.149 --> 01:05:23.280
know, the main question is uh you know, like,

1241
01:05:23.290 --> 01:05:26.050
how well are the experts doing right now? Um

1242
01:05:26.290 --> 01:05:28.310
Well, so there, there's two main questions for, for

1243
01:05:28.320 --> 01:05:29.739
a novice, right? One is how well are the

1244
01:05:29.750 --> 01:05:31.699
experts doing right now? And the other is, can

1245
01:05:31.709 --> 01:05:34.050
I even, can I even figure out who the

1246
01:05:34.060 --> 01:05:37.459
experts are? Right? Um So the second question, can

1247
01:05:37.469 --> 01:05:39.090
I even figure out who the experts are? This

1248
01:05:39.100 --> 01:05:42.399
is called the no novice expert question um or

1249
01:05:42.409 --> 01:05:47.169
the no sex problem by Alan Goldman uh has

1250
01:05:47.179 --> 01:05:49.610
a classic paper experts. Which ones can you trust

1251
01:05:50.419 --> 01:05:55.199
um about this problem? So the question is basically

1252
01:05:55.209 --> 01:05:58.199
like, you know, s you know, there's some mathematicians

1253
01:05:58.209 --> 01:06:02.419
disagreeing about a proof um two of them and

1254
01:06:02.429 --> 01:06:04.100
I'm trying to decide which one of them is,

1255
01:06:04.110 --> 01:06:05.360
right? One of them thinks is valid. The other

1256
01:06:05.370 --> 01:06:09.020
one thinks is in God. And so then I

1257
01:06:09.030 --> 01:06:10.870
look at all, you know, how do I decide

1258
01:06:10.879 --> 01:06:12.830
who's the math expert? Well, I can look at

1259
01:06:12.840 --> 01:06:17.300
all the other opinions about math. Mhm. Why? I

1260
01:06:17.310 --> 01:06:19.629
don't know if they read about those either. Right.

1261
01:06:19.639 --> 01:06:21.389
Because I'm not the math expert. So there's a

1262
01:06:21.399 --> 01:06:24.489
sense in which to evaluate which one of them

1263
01:06:24.500 --> 01:06:26.469
is a math expert. I have to be a

1264
01:06:26.479 --> 01:06:30.449
math expert, right to even know who the expert

1265
01:06:30.459 --> 01:06:32.899
is, especially in a case where experts are disagreeing.

1266
01:06:33.489 --> 01:06:38.060
Um And I think this is important in political

1267
01:06:38.070 --> 01:06:42.639
questions because in political questions, as we said, we're

1268
01:06:42.649 --> 01:06:45.120
always in this situation of dispute or conflict, right?

1269
01:06:45.620 --> 01:06:49.290
And one thing you see very commonly uh especially

1270
01:06:49.300 --> 01:06:52.689
in American politics, people will say, well, they have

1271
01:06:52.699 --> 01:06:55.469
their experts and we have ours, right? You have

1272
01:06:55.479 --> 01:06:57.449
think tanks, for example, you have right wing and

1273
01:06:57.459 --> 01:07:00.770
left wing think tanks which come up with the

1274
01:07:00.780 --> 01:07:07.250
published research, I maybe even peer reviewed articles and

1275
01:07:09.159 --> 01:07:13.530
the different experts think different things. Um So it's,

1276
01:07:13.540 --> 01:07:15.250
you know, it can be very difficult if you

1277
01:07:15.260 --> 01:07:17.879
have just one large literature on one side, one

1278
01:07:17.889 --> 01:07:20.750
large literature, on the other side. If you're not

1279
01:07:20.760 --> 01:07:21.840
going to take the time to be an expert

1280
01:07:21.850 --> 01:07:24.389
yourself, it's actually very difficult to choose a lot

1281
01:07:24.399 --> 01:07:28.540
of the time. Um And this is, you know,

1282
01:07:28.550 --> 01:07:30.159
again, this is gonna be very difficult to get

1283
01:07:30.169 --> 01:07:36.090
out of political epistemology because politics involves conflict um

1284
01:07:37.080 --> 01:07:40.060
and involves dispute. So they are gonna be experts

1285
01:07:40.070 --> 01:07:42.209
disputing these things most of the time. Now, the

1286
01:07:42.219 --> 01:07:43.760
question of how all the experts are doing, I

1287
01:07:43.770 --> 01:07:47.409
talked before about, you know, the these questions from

1288
01:07:47.419 --> 01:07:51.050
a little under a decade ago, about uh the

1289
01:07:51.060 --> 01:07:54.979
state of academic research, the state of social science,

1290
01:07:54.989 --> 01:07:59.439
the state of psychological research. Um uh FOR me,

1291
01:08:00.600 --> 01:08:02.969
one of the lessons of the replication crisis of

1292
01:08:02.979 --> 01:08:06.100
social psychology and also just one of the lessons

1293
01:08:06.110 --> 01:08:09.360
from my own experience in academia, you know, getting

1294
01:08:09.370 --> 01:08:11.449
to know some experts seeing what's, what's put into

1295
01:08:11.459 --> 01:08:15.129
different fields. Um And it's not so much, you

1296
01:08:15.139 --> 01:08:18.578
know, expert knowledge is always changing, it's always in

1297
01:08:18.587 --> 01:08:21.999
flux. Um Some of that is improvement, some of

1298
01:08:22.008 --> 01:08:25.988
it is fashion. But I, I think my sense

1299
01:08:25.999 --> 01:08:31.270
is that an ordinary person um if they see

1300
01:08:31.279 --> 01:08:34.470
some press release, so say, let's get, let's take

1301
01:08:34.479 --> 01:08:37.509
a, a practical situation, right? An ordinary person sees

1302
01:08:37.520 --> 01:08:42.490
some press release uh that says, you know, a

1303
01:08:42.520 --> 01:08:45.879
scientist at our, you know, at our university. Uh

1304
01:08:45.890 --> 01:08:49.439
THEY, they prove this in their lab. Um And

1305
01:08:49.450 --> 01:08:50.890
this is gonna be so amazing and it's gonna

1306
01:08:50.899 --> 01:08:55.890
have these great results. So just my experience, first

1307
01:08:55.899 --> 01:08:59.238
of all, did they actually prove it or not?

1308
01:08:59.729 --> 01:09:01.969
Who knows if it'll replicate, who knows if the

1309
01:09:01.979 --> 01:09:04.738
statistics were, were satisfying? Who knows if there's another

1310
01:09:04.749 --> 01:09:06.198
scientist on the other side of the issue who

1311
01:09:06.207 --> 01:09:09.828
has another paper saying the opposite. Right. Second question

1312
01:09:10.278 --> 01:09:12.898
is, is the, is the finding even being reported

1313
01:09:12.908 --> 01:09:15.839
accurately. Sometimes you talk to a scientist and they

1314
01:09:15.850 --> 01:09:17.810
say, oh, it's so embarrassing. The university wanted to

1315
01:09:17.819 --> 01:09:20.538
hype me up. And so they pretended I'd made

1316
01:09:20.548 --> 01:09:22.988
this big claim, but really, I'm making this very

1317
01:09:22.999 --> 01:09:27.158
small academic point, you know, so there's also a

1318
01:09:27.167 --> 01:09:29.769
question of, uh you know, as a reader can,

1319
01:09:29.778 --> 01:09:33.349
I even can I even really completely understand what's

1320
01:09:33.358 --> 01:09:39.319
being claimed um in this scientific paper. Um So

1321
01:09:39.330 --> 01:09:41.339
there's all, there's all these sorts of problems and

1322
01:09:41.350 --> 01:09:45.430
you start to, you know, then you know, another,

1323
01:09:45.439 --> 01:09:48.799
another question is which, which, which research gets in,

1324
01:09:48.810 --> 01:09:52.589
you know, it becomes known to you as a

1325
01:09:52.600 --> 01:09:56.020
normal person to begin with. Um Is it gonna

1326
01:09:56.029 --> 01:09:59.839
be the sort of depoliticized day to day, highly

1327
01:09:59.850 --> 01:10:03.399
reliable workmanlike research of a scientist? No, it's probably

1328
01:10:03.410 --> 01:10:05.470
gonna be the sort of research, the sort of

1329
01:10:05.479 --> 01:10:07.560
thing that would make waves, especially in the angry

1330
01:10:07.569 --> 01:10:10.350
Clickbait era. The sort of research that is gonna

1331
01:10:10.359 --> 01:10:12.220
make its way to your consciousness as a normal

1332
01:10:12.229 --> 01:10:14.899
person probably is going to be research that is

1333
01:10:14.910 --> 01:10:16.979
political, that is likely to be in dispute, that

1334
01:10:16.990 --> 01:10:19.589
is likely to be controversial in some way and

1335
01:10:19.600 --> 01:10:22.259
that people will think is relevant to political questions.

1336
01:10:22.270 --> 01:10:28.560
So it's not, you know, it's not clear to

1337
01:10:28.569 --> 01:10:32.750
me that, you know, the evidentiary value of the

1338
01:10:32.759 --> 01:10:35.640
missives that a normal person gets from above from

1339
01:10:35.649 --> 01:10:38.140
the experts is gonna be very high to begin

1340
01:10:38.149 --> 01:10:41.109
with, right? Because we don't know where they came

1341
01:10:41.120 --> 01:10:43.680
from. We don't know what the state of the

1342
01:10:43.689 --> 01:10:46.149
debate behind them is. We don't know exactly how

1343
01:10:46.160 --> 01:10:48.540
to make sense of them and the incentives that

1344
01:10:48.549 --> 01:10:51.240
lead them uh to make their way to us

1345
01:10:51.250 --> 01:10:54.089
are probably not great. So my, my general view

1346
01:10:54.100 --> 01:10:56.459
is that, um and this is all apart from

1347
01:10:56.470 --> 01:10:58.410
the question of whether we can even make notion

1348
01:10:58.419 --> 01:11:00.209
of the sense of we make sense of the

1349
01:11:00.220 --> 01:11:02.979
notion of a political expert, right? Um And whether

1350
01:11:02.990 --> 01:11:05.180
there are moral experts and things like that. So

1351
01:11:05.189 --> 01:11:07.899
my sense is that it's, it's very difficult um

1352
01:11:08.189 --> 01:11:10.310
Even if one wanted to trust the experts, it's

1353
01:11:10.319 --> 01:11:13.779
difficult to find them. Um And it's difficult to

1354
01:11:13.790 --> 01:11:16.790
interpret uh their sort of signs and symbols.

1355
01:11:17.049 --> 01:11:20.740
So in the book, you also go through some

1356
01:11:20.750 --> 01:11:26.080
political issues that people worry about like polarization, conspiracy

1357
01:11:26.089 --> 01:11:31.919
theories, fake news. Uh Let's talk perhaps about conspiracy

1358
01:11:31.930 --> 01:11:36.259
theories. You alluded to that earlier in our conversation.

1359
01:11:36.270 --> 01:11:39.970
So, um what would you have to say from

1360
01:11:39.979 --> 01:11:44.569
an epistemological standpoint about conspiracy theories?

1361
01:11:45.430 --> 01:11:49.390
Yeah. So I think conspiracy theories um are difficult

1362
01:11:49.399 --> 01:11:53.100
for epistemology. It, it seems to, I think to

1363
01:11:53.109 --> 01:11:54.950
a lot of epistemology, it seems like there must

1364
01:11:54.959 --> 01:11:58.700
be something wrong with, with conspiracy theories. Um But

1365
01:11:58.709 --> 01:12:01.060
it can be very hard to characterize exactly what

1366
01:12:01.240 --> 01:12:03.490
and in fact, it can be a little difficult

1367
01:12:03.500 --> 01:12:07.450
to even characterize what a conspiracy theory is. So

1368
01:12:08.250 --> 01:12:13.000
for example, uh in Ancient Rome, there is a

1369
01:12:13.009 --> 01:12:16.450
conspiracy that resulted in Julius Caesar being killed, right?

1370
01:12:17.669 --> 01:12:21.490
Believing that doesn't seem to make me a conspiracy

1371
01:12:21.500 --> 01:12:25.509
theorist. Um Even though I believe in the conspiracy,

1372
01:12:26.310 --> 01:12:29.419
maybe because it's become established fact or maybe because

1373
01:12:29.430 --> 01:12:31.810
it's so long ago or something like that, then

1374
01:12:31.819 --> 01:12:34.290
you also have these non-political conspiracies. So when I

1375
01:12:34.299 --> 01:12:39.439
taught a class, what were they? Um THERE'S a

1376
01:12:39.450 --> 01:12:42.790
famous rapper from the 19 nineties, Tupac. So one

1377
01:12:42.799 --> 01:12:44.839
of the conspiracy theories somebody came up with was

1378
01:12:44.850 --> 01:12:49.299
that Tupac was still alive. Um So that was

1379
01:12:49.310 --> 01:12:51.359
their notion of a conspiracy theory is that, you

1380
01:12:51.370 --> 01:12:54.259
know, is that a conspiracy? Well, I guess it

1381
01:12:54.270 --> 01:12:56.250
depends on how many people know and how, you

1382
01:12:56.259 --> 01:12:58.279
know, how they, they were involved with him. So

1383
01:12:58.290 --> 01:12:59.439
I think it, it can be a little bit

1384
01:12:59.450 --> 01:13:03.799
hard to get a handle on exactly which phenomenon

1385
01:13:03.810 --> 01:13:06.049
we're talking about. When we talk about conspiracy theories,

1386
01:13:06.279 --> 01:13:09.520
there is something that obviously seem like they're just

1387
01:13:09.529 --> 01:13:12.140
like core instances of the phenomenon of conspiracy theories.

1388
01:13:12.149 --> 01:13:15.370
And I do think that sometimes, you know, there's

1389
01:13:15.379 --> 01:13:17.910
basically two approaches when you have trouble defining something

1390
01:13:18.319 --> 01:13:21.029
you can say, well, let's work through all the

1391
01:13:21.040 --> 01:13:24.609
examples and counter examples unless you know, we really

1392
01:13:24.620 --> 01:13:26.640
have to make sure should this be in, should

1393
01:13:26.649 --> 01:13:28.470
this not be in, in other words to just

1394
01:13:28.479 --> 01:13:30.649
say, well, let's just talk about there are some

1395
01:13:30.660 --> 01:13:33.540
things that are obviously like core instances, the phenomenon,

1396
01:13:33.549 --> 01:13:35.859
let's just talk about them, right? So that's something

1397
01:13:35.870 --> 01:13:37.459
we could have done when we were worrying about

1398
01:13:37.470 --> 01:13:39.910
the definition of politics, we could have said. Well,

1399
01:13:39.919 --> 01:13:42.279
let's just think about the things everybody that must

1400
01:13:42.290 --> 01:13:44.879
be politics, you know, going to war, you know,

1401
01:13:45.470 --> 01:13:47.100
signing a bill that says we're going to go

1402
01:13:47.109 --> 01:13:50.250
to war, right? When the Republican and Democrats disagree

1403
01:13:50.259 --> 01:13:52.520
about it or something like that. So with conspiracy

1404
01:13:52.529 --> 01:13:55.319
theories, you know, the core ones, so take in

1405
01:13:55.330 --> 01:14:00.600
America, the Qanon theory, um this is, you know,

1406
01:14:01.000 --> 01:14:03.259
ok, I'm forgetting what it is. I think it

1407
01:14:03.270 --> 01:14:06.370
involves a guy called, you call himself Q who

1408
01:14:06.379 --> 01:14:08.830
claims to have some sort of inside source on

1409
01:14:08.839 --> 01:14:11.529
government information, maybe claims to be a government agent

1410
01:14:11.540 --> 01:14:15.049
himself who makes these online posts that says that,

1411
01:14:15.189 --> 01:14:17.060
you know, here's what's going on within the government,

1412
01:14:17.069 --> 01:14:19.379
here's what the information drop is gonna be about.

1413
01:14:19.890 --> 01:14:23.100
You know, the abuses people are doing or Hillary

1414
01:14:23.109 --> 01:14:25.200
Clinton is doing that or Bill Clinton is doing

1415
01:14:25.209 --> 01:14:28.919
this. Um And uh you know, a lot of

1416
01:14:28.930 --> 01:14:30.629
people are covering it up, you know, a lot

1417
01:14:30.640 --> 01:14:32.359
of the government is implicated or a lot of

1418
01:14:32.370 --> 01:14:34.919
the powerful people are implicated and they're doing this

1419
01:14:34.930 --> 01:14:37.279
for their own advantage. I mean, sometimes for a

1420
01:14:37.290 --> 01:14:39.819
very nefarious kind of advantage, right? Maybe, maybe to

1421
01:14:39.830 --> 01:14:42.169
abuse Children or something. So that obviously seems like

1422
01:14:42.180 --> 01:14:45.799
a conspiracy theory that posits a conspiracy, the conspiracy

1423
01:14:45.810 --> 01:14:51.290
goes against um the, the official story of the

1424
01:14:51.299 --> 01:14:56.049
US government and of US power. Um It's uh

1425
01:14:56.560 --> 01:14:58.129
you know, a lot of people are, are acting

1426
01:14:58.140 --> 01:15:00.339
badly, right. They're, they're doing it for nefarious or

1427
01:15:00.350 --> 01:15:05.350
evil purposes. Um They're doing it secretly, they're doing

1428
01:15:05.359 --> 01:15:07.640
it through secret mechanisms, things like that. So that

1429
01:15:07.649 --> 01:15:10.959
seems like a clear case of conspiracy theory. Um

1430
01:15:11.810 --> 01:15:13.970
On the other hand, if we look at those

1431
01:15:13.979 --> 01:15:17.169
aspects, it's hard to say which of those renders

1432
01:15:17.180 --> 01:15:23.549
the belief irrational. Because certainly believing that people act

1433
01:15:23.560 --> 01:15:26.600
in evil ways for their own interests is not

1434
01:15:26.609 --> 01:15:31.140
by itself irrational. Believing that people act in secret

1435
01:15:31.149 --> 01:15:36.640
is not by itself irrational. Believing that powerful people

1436
01:15:36.649 --> 01:15:39.160
can act in concert with each other, can coordinate

1437
01:15:39.169 --> 01:15:44.700
their actions that's not irrational. Um And believing that

1438
01:15:44.709 --> 01:15:48.189
sometimes your government is wrong, that doesn't seem irrational

1439
01:15:48.200 --> 01:15:52.850
either, right? Uh So once we come up with

1440
01:15:52.859 --> 01:15:55.990
it, even in the core case, once we say

1441
01:15:56.000 --> 01:15:58.189
these are the things that seem to have contributed

1442
01:15:58.370 --> 01:16:01.750
to it being a conspiracy theory, it's hard to

1443
01:16:01.759 --> 01:16:04.370
say which of these is really the problem. Uh

1444
01:16:04.379 --> 01:16:07.930
WHERE, where did these people really go wrong? Um

1445
01:16:08.669 --> 01:16:11.569
No, of course. So that's the problem with generalism,

1446
01:16:11.580 --> 01:16:13.209
right? Generalism is sort of like we try to

1447
01:16:13.220 --> 01:16:15.189
identify the aspects of the conspiracy theory and we

1448
01:16:15.200 --> 01:16:17.959
say one of this just is irrational. Now, particular

1449
01:16:17.970 --> 01:16:19.459
is, is the approach where you look at a

1450
01:16:19.470 --> 01:16:22.089
specific conspiracy theory and you say, well, they just

1451
01:16:22.100 --> 01:16:23.879
don't have very good evidence for this. No, of

1452
01:16:23.890 --> 01:16:25.089
course, that's probably true. In a lot of the

1453
01:16:25.100 --> 01:16:27.439
case, obviously in Qanon, it's true in the case

1454
01:16:27.450 --> 01:16:31.220
of Qanon is true. But that's something where you

1455
01:16:31.229 --> 01:16:35.250
can just say, well, there's this guy saying some

1456
01:16:35.259 --> 01:16:38.939
stuff is gonna happen and mostly it doesn't happen.

1457
01:16:38.950 --> 01:16:42.370
You only have one source, uh, everybody else, you

1458
01:16:42.379 --> 01:16:44.209
know, disagrees with the guy. So you sort of

1459
01:16:44.220 --> 01:16:47.990
have, you know, in general, you know, if 100

1460
01:16:48.000 --> 01:16:49.810
people say one thing and one person said another

1461
01:16:49.819 --> 01:16:52.560
thing in general, you go with 100 people, right?

1462
01:16:52.979 --> 01:16:57.759
Um So in the case of qanon, you know,

1463
01:16:58.259 --> 01:17:00.870
there's gonna be specific things where you can look

1464
01:17:00.879 --> 01:17:02.790
at here are the dynamics of the theory. Here

1465
01:17:02.799 --> 01:17:05.600
are the dynamics of how people are acquainted with

1466
01:17:05.609 --> 01:17:08.859
the theory. And those dynamics don't suffice to make

1467
01:17:08.870 --> 01:17:12.229
a belief in irrational. Um But those dynamics will

1468
01:17:12.240 --> 01:17:13.779
be different from one theory to another, right? They'll

1469
01:17:13.790 --> 01:17:15.970
be different for different conspiracy theories, things that interest

1470
01:17:15.979 --> 01:17:18.290
me. Um And this sort of came up with

1471
01:17:18.299 --> 01:17:22.609
the, the notion of ideology. Uh There's this way

1472
01:17:22.620 --> 01:17:25.680
of people, people will take kind of conspiratorial beliefs

1473
01:17:26.240 --> 01:17:28.770
and make them, they'll remove this sort of individual

1474
01:17:28.779 --> 01:17:32.160
agents from them and just say, well, the conspiracy

1475
01:17:32.169 --> 01:17:34.709
is sort of like internalized or in people's heads

1476
01:17:34.720 --> 01:17:39.379
somehow. Um Nobody is intentionally doing this, it just

1477
01:17:39.390 --> 01:17:42.790
sort of emerges um out of the structure of

1478
01:17:42.799 --> 01:17:44.959
society, the conspiracy emerges out of the structure of

1479
01:17:44.970 --> 01:17:49.609
society. Um THAT I've never really understood why that's

1480
01:17:49.620 --> 01:17:53.100
any different than a normal, I mean, individual agency,

1481
01:17:53.109 --> 01:17:55.029
people always act with individual, you know, whether they

1482
01:17:55.040 --> 01:17:57.299
act with individual agency or don't, you know, like

1483
01:17:57.390 --> 01:17:59.450
that doesn't seem like something that licenses or, or,

1484
01:17:59.459 --> 01:18:02.160
or changes the rationality of, of a conspiracy theory

1485
01:18:02.169 --> 01:18:05.450
to me. Um So there's two types of explanation.

1486
01:18:05.459 --> 01:18:06.709
It doesn't seem to me that one of them

1487
01:18:06.720 --> 01:18:09.220
is gonna be, you know, if one of them

1488
01:18:09.229 --> 01:18:10.990
is super irrational, the other one probably will be

1489
01:18:11.000 --> 01:18:15.140
too. So basically, and I think this is the

1490
01:18:15.149 --> 01:18:19.279
philosophical consensus right now. Um Basically, we have to

1491
01:18:19.290 --> 01:18:22.049
look at conspiracy theories on their merits. Um Often

1492
01:18:22.060 --> 01:18:24.330
they don't have, you know, often they're not meritorious

1493
01:18:24.339 --> 01:18:32.129
belief, um rationally, not epidemically meritorious. Um But it's

1494
01:18:32.140 --> 01:18:33.799
gonna be hard to say something very general about

1495
01:18:33.810 --> 01:18:35.549
what's wrong with conspiracy theorizing.

1496
01:18:36.310 --> 01:18:39.100
So let me ask you ju uh about just

1497
01:18:39.109 --> 01:18:42.709
one last topic. So, uh because there are people

1498
01:18:42.720 --> 01:18:45.589
that make claims about how the history of a

1499
01:18:45.600 --> 01:18:50.569
particular society might also influence people's political beliefs. So

1500
01:18:51.220 --> 01:18:54.080
does that happen? I mean, does the history of

1501
01:18:54.089 --> 01:18:58.410
our society that society have any bearing on our

1502
01:18:58.419 --> 01:18:59.609
political beliefs?

1503
01:19:00.870 --> 01:19:06.009
Yeah, absolutely. Um Yeah. So I think in the

1504
01:19:06.020 --> 01:19:11.359
most general sense, um the way our societies develop

1505
01:19:11.370 --> 01:19:17.959
are going, you know, are going to affect social

1506
01:19:17.970 --> 01:19:22.000
circumstances. Um AND they're going to affect kind of

1507
01:19:22.009 --> 01:19:26.350
traditions, as well as intellectual traditions, there's gonna be

1508
01:19:26.359 --> 01:19:30.049
received wisdom and things like that. Um One thing

1509
01:19:30.060 --> 01:19:32.200
I found interesting in writing this book was the

1510
01:19:32.209 --> 01:19:38.819
sort of different uh views. Once I recognized the

1511
01:19:38.830 --> 01:19:44.910
tradition was part of political epistemology. Um It struck

1512
01:19:44.919 --> 01:19:51.490
me that, you know, different political groups are characterized

1513
01:19:51.500 --> 01:19:55.549
in part by different attitudes towards tradition. Um So

1514
01:19:55.560 --> 01:19:58.279
obviously, conservatives are usually bigger fans of tradition and

1515
01:19:58.290 --> 01:20:02.350
progressives, not so much, right? Um In a way

1516
01:20:02.359 --> 01:20:04.810
it's in the name, right? So progressives think we

1517
01:20:04.819 --> 01:20:07.089
make progress and conservatives want to keep what we've

1518
01:20:07.100 --> 01:20:11.180
already got, right? Um uh And neither, you know,

1519
01:20:11.189 --> 01:20:13.759
without saying more, neither of those seem bad, right?

1520
01:20:13.770 --> 01:20:15.450
What, what if we could make progress while also

1521
01:20:15.459 --> 01:20:17.009
keeping the good things we already have, that would

1522
01:20:17.020 --> 01:20:19.990
be insanely good, right? Um That would make for

1523
01:20:20.000 --> 01:20:24.509
a really great society. Um But so there's a

1524
01:20:24.520 --> 01:20:27.470
few different ways of looking at. So from the

1525
01:20:27.479 --> 01:20:31.680
progressive side, there's this fact that I think to

1526
01:20:31.689 --> 01:20:33.430
a lot of people at least seems difficult to,

1527
01:20:33.439 --> 01:20:38.060
to dispute this fact of moral progress. So the

1528
01:20:38.069 --> 01:20:42.100
fact that for almost all of human history, um

1529
01:20:42.839 --> 01:20:47.080
there wasn't, it wasn't taken as obvious that you

1530
01:20:47.089 --> 01:20:50.930
shouldn't own slaves. Um Of course, in America, we

1531
01:20:50.939 --> 01:20:54.160
had a chattel slavery which was racialized, um a

1532
01:20:54.169 --> 01:20:57.339
war which was just, you know, based on race

1533
01:20:57.350 --> 01:21:01.270
or whatever the right word is. Um But, you

1534
01:21:01.279 --> 01:21:03.689
know, even in other, you know, there were Aristotle

1535
01:21:03.700 --> 01:21:07.240
root in favor of slavery um or defended slavery.

1536
01:21:07.689 --> 01:21:09.740
Uh You had, you had slavery. And basically, you

1537
01:21:09.750 --> 01:21:13.459
know, my understanding is most civilizations um for most

1538
01:21:13.470 --> 01:21:16.950
of human history. Um So we've made progress by

1539
01:21:16.959 --> 01:21:21.549
recognizing that slavery in some sense is just an

1540
01:21:21.560 --> 01:21:25.709
extreme affront to morality and dignity and things like

1541
01:21:25.720 --> 01:21:31.549
that. Um And so the progressive view might be,

1542
01:21:31.560 --> 01:21:36.229
well as time goes on, we'll keep having, you

1543
01:21:36.240 --> 01:21:39.660
know, we'll keep figuring out what's right and wrong,

1544
01:21:39.669 --> 01:21:41.759
you know, we'll keep making progress with that, we'll

1545
01:21:41.770 --> 01:21:46.149
keep figuring it out. Um And uh on the

1546
01:21:46.160 --> 01:21:51.830
other side, you have some arguments about uh why

1547
01:21:51.839 --> 01:21:56.470
we shouldn't change things so much. Now, 11 argument.

1548
01:21:56.609 --> 01:21:58.950
So GK Chesterton has a couple arguments, you know,

1549
01:21:58.959 --> 01:22:02.259
he's a great British traditionalist, um or conservative or

1550
01:22:02.270 --> 01:22:05.350
whatever the right word is uh defender of tradition.

1551
01:22:05.810 --> 01:22:07.850
One argument is the, the idea of the democracy

1552
01:22:07.859 --> 01:22:11.620
of the dead, basically. Like, you know, if you

1553
01:22:11.629 --> 01:22:14.129
believe in democracy and believe that adding people makes

1554
01:22:14.140 --> 01:22:17.649
things better. Why not, why not include the perspectives

1555
01:22:17.660 --> 01:22:19.459
of people who have, who have already come before

1556
01:22:19.470 --> 01:22:22.640
us, right? Um And another is this idea of

1557
01:22:22.649 --> 01:22:25.770
Chesterton's fence. So he says, you know, before you

1558
01:22:25.779 --> 01:22:27.790
destroy something in society, you have to make sure

1559
01:22:27.799 --> 01:22:30.609
you understand why it's there, right? Um The people

1560
01:22:30.620 --> 01:22:33.529
who built it know why they built it. And

1561
01:22:33.540 --> 01:22:35.100
if you don't know why it's there, that means

1562
01:22:35.109 --> 01:22:37.490
that they have the advantage over you, right? That

1563
01:22:37.500 --> 01:22:39.359
means you should trust them because they're an expert

1564
01:22:39.370 --> 01:22:42.290
on experts on why it was built then in

1565
01:22:42.299 --> 01:22:44.859
a slightly different way. Friedrich Hayek uh has the

1566
01:22:44.870 --> 01:22:47.359
same view. I mean, Hayek believes that nobody, even

1567
01:22:47.370 --> 01:22:48.899
the people who built it probably don't understand why

1568
01:22:48.910 --> 01:22:51.310
it was there. Um So Hayek has a more

1569
01:22:51.319 --> 01:22:54.399
sort of invisible hand evolutionary type approach where he

1570
01:22:54.410 --> 01:22:57.339
says, you know, just as no one person can

1571
01:22:57.350 --> 01:22:59.169
understand why an item is priced the way it

1572
01:22:59.180 --> 01:23:02.120
is. No one person can understand why a social

1573
01:23:02.129 --> 01:23:04.259
structure is what it is and what functions it

1574
01:23:04.270 --> 01:23:08.379
might start. So, just like, um you know, just

1575
01:23:08.390 --> 01:23:09.979
like the theories of ideology, he talks about the

1576
01:23:09.990 --> 01:23:14.339
functional role that things play in society. Um But

1577
01:23:14.350 --> 01:23:16.240
he thinks that those functions can be very obscure

1578
01:23:16.250 --> 01:23:19.729
to us. Um And so in terms of resolving,

1579
01:23:19.740 --> 01:23:23.910
you know, resolving these disputes about political epistemology between

1580
01:23:23.919 --> 01:23:27.450
um progressives and traditionalists. Well, I, I don't think

1581
01:23:27.459 --> 01:23:29.379
it's, you know, I, I don't think it's something

1582
01:23:29.790 --> 01:23:35.330
um that, that has a final resolution um necessarily,

1583
01:23:35.339 --> 01:23:37.520
or at least not one that I've found. But

1584
01:23:37.529 --> 01:23:39.120
I have, you know, I did start to wonder

1585
01:23:39.129 --> 01:23:40.799
about this notion of moral progress a little bit

1586
01:23:40.810 --> 01:23:43.640
as I was writing and I started to wonder

1587
01:23:43.770 --> 01:23:47.640
why, you know, OK, so I know obviously we're

1588
01:23:47.649 --> 01:23:52.790
doing better now from my perspective than we were

1589
01:23:52.919 --> 01:23:55.120
back when people didn't realize how bad slavery was,

1590
01:23:55.129 --> 01:23:58.299
right. We've made progress in figuring out the slavery

1591
01:23:59.379 --> 01:24:02.240
is bad. And then maybe in 100 years there

1592
01:24:02.250 --> 01:24:04.229
will be another change where people say this thing

1593
01:24:04.240 --> 01:24:05.700
is good or this thing is bad and they'll

1594
01:24:05.709 --> 01:24:10.250
disagree with me. One thing I didn't quite understand

1595
01:24:10.310 --> 01:24:13.759
when I was thinking about moral progress is why,

1596
01:24:13.770 --> 01:24:16.240
you know, what is it about 100 years passing?

1597
01:24:17.270 --> 01:24:19.490
That should make me think they're right and I'm

1598
01:24:19.500 --> 01:24:22.290
wrong. Why shouldn't I say? Well, I'm the high

1599
01:24:22.299 --> 01:24:25.020
point, I'm right about everything people in the past

1600
01:24:25.029 --> 01:24:27.279
were wrong about slavery, then it'll change again and

1601
01:24:27.290 --> 01:24:28.669
people in the future will be wrong about this

1602
01:24:28.680 --> 01:24:30.240
other thing, but I can stick with my own

1603
01:24:30.250 --> 01:24:34.140
beliefs, right? Um And so I was trying to

1604
01:24:34.149 --> 01:24:36.120
figure out why that was one is just maybe

1605
01:24:36.129 --> 01:24:37.990
you just infer from the fact that progress has

1606
01:24:38.000 --> 01:24:41.669
been made so far. Um The notion that it

1607
01:24:41.680 --> 01:24:43.919
will continue to be made, um That seems like

1608
01:24:43.930 --> 01:24:45.609
a strong, you know, that seems like an unlikely

1609
01:24:45.620 --> 01:24:48.189
inference. Yeah. So I, I'm still thinking about this

1610
01:24:48.200 --> 01:24:51.620
question of moral progress. Um A lot. Uh And

1611
01:24:51.629 --> 01:24:55.779
there's, you know, there's different areas, uh there's different

1612
01:24:55.790 --> 01:24:58.620
areas of political life where, where, where the notion

1613
01:24:58.629 --> 01:25:00.649
of moral progress is relevant. II, I really think

1614
01:25:00.660 --> 01:25:04.459
it's um it's an under discussed notion, uh the

1615
01:25:04.470 --> 01:25:08.069
notion of progress. Uh There's questions of whether morality

1616
01:25:08.080 --> 01:25:10.529
really works like science, what, what assumptions do you

1617
01:25:10.540 --> 01:25:13.250
need to make about morality and politics? To even

1618
01:25:13.390 --> 01:25:17.060
think that there's an analogy um between moral or

1619
01:25:17.069 --> 01:25:20.240
political progress and scientific progress. Um Probably some pretty

1620
01:25:20.250 --> 01:25:23.009
robust assumptions. Uh So that I'm glad you asked

1621
01:25:23.020 --> 01:25:25.609
about that. That is, you know, so the connection

1622
01:25:25.620 --> 01:25:27.899
between political epistemology and history probably of everything in

1623
01:25:27.910 --> 01:25:30.689
the book. Um It is the thing that has

1624
01:25:30.700 --> 01:25:33.549
been that that's like completely new. Um I, I

1625
01:25:33.560 --> 01:25:36.430
don't think anybody has discussed that before. Um So

1626
01:25:36.439 --> 01:25:37.919
I'm excited to see what people think of it

1627
01:25:37.930 --> 01:25:40.720
at least. But uh yes, I appreciate you asking

1628
01:25:40.729 --> 01:25:44.620
about it. Great. So the book is again, political

1629
01:25:44.629 --> 01:25:48.200
beliefs of philosophical introduction. And I'm leaving a link

1630
01:25:48.209 --> 01:25:51.100
to it in the description of the interview and

1631
01:25:51.109 --> 01:25:54.140
Oliver just before we go apart from the book,

1632
01:25:54.149 --> 01:25:56.259
would you like to tell people where they can

1633
01:25:56.270 --> 01:25:58.899
find you when your work on the internet?

1634
01:26:00.240 --> 01:26:02.459
Yeah. So I keep a site on Weebly uh

1635
01:26:02.470 --> 01:26:05.339
Oliver tral.weebly.com. You can find, I do a lot

1636
01:26:05.350 --> 01:26:06.979
of public writing so you can find my book

1637
01:26:06.990 --> 01:26:09.459
reviews and, and cultural essays and things like that

1638
01:26:09.470 --> 01:26:13.189
there. Um I have a Twitter account but usually,

1639
01:26:13.200 --> 01:26:15.359
you know, I'm not very good at Twitter so

1640
01:26:15.370 --> 01:26:17.549
um people can find me there if they like.

1641
01:26:17.560 --> 01:26:23.359
Um uh And yeah, um I appreciate you having

1642
01:26:23.370 --> 01:26:23.830
me on.

1643
01:26:24.310 --> 01:26:26.870
No, of course. I really love the book. So

1644
01:26:26.879 --> 01:26:29.100
thank you so much for taking the time to

1645
01:26:29.109 --> 01:26:30.870
come on the show. It's been fun to talk

1646
01:26:30.879 --> 01:26:35.009
with you. Yeah. Thank you. Hi guys. Thank you

1647
01:26:35.020 --> 01:26:37.189
for watching this interview. Until the end. If you

1648
01:26:37.200 --> 01:26:39.589
liked it, please share it. Leave a like and

1649
01:26:39.600 --> 01:26:42.140
hit the subscription button. The show is brought to

1650
01:26:42.149 --> 01:26:45.200
you by N Lights learning and development. Then differently

1651
01:26:45.209 --> 01:26:48.430
check the website at N lights.com and also please

1652
01:26:48.439 --> 01:26:52.520
consider supporting the show on Patreon or paypal. I

1653
01:26:52.529 --> 01:26:54.500
would also like to give a huge thank you

1654
01:26:54.509 --> 01:26:58.529
to my main patrons and paypal supporters, Perego Larson,

1655
01:26:58.540 --> 01:27:01.410
Jerry Muller and Frederick Suno Bernard Seche O of

1656
01:27:01.439 --> 01:27:04.439
Alex Adam Castle Matthew Whitting Bear. No wolf, Tim

1657
01:27:04.450 --> 01:27:08.169
Ho Erica LJ Connors, Philip Forrest Connelly. Then the

1658
01:27:08.180 --> 01:27:11.140
Met Robert Wine in NAI Z Mar Nevs calling

1659
01:27:11.149 --> 01:27:15.229
in Hobel Governor Mikel Stormer Samuel Andre Francis for

1660
01:27:15.959 --> 01:27:18.970
Agns Ferger Ken Hall, her ma J and Lain

1661
01:27:19.169 --> 01:27:22.169
Jung Y and the Samuel K Hes Mark Smith

1662
01:27:22.240 --> 01:27:26.979
J. Tom Hummel s friends, David Sloan Wilson, Yaar

1663
01:27:27.020 --> 01:27:31.799
Roman. Roach Diego, Jan Punter, Romani Charlotte, Bli Nicole

1664
01:27:31.859 --> 01:27:35.629
Barba, Adam Hunt, Pavlo, Stassi, Nale Me, Gary G

1665
01:27:35.839 --> 01:27:39.750
Alman, Samo, Zal Ari and YPJ Barboza Julian Price

1666
01:27:40.009 --> 01:27:44.350
Edward Hall, Eden Broner Douglas Fry Franka La Gilon

1667
01:27:44.779 --> 01:27:51.859
Cortez or Scott Zachary ftdw Daniel Friedman, William Buckner,

1668
01:27:51.870 --> 01:27:55.910
Paul Giorgio, Luke Loki, Georgio Theophano, Chris Williams and

1669
01:27:55.919 --> 01:28:00.540
Peter Wo David Williams, the Ausa Anton Erickson, Charles

1670
01:28:00.549 --> 01:28:05.899
Murray, Alex Shaw, Marie Martinez, Coralie Chevalier, Bangalore Larry

1671
01:28:06.169 --> 01:28:10.640
Dey Junior, Old Ebon Starry Michael Bailey. Then Spur

1672
01:28:10.649 --> 01:28:14.939
by Robert Grassy Zorn, Jeff mcmahon, Jake Zul Barnabas

1673
01:28:14.959 --> 01:28:18.669
Radick Mark Temple, Thomas Dvor Luke Neeson, Chris Tory

1674
01:28:18.680 --> 01:28:22.390
Kimberley Johnson. Benjamin Gilbert Jessica week in the B

1675
01:28:22.399 --> 01:28:27.870
brand Nicholas Carlson Ismael Bensley Man, George Katis, Valentine

1676
01:28:27.879 --> 01:28:33.560
Steinman, Perros, Kate Van Goler, Alexander, Abert Liam Dan

1677
01:28:33.919 --> 01:28:39.200
Biar Masoud Ali Mohammadi Perpendicular J Ner Urla. Good

1678
01:28:39.529 --> 01:28:43.359
enough, Gregory Hastings David Pins of Shan Nelson, Mike

1679
01:28:43.370 --> 01:28:46.879
Levin and Jos Net. A special thanks to my

1680
01:28:46.890 --> 01:28:49.919
producers is our web, Jim Frank Luca Toni, Tom

1681
01:28:49.959 --> 01:28:54.020
Ween, Bernard N Cortes Dixon Bendik Muller Thomas Trumble,

1682
01:28:54.299 --> 01:28:57.870
Catherine and Patrick Tobin, John Carl, Negro, Nick Ortiz

1683
01:28:57.879 --> 01:29:00.890
and Nick Golden. And to my executive producers, Matthew

1684
01:29:00.899 --> 01:29:04.500
Lavender, Si Adrian Bogdan Knits and Rosie. Thank you

1685
01:29:04.509 --> 01:29:04.970
for all.

