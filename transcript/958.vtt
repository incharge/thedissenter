WEBVTT

1
00:00:00.100 --> 00:00:02.819
Hello everybody. Welcome to a new episode of the

2
00:00:02.940 --> 00:00:05.579
Center. I'm your host, Ricard Lobs. And today I'm

3
00:00:05.590 --> 00:00:08.859
joined by Doctor Robert mckennon. He is senior lecturer

4
00:00:08.869 --> 00:00:11.960
in Philosophy at the University of Liverpool. And today

5
00:00:11.970 --> 00:00:16.209
we're talking about his book, Non Ideal Epistemology. So

6
00:00:16.219 --> 00:00:18.739
Robin, welcome to the show. It's a pleasure to

7
00:00:18.750 --> 00:00:19.200
everyone.

8
00:00:19.729 --> 00:00:21.110
Yeah, nice to be here. So I noticed that

9
00:00:21.120 --> 00:00:23.989
you've actually covered this confusion in your intro, Ricardo,

10
00:00:24.000 --> 00:00:26.489
you call me Robert first and then Robin, this

11
00:00:26.500 --> 00:00:29.459
is a constant problem. My official legal name is

12
00:00:29.469 --> 00:00:32.650
Robert, but everyone calls me Robin. I publish under

13
00:00:32.659 --> 00:00:36.060
Robin, but my university refuses to call me Robin.

14
00:00:36.069 --> 00:00:38.069
I have to be Robert and all the official

15
00:00:38.080 --> 00:00:40.069
system. So, yeah, as I said, endless cause of

16
00:00:40.080 --> 00:00:42.549
confusion um throughout my entire life.

17
00:00:43.169 --> 00:00:46.950
OK. So let's get into it then. So we're

18
00:00:46.959 --> 00:00:51.130
talking about epistemology today and particularly about non ideal

19
00:00:51.139 --> 00:00:54.029
epistemology. But I guess that before we get into

20
00:00:54.040 --> 00:00:57.279
what non ideal epistemology is, we should talk a

21
00:00:57.290 --> 00:01:03.174
little bit more broadly about what epistemology usually do

22
00:01:03.275 --> 00:01:07.614
and what ideal theory, the opposite of non ideal

23
00:01:07.625 --> 00:01:10.455
really is. So let's start with that. W what

24
00:01:10.464 --> 00:01:14.495
kinds of what are the the most common approaches

25
00:01:14.504 --> 00:01:20.334
in epistemology and uh how do epistemology usually approach

26
00:01:20.485 --> 00:01:21.535
questions?

27
00:01:22.519 --> 00:01:24.019
Yeah, that's a good question. I mean, I know

28
00:01:24.029 --> 00:01:26.129
that if you go online, you find all sorts

29
00:01:26.139 --> 00:01:29.489
of kind of nice, simple definitions of epistemology. Like

30
00:01:29.500 --> 00:01:32.160
for example, it's the theory of knowledge or, or

31
00:01:32.169 --> 00:01:34.819
the ethics of belief or something like that. But

32
00:01:34.830 --> 00:01:36.540
the thing that really impresses me as someone who

33
00:01:36.550 --> 00:01:40.300
works in epistemology is that epistemology do lots of

34
00:01:40.309 --> 00:01:43.120
different things and they ask lots of different kinds

35
00:01:43.129 --> 00:01:46.870
of questions. So some epistemology ask what you might

36
00:01:46.889 --> 00:01:49.214
call or like what is questions like what is

37
00:01:49.224 --> 00:01:52.955
knowledge, what is evidence, what is justified belief? Um

38
00:01:53.074 --> 00:01:54.654
A lot of the time to answer them, they

39
00:01:54.665 --> 00:01:57.675
kind of do conceptual analysis, the kind of traditional

40
00:01:57.684 --> 00:01:59.555
method of philosophy if you like, but you don't

41
00:01:59.565 --> 00:02:01.785
have to do that. You can answer those questions

42
00:02:01.794 --> 00:02:05.535
using different kinds of methods. Um Those aren't the

43
00:02:05.544 --> 00:02:07.944
kinds of kinds of questions I'm particularly interested in,

44
00:02:07.955 --> 00:02:11.264
at least in the book anyway, perhaps another moods

45
00:02:11.274 --> 00:02:12.835
of interested in them but not in the book.

46
00:02:12.845 --> 00:02:17.279
I'm more interested in kind of broadly normative questions

47
00:02:18.100 --> 00:02:22.009
about knowledge, but also about belief, questions such as

48
00:02:22.020 --> 00:02:24.929
how should I go about foreign belief, but also

49
00:02:24.940 --> 00:02:27.899
questions like how should I go about gathering evidence?

50
00:02:28.070 --> 00:02:31.300
I guess most broadly, how should I go about

51
00:02:31.309 --> 00:02:34.740
conducting my inquiries and then a couple that as

52
00:02:34.750 --> 00:02:39.904
many other epistemology do with interest in social dimensions

53
00:02:39.914 --> 00:02:43.014
of inquiry. So most inquiries are not just you

54
00:02:43.024 --> 00:02:45.414
by yourself, it's you with your friends or your

55
00:02:45.425 --> 00:02:48.014
colleagues, you're getting information from a wide range of

56
00:02:48.024 --> 00:02:51.404
sources. So the question isn't so much, how should

57
00:02:51.414 --> 00:02:55.554
I inquire? It's rather how should we inquire together?

58
00:02:55.565 --> 00:02:57.945
Um So those are the sorts of personal questions

59
00:02:57.955 --> 00:03:00.794
that I'm interested in these sort of social questions

60
00:03:00.804 --> 00:03:06.630
about how to conduct inquiries. So you asked about

61
00:03:06.639 --> 00:03:11.929
ideal theory in epistemology and what that is. So

62
00:03:11.940 --> 00:03:13.880
one reason why I like putting it in terms

63
00:03:13.889 --> 00:03:16.789
of these broad normative questions is that you can

64
00:03:16.800 --> 00:03:21.570
then distinguish between two different kinds of approach to

65
00:03:21.580 --> 00:03:23.550
them. So one kind of approach and this is

66
00:03:23.559 --> 00:03:27.490
the Ideal Theory approach I would say is you

67
00:03:27.500 --> 00:03:29.559
sort of, I don't know, this is maybe a

68
00:03:29.570 --> 00:03:31.110
bit of a caricature, but it's not too much

69
00:03:31.119 --> 00:03:32.649
of a caricature, you could have sit there and

70
00:03:32.660 --> 00:03:34.600
you think, well, what would a really good inquiry

71
00:03:34.610 --> 00:03:37.539
look like? What sorts of things would our good

72
00:03:37.550 --> 00:03:42.270
inquirers do? Um How would they go about gathering

73
00:03:42.279 --> 00:03:45.110
information on the assumption that lots of the information

74
00:03:45.119 --> 00:03:47.279
that's out there is kind of good quality. How

75
00:03:47.289 --> 00:03:49.960
would they share information on the assumption that they're

76
00:03:49.970 --> 00:03:51.279
not going to be lying to each other all

77
00:03:51.289 --> 00:03:54.770
the time? Um So you're kind of answering these

78
00:03:54.779 --> 00:03:59.020
normative questions about inquiry, communal inquiry in a way

79
00:03:59.029 --> 00:04:01.869
that's in a kind of obvious sense idealized. Um

80
00:04:01.899 --> 00:04:03.970
If you like, you're kind of constructing a model

81
00:04:03.979 --> 00:04:07.729
of what a kind of idealized community of Inquirers

82
00:04:07.740 --> 00:04:10.619
would look like. And when you start to talk

83
00:04:10.630 --> 00:04:13.850
about norms of inquiry, these are things that your

84
00:04:13.860 --> 00:04:17.519
idealized Inquirers would do. The idea then being that,

85
00:04:17.529 --> 00:04:19.750
you know, we're not idealized Inquirer, we've got all

86
00:04:19.760 --> 00:04:22.760
sorts of perfections. But what we should do is

87
00:04:22.769 --> 00:04:26.470
if you like, try to emulate this idealized community

88
00:04:26.480 --> 00:04:29.179
of Inquirers and I guess the better the job

89
00:04:29.190 --> 00:04:32.100
we do of emulating that idealized community, the better

90
00:04:32.109 --> 00:04:35.500
we're doing. So that's the Ideal Theory approach. That's

91
00:04:35.510 --> 00:04:38.149
not as the title of my book suggests. That's

92
00:04:38.160 --> 00:04:41.029
not the approach I favor I favor the non

93
00:04:41.040 --> 00:04:45.329
ideal approach where instead of constructing this idealized model,

94
00:04:45.339 --> 00:04:50.230
we look at actual inquiries. So here, of course,

95
00:04:50.239 --> 00:04:52.839
scientific inquiries are a natural place to look. We

96
00:04:52.920 --> 00:04:56.820
want to look at how actual inquiries work. We

97
00:04:56.829 --> 00:05:00.165
want to defy the actual inquiries that seem to

98
00:05:00.174 --> 00:05:01.845
be getting something right? Uh They seem to be

99
00:05:01.855 --> 00:05:05.364
productive across a few different dimensions. And let me

100
00:05:05.375 --> 00:05:07.255
try to figure out well, what exactly is it

101
00:05:07.265 --> 00:05:10.195
about those inquiries that enable them to work? What

102
00:05:10.204 --> 00:05:11.894
are they doing? It could be that as it

103
00:05:11.904 --> 00:05:16.165
turns out what our actual inquirer uh do in

104
00:05:16.174 --> 00:05:19.304
these well functioning inquiries is quite different from what

105
00:05:19.315 --> 00:05:23.089
the uh idealized inquirers do. So, for example, and

106
00:05:23.100 --> 00:05:25.149
men will have this view, it might turn out

107
00:05:25.290 --> 00:05:28.809
that a degree of dogmatism and closed mindedness is

108
00:05:28.820 --> 00:05:32.390
actually a kind of essential ingredient of successful scientific

109
00:05:32.399 --> 00:05:35.299
inquiry, obviously, you want to be limits on this.

110
00:05:35.309 --> 00:05:38.440
But it may well be the case that um

111
00:05:38.529 --> 00:05:43.160
good scientific Inquirer are rather more dogmatic than the

112
00:05:43.170 --> 00:05:47.440
more idealized versions would be. That's just one example.

113
00:05:47.450 --> 00:05:49.329
But that's kind of basic idea. You can, if

114
00:05:49.339 --> 00:05:51.220
you look at how things actually work and from

115
00:05:51.230 --> 00:05:54.869
that, you try to get a sense of what

116
00:05:54.880 --> 00:05:58.440
sorts of things we should do given the limitations

117
00:05:58.450 --> 00:06:00.399
of the situations which we find ourselves.

118
00:06:01.589 --> 00:06:05.489
This is actually very interesting because I mean, shouldn't

119
00:06:05.500 --> 00:06:10.899
all epistemology from the very beginning be non ideal

120
00:06:10.910 --> 00:06:15.380
in the sense that it should realistically look at

121
00:06:15.390 --> 00:06:22.130
how actual people uh approach questions in epistemology, how

122
00:06:22.140 --> 00:06:28.010
actual people work cognitively and so on be because

123
00:06:28.019 --> 00:06:31.600
I mean, isn't it a bit? Um uh I

124
00:06:31.609 --> 00:06:33.779
don't know, I don't even know the exact word

125
00:06:33.790 --> 00:06:39.480
to use here, but idealizing something and expecting people

126
00:06:39.489 --> 00:06:43.079
to work in a particular idealized way, isn't it?

127
00:06:43.250 --> 00:06:47.109
Isn't it a little bit uh unrealistic?

128
00:06:47.839 --> 00:06:50.339
Well, obviously, I'm sympathetic to what I think is

129
00:06:50.350 --> 00:06:53.209
behind the question. Um But I guess maybe what

130
00:06:53.220 --> 00:06:57.369
I should say is that um how to put

131
00:06:57.380 --> 00:07:02.649
this the point of my book and the kind

132
00:07:02.660 --> 00:07:04.600
of criticism I want to make of what I

133
00:07:04.609 --> 00:07:08.910
was earlier calling ideal theory is not that there

134
00:07:08.920 --> 00:07:12.579
are no conceivable reasons for which you might adopt

135
00:07:12.589 --> 00:07:15.730
this sort of approach. So for example, one very

136
00:07:15.739 --> 00:07:20.440
popular subfield and contemporary epistemology is formal epistemology So

137
00:07:20.450 --> 00:07:24.769
what former Teos are doing essentially is they're constructing

138
00:07:24.779 --> 00:07:29.559
very idealized models of inquirers. These inquirers have got

139
00:07:29.570 --> 00:07:33.200
properties such as logical, logical omniscient. So they can,

140
00:07:33.500 --> 00:07:36.609
they see all the logical consequences of their beliefs

141
00:07:36.619 --> 00:07:39.609
and they endorse those consequences. Obviously, people aren't like

142
00:07:39.619 --> 00:07:42.839
that. You know, you might think, why would you

143
00:07:42.850 --> 00:07:45.190
do this as you put it? This is very

144
00:07:45.200 --> 00:07:49.339
unrealistic. But as it turns out, formal epistemology has

145
00:07:49.350 --> 00:07:52.700
been influential in a wide range of fields outside

146
00:07:52.709 --> 00:07:55.500
of philosophy. So it seems that non philosophers are

147
00:07:55.510 --> 00:07:58.000
very much interested in the sorts of work that

148
00:07:58.010 --> 00:08:00.079
formal epistemology do. I mean, one way of thinking

149
00:08:00.089 --> 00:08:04.260
about this perhaps would be, you know, fields like

150
00:08:05.070 --> 00:08:08.739
artificial intelligence and robotics, you're trying to construct agents,

151
00:08:08.750 --> 00:08:11.140
you have to kind of give them rules for

152
00:08:11.149 --> 00:08:13.459
how to go about doing things it seems like

153
00:08:13.470 --> 00:08:17.059
here you might be actually interested in trying to

154
00:08:17.220 --> 00:08:21.290
figure out what would be good rules for someone

155
00:08:21.299 --> 00:08:23.619
to try and follow. I'm not sure if that's

156
00:08:23.630 --> 00:08:24.839
the best example of what I have in mind,

157
00:08:24.850 --> 00:08:28.049
but hopefully the audience gets the idea um this

158
00:08:28.059 --> 00:08:30.010
sort of form of work, even though it's not

159
00:08:30.019 --> 00:08:32.710
realistic as a model of human beings. Um Well,

160
00:08:32.719 --> 00:08:33.719
it's not meant to be, that's not what it

161
00:08:33.729 --> 00:08:35.530
is, it's not trying to be a model of

162
00:08:35.539 --> 00:08:38.030
human beings that is doing something else kind of

163
00:08:38.039 --> 00:08:40.450
roughly analogous, I guess to the way in which

164
00:08:40.500 --> 00:08:43.390
things like logic, right? So logic isn't meant to

165
00:08:43.400 --> 00:08:46.809
be a model of how human beings reason. Um

166
00:08:47.260 --> 00:08:48.979
If you take it that way, it's very, it's

167
00:08:48.989 --> 00:08:50.530
a very bad model, but that's not what logic

168
00:08:50.539 --> 00:08:53.609
is. A logic of course, has got all sorts

169
00:08:53.619 --> 00:08:58.570
of very important practical um uses. So the critique

170
00:08:58.580 --> 00:09:02.369
of Ideal theory in epistemology is not that it

171
00:09:02.380 --> 00:09:06.890
has no point. The critique is rather more precise

172
00:09:06.900 --> 00:09:11.020
than that. Roughly speaking, the critique is epistemology, justs

173
00:09:11.030 --> 00:09:15.489
increasingly are turning their attention to real world problems,

174
00:09:15.500 --> 00:09:18.599
often real world political problems, but sometimes just more

175
00:09:18.609 --> 00:09:21.059
broadly social problems. And they want to use the

176
00:09:21.070 --> 00:09:25.010
tool of epistemology to say something about those problems.

177
00:09:25.270 --> 00:09:27.840
In some cases, one gets the impression they think

178
00:09:27.849 --> 00:09:29.869
that we can use the tools of epistemology to

179
00:09:29.880 --> 00:09:34.979
actually put forward sensible solutions to these problems. That's

180
00:09:34.989 --> 00:09:38.539
where this tendency towards idealization becomes a problem because

181
00:09:38.549 --> 00:09:41.349
now you're addressing an actual problem faced by actual

182
00:09:41.359 --> 00:09:44.150
human beings. But if you're using these tools that

183
00:09:44.159 --> 00:09:49.030
embed all sorts of idealized assumptions, then it's not

184
00:09:49.039 --> 00:09:52.380
particularly surprising if it turns out that, that those

185
00:09:52.390 --> 00:09:56.489
solutions aren't really particularly fitting to the actual problems

186
00:09:56.500 --> 00:09:59.010
people face. Um And part of the reason you

187
00:09:59.020 --> 00:10:01.099
might think, well, why would anyone do this? Right?

188
00:10:01.109 --> 00:10:03.570
Like why would anyone get confused and think that

189
00:10:03.580 --> 00:10:09.119
um these idealized models have got um practical application?

190
00:10:09.140 --> 00:10:11.260
Well, part of the reason for this is that

191
00:10:11.320 --> 00:10:13.549
the way put it earlier was the ideal theory

192
00:10:13.559 --> 00:10:16.650
and non ideal theory are kind of two diametrically

193
00:10:16.659 --> 00:10:18.809
opposed things, but that's not quite right, of course.

194
00:10:18.820 --> 00:10:21.669
So you can have think about it as kind

195
00:10:21.679 --> 00:10:23.820
of a continuum, right. So one end, you've got

196
00:10:23.830 --> 00:10:26.440
the most idealized ideal theory at the other end,

197
00:10:26.450 --> 00:10:29.260
but the least idealized the most non ideal theory.

198
00:10:29.270 --> 00:10:33.530
Um THE actual epistemology with actual views will be

199
00:10:33.539 --> 00:10:37.429
located somewhere along this continuum. I would say that

200
00:10:37.440 --> 00:10:40.809
kind of the dominant trend in my kind of

201
00:10:40.820 --> 00:10:44.340
epistemology and the epistemology is more towards the idealized

202
00:10:44.349 --> 00:10:47.789
side. But there is a recognition of certain human

203
00:10:47.799 --> 00:10:50.460
limitations and the critique in the book is that

204
00:10:50.630 --> 00:10:53.280
the problem becomes, once you start to recognize certain

205
00:10:53.289 --> 00:10:55.539
limitations, you can kind of lose track of the

206
00:10:55.549 --> 00:10:59.276
fact that actually you're ignoring other limitations. Um So

207
00:10:59.286 --> 00:11:01.125
you end up kind of in a sense confused

208
00:11:01.135 --> 00:11:05.356
about just how ideal your approach is. Um IT

209
00:11:05.366 --> 00:11:09.815
is less than fully ideal perhaps, but um it

210
00:11:10.056 --> 00:11:14.056
is idealized in ways that cause problems given the

211
00:11:14.065 --> 00:11:15.635
sorts of uses to which you want to be

212
00:11:15.645 --> 00:11:17.276
your theory. That wasn't too cryptic.

213
00:11:17.645 --> 00:11:19.986
No, no, not at all. Thank you for that

214
00:11:19.995 --> 00:11:23.541
clarification. And really to, to point that you made

215
00:11:23.552 --> 00:11:26.642
there in the book, you talk about non ideal

216
00:11:26.651 --> 00:11:32.591
epistemology as being explicitly ethical and political. Could you

217
00:11:32.601 --> 00:11:34.471
explain that point? What

218
00:11:36.262 --> 00:11:38.151
I'm trying to think what exactly you're picking up

219
00:11:38.161 --> 00:11:39.851
on in the book? Um Well, there's, there's a

220
00:11:39.861 --> 00:11:44.331
clear sense in which no idea ology is explicitly

221
00:11:44.341 --> 00:11:47.202
ethical and political, which is that I think that

222
00:11:47.211 --> 00:11:52.799
once you are approaching things in this non ideal

223
00:11:52.809 --> 00:11:57.000
way, um It's very difficult to put down a

224
00:11:57.010 --> 00:12:01.409
kind of clear distinction between epistemological and say ethical

225
00:12:01.419 --> 00:12:04.650
or political questions. Um But if you work at

226
00:12:04.659 --> 00:12:07.119
a higher level of distraction, you can sort of

227
00:12:07.130 --> 00:12:10.520
try and preserve this, this night, this nice, neat

228
00:12:10.530 --> 00:12:14.150
distinction. Um So that's one way in which I

229
00:12:14.159 --> 00:12:18.010
think um it's explicitly ethical and political. So one

230
00:12:18.020 --> 00:12:21.650
topic I discuss in the book, for example, is

231
00:12:21.659 --> 00:12:27.570
um science denial, in particular climate change denial. Um

232
00:12:27.590 --> 00:12:30.489
So I'm interested in the epistemology of science denial.

233
00:12:30.500 --> 00:12:32.349
So this is as I see it an inquiry

234
00:12:32.359 --> 00:12:34.409
in epistemology. But of course, this has got a

235
00:12:34.419 --> 00:12:37.599
political dimension because I'm talking about climate change skepticism

236
00:12:37.700 --> 00:12:39.969
and that's connected up with all these big political

237
00:12:39.979 --> 00:12:42.039
questions. So there's just as I see it, no

238
00:12:42.049 --> 00:12:46.280
way of doing the epistemology of science denial that

239
00:12:46.289 --> 00:12:50.159
is not also to do some kind of ethics

240
00:12:50.169 --> 00:12:54.760
or political philosophy. Um So perhaps that's what I

241
00:12:54.770 --> 00:12:57.320
had in mind in the passage you're you're referring

242
00:12:57.330 --> 00:12:57.559
to

243
00:12:58.559 --> 00:13:03.140
uh and also you're interested in feminist epistemology and

244
00:13:03.150 --> 00:13:06.219
it's also something that you talk about in the

245
00:13:06.229 --> 00:13:11.099
book. And so how does non ideal epistemology relate

246
00:13:11.369 --> 00:13:13.659
to feminist epistemology?

247
00:13:14.590 --> 00:13:16.260
Good. Yeah. So I guess I should give me

248
00:13:16.330 --> 00:13:18.349
the opportunity to expand on what I said in

249
00:13:18.359 --> 00:13:22.549
response to your previous question about Non Ideal epistemology

250
00:13:22.580 --> 00:13:25.700
as explicitly ethical and political. So one way of

251
00:13:25.710 --> 00:13:31.775
characterizing feminist epistemology is that it is an approach

252
00:13:31.784 --> 00:13:35.914
to recognizably piston question. So questions about knowledge, the

253
00:13:35.924 --> 00:13:39.554
nature of knowledge, for example, that is informed by

254
00:13:39.565 --> 00:13:42.815
feminist concerns and of course, feminist concerns are ethical

255
00:13:42.825 --> 00:13:47.039
and political concerns. Um BUT it's still an approach

256
00:13:47.049 --> 00:13:52.219
to ethology. So a feminist emo for example, is

257
00:13:52.229 --> 00:13:57.450
still interested in a question such as how we

258
00:13:57.460 --> 00:13:59.619
should go about forming beliefs or a question such

259
00:13:59.630 --> 00:14:02.780
as how we should go about conducting inquiries. But

260
00:14:02.789 --> 00:14:07.580
they see that question as as relevant to the

261
00:14:07.669 --> 00:14:11.659
their feminist political concerns. So for example, when a

262
00:14:11.669 --> 00:14:14.119
feminist pasmo just takes a look at science, they

263
00:14:14.130 --> 00:14:16.210
might ask questions such as well. Why is it

264
00:14:16.219 --> 00:14:19.590
that so much scientific inquiry seems to ignore questions

265
00:14:19.599 --> 00:14:22.099
of gender. Why is it that it seems like

266
00:14:22.109 --> 00:14:24.919
when you look at certain fields such as evolutionary

267
00:14:25.109 --> 00:14:29.409
psychology, there are certain assumptions but male dominance that

268
00:14:30.270 --> 00:14:32.859
seem to be unquestioned within the fields, perhaps we

269
00:14:32.869 --> 00:14:37.289
should question those assumptions. So feminist psalmists are asking

270
00:14:37.840 --> 00:14:41.570
epistemological questions but with these kind of political and

271
00:14:41.580 --> 00:14:44.570
ethical concerns in the background. So now to go

272
00:14:44.580 --> 00:14:47.099
back to your question, Ricardo about how this all

273
00:14:47.109 --> 00:14:50.039
relates to non ideal epistemology. So the way in

274
00:14:50.049 --> 00:14:51.109
which I put it in the book and I

275
00:14:51.119 --> 00:14:55.260
guess I stand by this is that feminist ology

276
00:14:55.270 --> 00:14:58.590
is a kind of non idea epistemology. So feminist

277
00:14:58.760 --> 00:15:02.340
cosmologists are doing non idea epistemology. I think they've

278
00:15:02.349 --> 00:15:04.419
done some of the best non idealist, which is

279
00:15:04.429 --> 00:15:07.205
why I draw on feminist so much in the

280
00:15:07.216 --> 00:15:11.416
book. But you can do non idea epistemology without

281
00:15:11.426 --> 00:15:16.585
having these sorts of feminist concerns in the foreground,

282
00:15:16.596 --> 00:15:18.145
so to speak. So I thought to say you're

283
00:15:18.156 --> 00:15:20.075
doing kind of anti feminist ethology. It's just that

284
00:15:20.085 --> 00:15:25.106
that's maybe not the sorts of considerations you are

285
00:15:25.116 --> 00:15:27.335
focusing on. So for example, to go back to

286
00:15:27.346 --> 00:15:29.942
the earlier example, perhaps you could bring a kind

287
00:15:29.952 --> 00:15:32.622
of feminist dimension to your analysis of science denial.

288
00:15:33.132 --> 00:15:36.541
But that's kind of optional as it were. Um

289
00:15:36.552 --> 00:15:38.901
YOU might be thinking about a phenomenon such as

290
00:15:38.911 --> 00:15:42.392
science denial in just a different way. Um As

291
00:15:42.401 --> 00:15:44.262
I said earlier in the book, I'm doing something

292
00:15:44.271 --> 00:15:47.052
like the epistemology of science denial. And that just

293
00:15:47.062 --> 00:15:48.632
doesn't seem to me to be a kind of

294
00:15:48.642 --> 00:15:51.719
feminist system. Um So yeah, that would mean no,

295
00:15:51.789 --> 00:15:54.520
the epistemology is if like a broader category than

296
00:15:54.530 --> 00:15:57.250
feminist ology. So feminist p would be a kind

297
00:15:57.260 --> 00:16:00.830
of species of non idea, perhaps a particularly interesting

298
00:16:00.840 --> 00:16:04.530
species but still just a species. Then there's kind

299
00:16:04.539 --> 00:16:07.020
of other ways of doing non ideal epistemology. That

300
00:16:07.030 --> 00:16:08.770
wouldn't be obviously feminist,

301
00:16:09.780 --> 00:16:11.679
please correct me if I'm wrong. But I guess

302
00:16:11.690 --> 00:16:14.109
that some of the things that you mentioned there

303
00:16:14.119 --> 00:16:19.369
regarding feminist epistemology apply more broadly to uh non

304
00:16:19.580 --> 00:16:22.919
ideal epistemology. And as you said, feminist epistemology, you

305
00:16:22.929 --> 00:16:25.200
look at it as a form of non ideal

306
00:16:25.210 --> 00:16:29.359
epistemology. But in the sense that for uh it

307
00:16:29.369 --> 00:16:35.359
looks at epistemic agents as situated in a particular

308
00:16:35.369 --> 00:16:40.650
social, cultural, perhaps political and environment, right? For example,

309
00:16:40.659 --> 00:16:45.799
you mentioned the fact that feminist uh feminist epistemology

310
00:16:45.809 --> 00:16:51.440
sometimes look at scientific inquiry in certain disciplines. And

311
00:16:51.450 --> 00:16:56.630
for example, they notice that the scientists many times

312
00:16:56.799 --> 00:16:59.659
come to the table, come to the table with

313
00:16:59.669 --> 00:17:04.319
certain uh assumptions that then drive the way they

314
00:17:04.329 --> 00:17:08.319
produce science or the way they formulate hypothesis to

315
00:17:08.329 --> 00:17:13.000
be tested, for example, certain assumptions about uh sex

316
00:17:13.010 --> 00:17:16.930
or gender differences and where they come from and

317
00:17:16.939 --> 00:17:22.000
then that has an impact on the production of

318
00:17:22.010 --> 00:17:26.290
science, how they evaluate evidence and so on. So

319
00:17:26.300 --> 00:17:29.130
I is that uh right, I mean, am I

320
00:17:29.140 --> 00:17:31.229
right in pointing that out or?

321
00:17:31.520 --> 00:17:35.890
Yes, certainly. So. So on the official sort of

322
00:17:35.900 --> 00:17:40.079
uh account in the book, Non Ideal Epistemology means

323
00:17:40.089 --> 00:17:43.400
an approach to these normative questions I was mentioning

324
00:17:43.410 --> 00:17:50.689
earlier that involves several different dimensions of non ideality,

325
00:17:50.699 --> 00:17:53.760
so to speak. And one of those dimensions is

326
00:17:53.770 --> 00:17:57.589
a recognition of the extent to which actual inquirers

327
00:17:57.599 --> 00:18:02.040
are socially situated. So that means that they've got

328
00:18:02.050 --> 00:18:04.569
the various markers of social identity, right? So they've

329
00:18:04.579 --> 00:18:09.229
got a gender, a race, a class and so

330
00:18:09.239 --> 00:18:10.859
on. But then obviously a whole bunch of other

331
00:18:10.869 --> 00:18:14.359
things besides, right. So they've got a job, a

332
00:18:14.369 --> 00:18:18.109
scientist, for example, um they're embedded within a particular

333
00:18:18.119 --> 00:18:20.969
community which has implications for the sorts of information

334
00:18:20.979 --> 00:18:23.729
sources they have available. All these sorts of things

335
00:18:23.739 --> 00:18:27.109
have to be taken into account. Um So kind

336
00:18:27.119 --> 00:18:29.449
of one nice way of putting the sense in

337
00:18:29.459 --> 00:18:33.079
which feminist epistemology is almost by definition, a version

338
00:18:33.089 --> 00:18:36.589
of non ideal epistemology is precisely that in feminist

339
00:18:36.599 --> 00:18:39.890
Phal, uh it's kind of definitional that it is

340
00:18:39.900 --> 00:18:44.189
based on a recognition of the social situation this,

341
00:18:44.199 --> 00:18:47.430
of, of no, and in particular, the fact that

342
00:18:47.439 --> 00:18:52.180
everyone has got a gender identity and that kind

343
00:18:52.189 --> 00:18:54.579
of informs the way in which they might look

344
00:18:54.589 --> 00:18:56.349
at the world in all sorts of ways, not

345
00:18:56.359 --> 00:18:57.829
necessarily in a kind of crude way. You know,

346
00:18:57.839 --> 00:19:00.109
the idea is not, there's like a male way

347
00:19:00.119 --> 00:19:01.280
of looking at the world and a female way

348
00:19:01.290 --> 00:19:03.199
of looking at the world. That's you can find

349
00:19:03.209 --> 00:19:05.930
that idea in older writings and feminist pathology, but

350
00:19:05.939 --> 00:19:08.064
that's not the more modern idea. The more modern

351
00:19:08.074 --> 00:19:09.885
idea is just the kind of to me obvious

352
00:19:09.895 --> 00:19:13.114
fact that who you are has an implication has

353
00:19:13.125 --> 00:19:14.895
implications, sorry for, for how you look at the

354
00:19:14.905 --> 00:19:19.045
world and therefore is something that can't be ignored

355
00:19:19.055 --> 00:19:22.334
if you are asking certain epistemological questions. I mean,

356
00:19:22.344 --> 00:19:24.344
to me, it just seems obviously true. So to

357
00:19:24.354 --> 00:19:26.415
that sense, in that sense, I can't really see

358
00:19:26.425 --> 00:19:29.685
why you would disagree with the basic idea behind

359
00:19:29.694 --> 00:19:30.964
feminist pathology.

360
00:19:31.599 --> 00:19:36.020
So let's get into some specific issues problems that

361
00:19:36.030 --> 00:19:38.579
you deal with in the book related to Non

362
00:19:38.689 --> 00:19:43.469
Ideal Epistemology and uh how they play out in

363
00:19:43.479 --> 00:19:47.420
non ideal epistemology. So uh one of the issues

364
00:19:47.430 --> 00:19:52.089
you address there is the problem of the identification

365
00:19:52.099 --> 00:19:56.579
of expertise. So what is this problem? And how

366
00:19:56.589 --> 00:20:00.260
would it play out in non ideal epistemology?

367
00:20:00.619 --> 00:20:04.439
Yeah, good. This is an interesting problem because it's

368
00:20:04.670 --> 00:20:09.069
well interesting because it's, it's an interesting problem, both,

369
00:20:09.079 --> 00:20:11.699
both as a theatrical problem and also as a

370
00:20:11.709 --> 00:20:14.479
kind of like more practical or applied problem. So

371
00:20:14.489 --> 00:20:19.250
the theoretical problem is just that um unless we

372
00:20:19.260 --> 00:20:22.079
assume that all experts agree about everything, there are

373
00:20:22.089 --> 00:20:24.900
always going to be situations where different experts say

374
00:20:24.910 --> 00:20:28.609
different things. So for example, one person says that

375
00:20:28.619 --> 00:20:31.229
the um a good diet is one that is

376
00:20:31.239 --> 00:20:34.530
high in protein. Another expert says that a good

377
00:20:34.540 --> 00:20:37.010
diet is one that is high in fat, for

378
00:20:37.020 --> 00:20:41.810
example. So two medical experts saying different things. Um

379
00:20:41.910 --> 00:20:44.290
So the question becomes well, how are you as

380
00:20:44.300 --> 00:20:46.900
someone who is not an expert in the relevant

381
00:20:46.910 --> 00:20:49.530
subject? In this case, uh Nutrition, how are you

382
00:20:49.540 --> 00:20:53.520
meant to um decide who to listen to? And

383
00:20:53.530 --> 00:20:56.189
you can formulate this is kind of abstract philosophical

384
00:20:56.199 --> 00:20:58.099
problem. You can say lots of interesting things about

385
00:20:58.109 --> 00:21:00.089
it. Um So the work you can kind of

386
00:21:00.099 --> 00:21:01.589
do and this is an abstract problem I think

387
00:21:01.599 --> 00:21:06.199
is independently um worth worth doing that said I'm

388
00:21:06.209 --> 00:21:09.369
not really interested in the abstract problem I'm interested

389
00:21:09.380 --> 00:21:11.349
in. If you like kind of the form the

390
00:21:11.359 --> 00:21:16.130
abstract problem takes in actual environments where it's not

391
00:21:16.140 --> 00:21:19.229
that you're kind of sitting there as a uninformed

392
00:21:19.239 --> 00:21:22.900
layperson without any kind of inclination towards one side

393
00:21:22.910 --> 00:21:24.949
rather than the other. No, often you've got an

394
00:21:24.959 --> 00:21:28.489
inclination towards one side. So for example, um I

395
00:21:28.500 --> 00:21:32.150
don't know, you've been brought up being told that

396
00:21:32.160 --> 00:21:36.770
um uh fatty foods are bad for you. Uh

397
00:21:36.859 --> 00:21:39.400
So when you come kind of come across this

398
00:21:39.410 --> 00:21:42.810
sort of disagreement between these two experts, your inclinations

399
00:21:42.819 --> 00:21:45.729
automatically side with the doctor who is saying that

400
00:21:46.109 --> 00:21:49.020
you should avoid fat. Um But of course, you

401
00:21:49.030 --> 00:21:51.209
know, that should lead you to kind of interrogate

402
00:21:51.540 --> 00:21:55.369
your prior inclinations. Um You might want to ask,

403
00:21:55.380 --> 00:21:57.550
where did they come from? Am I terribly well

404
00:21:57.560 --> 00:22:00.609
informed? Perhaps I've been misinformed. So that just makes

405
00:22:00.619 --> 00:22:04.510
the problem even more difficult because it's not just

406
00:22:04.520 --> 00:22:07.780
that from a kind of abstract, rational perspective, it's

407
00:22:07.790 --> 00:22:11.719
not obvious how you should decide the issue. Um

408
00:22:11.849 --> 00:22:13.880
You've also got to reckon with the fact that,

409
00:22:14.000 --> 00:22:16.140
you know, you're not impartial, you're not an impartial

410
00:22:16.150 --> 00:22:18.770
observer. And um but then how do you go

411
00:22:18.780 --> 00:22:22.439
about questioning your kind of inclinations towards one side

412
00:22:22.449 --> 00:22:24.239
rather than the other? It doesn't really seem obvious

413
00:22:24.250 --> 00:22:26.670
how you go about doing that. And that to

414
00:22:26.680 --> 00:22:28.949
me seems like a kind of difficult problem. So

415
00:22:28.959 --> 00:22:30.900
of course, the example I've given perhaps isn't the

416
00:22:30.910 --> 00:22:33.579
most, isn't the best one for making the political

417
00:22:33.589 --> 00:22:36.729
dimensions of the problem salient. So just replace this

418
00:22:36.739 --> 00:22:40.050
with a different example. Um You've got one doctor

419
00:22:40.060 --> 00:22:43.319
saying that uh wearing masks uh is going to

420
00:22:43.329 --> 00:22:45.630
protect you against uh let's imagine a brand new

421
00:22:45.640 --> 00:22:47.410
pandemic. So not COVID but a new one in

422
00:22:47.420 --> 00:22:50.479
10 years time. Another doctor saying that they won't

423
00:22:50.489 --> 00:22:53.739
um you've got inclinations one with other. Perhaps you

424
00:22:53.750 --> 00:22:57.739
think, you know, masking is uh is fine or

425
00:22:57.750 --> 00:22:59.569
perhaps you think it's, it's not, it's not so

426
00:22:59.579 --> 00:23:03.319
good that's going to color how you interpret this

427
00:23:03.329 --> 00:23:06.069
expert disagreement. But of course, you know, you should

428
00:23:06.079 --> 00:23:08.910
be kind of questioning the interpretation that you give

429
00:23:08.920 --> 00:23:11.630
to the disagreement, not just Lynn leaves you, I

430
00:23:11.640 --> 00:23:14.439
would say in, in a very difficult situation.

431
00:23:16.859 --> 00:23:19.359
Uh And so I guess that at least to

432
00:23:19.369 --> 00:23:23.319
some extent associated with that, uh there's also uh

433
00:23:23.329 --> 00:23:26.760
the idea of intellectual autonomy and the uh the

434
00:23:26.770 --> 00:23:29.540
reason why I'm bringing that up now is that

435
00:23:29.739 --> 00:23:34.000
many times, uh I mean, relying on experts, people

436
00:23:34.010 --> 00:23:38.724
pitted against intellectual autonomy against the idea of to

437
00:23:38.734 --> 00:23:43.155
put it in simpler terms, thinking for yourself instead

438
00:23:43.165 --> 00:23:47.224
of relying on the intellectual or the epistemic work

439
00:23:47.234 --> 00:23:50.464
of others in this case, experts. So to what

440
00:23:50.474 --> 00:23:54.974
extent should we have it according to this non

441
00:23:55.025 --> 00:23:57.265
ideal epistemological approach?

442
00:23:58.189 --> 00:24:01.489
So, so there's a view I think probably I

443
00:24:01.510 --> 00:24:03.989
defend in the book that I know have come

444
00:24:04.000 --> 00:24:05.869
to think is a little bit too strong. So

445
00:24:05.880 --> 00:24:07.510
what I'll do is I'll see what the view

446
00:24:07.520 --> 00:24:09.609
was in the book and then maybe explain why.

447
00:24:09.619 --> 00:24:11.150
Now I think it's a little bit too strong

448
00:24:11.170 --> 00:24:13.640
alone, its essentials, I think still. Right. So, the

449
00:24:13.650 --> 00:24:14.849
view in the book, I think, or at least

450
00:24:14.859 --> 00:24:17.160
one could justifiably get the impression from the book

451
00:24:17.170 --> 00:24:20.270
from the book that I think that, um, intellectual

452
00:24:20.280 --> 00:24:25.180
autonomy would be nice, but we can't really have

453
00:24:25.189 --> 00:24:26.959
it. And in so far as we can try

454
00:24:26.969 --> 00:24:28.859
to have it, it's often going to make things

455
00:24:28.869 --> 00:24:31.359
worse. So, what do you mean by that? Well,

456
00:24:31.369 --> 00:24:33.319
let's go back to my example where you're trying

457
00:24:33.329 --> 00:24:35.500
to decide between, I don't know, the pro masking

458
00:24:35.510 --> 00:24:39.189
doctor and the anti masking doctor. Um So someone

459
00:24:39.199 --> 00:24:41.939
who wants to kind of focus on the importance

460
00:24:41.949 --> 00:24:44.900
of intellectual autonomy is going to say, well, sure,

461
00:24:44.910 --> 00:24:46.560
you're going to end up in this very difficult

462
00:24:46.569 --> 00:24:48.760
situation where you almost have to kind of question

463
00:24:48.770 --> 00:24:52.589
your assumptions before you can start to decide what

464
00:24:52.609 --> 00:24:55.140
to listen to, but still you should try and

465
00:24:55.150 --> 00:24:56.760
do that, right? So you should try and sort

466
00:24:56.770 --> 00:24:58.709
through the issue for yourself. Perhaps you should go

467
00:24:58.719 --> 00:25:02.119
and read a bunch of um uh medical literature.

468
00:25:02.130 --> 00:25:04.540
Um You should talk to people, you should really

469
00:25:04.550 --> 00:25:07.609
try to uh come to your own view uh

470
00:25:07.640 --> 00:25:10.949
about the, the matter in question. Um So that's

471
00:25:10.959 --> 00:25:13.020
what the defender of intellectual autonomy would say in

472
00:25:13.030 --> 00:25:16.069
the book. What I essentially say is that um

473
00:25:16.250 --> 00:25:18.699
that's not going to work out very well. Um

474
00:25:18.709 --> 00:25:22.160
Essentially because I think that this is going back

475
00:25:22.170 --> 00:25:23.780
to what I said earlier, there's just no good

476
00:25:23.790 --> 00:25:28.069
way for you to remove your prior biases from

477
00:25:28.079 --> 00:25:32.130
whatever process of inquiry you try to carry out.

478
00:25:32.140 --> 00:25:35.449
So the fact that for example, you've got nations

479
00:25:35.459 --> 00:25:38.180
in favor of masks is inevitably going to lead

480
00:25:38.189 --> 00:25:41.579
you to perform a sort of biased literature review.

481
00:25:41.709 --> 00:25:44.579
And the chances are you'll end up just gathering

482
00:25:44.589 --> 00:25:48.459
evidence that supports your, your initial view of the

483
00:25:48.469 --> 00:25:50.199
situation. And the claim is not that you'll do

484
00:25:50.209 --> 00:25:52.010
that consciously, right? The claim is that you're in

485
00:25:52.020 --> 00:25:54.160
a sense, sort of kind of fool yourself into

486
00:25:54.170 --> 00:25:57.650
thinking that you're doing an unbiased search. Whereas in

487
00:25:57.660 --> 00:26:00.680
fact, you're doing a biased search, like an idea

488
00:26:00.689 --> 00:26:02.459
I have in the book is what this shows

489
00:26:02.469 --> 00:26:05.079
you is that at best, trying to be intellectually

490
00:26:05.089 --> 00:26:09.459
autonomous is, is useless at worst. Actually, it's going

491
00:26:09.469 --> 00:26:13.280
to lead you further away from the truth because

492
00:26:13.290 --> 00:26:16.520
what you could do instead is rather than trying

493
00:26:16.530 --> 00:26:20.709
to figure out the matter for yourself is you

494
00:26:20.719 --> 00:26:23.079
could, well, you could, it's also you could do

495
00:26:23.089 --> 00:26:25.719
something. It's rather that a better environment could be

496
00:26:25.729 --> 00:26:29.800
constructed for you uh in which um there's less

497
00:26:29.810 --> 00:26:33.449
need for you to uh to do these things

498
00:26:33.459 --> 00:26:35.010
by yourself. So the idea in the book is

499
00:26:35.020 --> 00:26:38.569
really that left their own devices, perhaps we'll try

500
00:26:38.579 --> 00:26:41.209
to intellectual autonomous that will work out very well.

501
00:26:41.219 --> 00:26:44.650
So really what would be required is an attempt

502
00:26:44.660 --> 00:26:47.689
to engineer a situation where people don't need to

503
00:26:47.699 --> 00:26:50.939
be. Um, WELL, people end up with the right

504
00:26:50.949 --> 00:26:55.459
sorts of views, um, irrespective of what they do

505
00:26:55.469 --> 00:26:59.079
as individual Inquirer. That sounds a little bit possibly

506
00:26:59.089 --> 00:27:01.099
Orwell. And we can talk about that later anyway.

507
00:27:01.109 --> 00:27:03.199
So the, the kind of shift in my thinking

508
00:27:03.209 --> 00:27:07.609
that I've kind of come to more recently is

509
00:27:07.619 --> 00:27:10.300
perhaps that is a little bit overstated. I think

510
00:27:10.310 --> 00:27:12.680
probably they are going to be a wide range

511
00:27:12.689 --> 00:27:17.305
of important cases where trying to be intellect autonomous

512
00:27:17.314 --> 00:27:22.025
could actually be to your benefit. Perhaps in certain

513
00:27:22.035 --> 00:27:25.064
situations, for example, in situations where there's a kind

514
00:27:25.074 --> 00:27:28.885
of problem at the level of expertise, perhaps the

515
00:27:28.895 --> 00:27:31.375
experts themselves are subject to a kind of group,

516
00:27:31.385 --> 00:27:35.334
think there could be value in a kind of

517
00:27:35.344 --> 00:27:37.974
intellectual autonomy. But the point I wanted to make

518
00:27:37.984 --> 00:27:40.905
the book was really independent of all this. The

519
00:27:40.915 --> 00:27:45.839
point was more just that we shouldn't be led

520
00:27:45.849 --> 00:27:49.589
into the following mistake. Um JUST because it would

521
00:27:49.599 --> 00:27:52.829
be good for an idealized Inquirer to strive to

522
00:27:52.839 --> 00:27:55.989
be intellectually autonomous, doesn't mean that we should strive

523
00:27:56.000 --> 00:27:59.810
intellectually autonomous because an idealized Inquirer is going to

524
00:27:59.819 --> 00:28:04.969
lack the sorts of features such as susceptibility towards

525
00:28:04.979 --> 00:28:07.699
various forms of bias. That mean that when we

526
00:28:07.709 --> 00:28:09.839
try to intellectually autonomous, we end up making a

527
00:28:09.849 --> 00:28:11.969
mess of it. So the point is if you

528
00:28:11.979 --> 00:28:15.520
like kind of more almost conceptual, um it's kind

529
00:28:15.530 --> 00:28:17.530
of natural if you have this kind of ideal

530
00:28:17.540 --> 00:28:20.760
theory way of thinking to um to think that

531
00:28:20.770 --> 00:28:23.760
intellectually autonomous, intellectual autonomy is important because it would

532
00:28:23.770 --> 00:28:27.300
be important for an idealized Inquirer. But again, we're

533
00:28:27.310 --> 00:28:30.239
not idealized Enquirer. Um So the argument isn't that

534
00:28:30.250 --> 00:28:32.640
simple, what you'd have to do is show that

535
00:28:32.650 --> 00:28:36.400
for actual inquirer in the actual situations that Inquirers

536
00:28:36.410 --> 00:28:40.380
find themselves trying to be intellectually autonomous works out

537
00:28:40.390 --> 00:28:42.829
well for them. Um So perhaps now I think

538
00:28:42.839 --> 00:28:45.449
that that will happen more often than I give

539
00:28:45.459 --> 00:28:47.390
the impression I do in the book. But the

540
00:28:47.400 --> 00:28:48.619
point is still, that's what you would have to

541
00:28:48.630 --> 00:28:51.579
show. OK. That made sense. That was quite

542
00:28:52.130 --> 00:28:53.959
no, no, no, of course. I I just want

543
00:28:53.969 --> 00:28:56.410
to ask you one or two follow ups to

544
00:28:56.439 --> 00:28:59.579
it. So at a certain point there, you mentioned

545
00:28:59.589 --> 00:29:02.380
the idea or touched on the idea of constructing

546
00:29:02.390 --> 00:29:05.369
a better epistemic environment, but I I would like

547
00:29:05.380 --> 00:29:08.359
to understand a little bit better. Uh If we

548
00:29:08.369 --> 00:29:13.739
are operating under this non ideal epistemological framework, what

549
00:29:13.750 --> 00:29:17.520
does it mean that to have a better epistemic

550
00:29:17.530 --> 00:29:20.140
environment? What does it entail? Exactly,

551
00:29:20.339 --> 00:29:22.189
right. Yeah. So I mean, let me put it

552
00:29:22.199 --> 00:29:25.219
in a kind of abstract terms first, that will

553
00:29:25.229 --> 00:29:29.219
help your audience get the idea. So let's imagine

554
00:29:29.229 --> 00:29:33.040
the best of all possible epidemic worlds as it

555
00:29:33.050 --> 00:29:35.339
were. So this is a world where there is

556
00:29:35.349 --> 00:29:39.739
plentiful information to hand and that is genuine information.

557
00:29:39.750 --> 00:29:42.810
So we can even imagine there's no misinformation out

558
00:29:42.819 --> 00:29:45.150
there, right? So that all the information that you

559
00:29:45.160 --> 00:29:47.000
get from all the sources that there are in

560
00:29:47.010 --> 00:29:51.160
this perfect world, are giving out good genuine information.

561
00:29:52.109 --> 00:29:54.949
So someone who lives in that world is going

562
00:29:54.959 --> 00:29:57.010
to have a very easy time forming true beliefs,

563
00:29:57.020 --> 00:29:59.459
right? They should literally pick up the first thing

564
00:29:59.469 --> 00:30:01.319
they come across and that will give them a

565
00:30:01.329 --> 00:30:03.329
bunch of true beliefs. And perhaps those aren't the

566
00:30:03.339 --> 00:30:05.489
true beliefs they want. Perhaps this is true beliefs

567
00:30:05.500 --> 00:30:09.319
about the nutritional properties of some new food stuff.

568
00:30:09.329 --> 00:30:10.750
And they don't really care about that. Perhaps they

569
00:30:10.760 --> 00:30:12.839
want to know about, perhaps they want to know,

570
00:30:12.849 --> 00:30:15.540
perhaps they want to know about how uh nuclear

571
00:30:15.550 --> 00:30:17.900
energy works, for example. Um So they don't really

572
00:30:17.910 --> 00:30:20.140
care about uh nutrition. Uh But the point it

573
00:30:20.150 --> 00:30:22.050
still stands that what they're going to get from

574
00:30:22.060 --> 00:30:23.630
just picking up this bit of paper is lots

575
00:30:23.640 --> 00:30:25.819
of genuine information. So, of course, we don't live

576
00:30:25.829 --> 00:30:27.280
in that world, right. So that's not what our

577
00:30:27.290 --> 00:30:31.420
world's like. There's um even if you disagree with

578
00:30:31.430 --> 00:30:34.459
the authorities, but what is misinformation? You're still going

579
00:30:34.469 --> 00:30:36.239
to agree that there's lots of misinformation out there.

580
00:30:36.250 --> 00:30:38.459
It's just, you might think that what many regard

581
00:30:38.469 --> 00:30:41.760
as misinformation is, is inaccurately labeled as such. Everyone's

582
00:30:41.770 --> 00:30:44.479
going to agree that there's lots of misinformation out

583
00:30:44.489 --> 00:30:47.130
there. So that makes it hard for us to

584
00:30:47.140 --> 00:30:50.839
form true beliefs. And this is the point made

585
00:30:50.849 --> 00:30:53.839
earlier. It's especially hard given that, you know, we're

586
00:30:53.849 --> 00:30:57.989
not unbiased observers of this situation. We've got inclinations

587
00:30:58.000 --> 00:30:59.979
one way or the other and that's going to

588
00:30:59.989 --> 00:31:04.969
lead us to essentially focus on the bits of

589
00:31:04.979 --> 00:31:11.560
information that support our our prior views. So constructing

590
00:31:11.569 --> 00:31:15.900
a better epidemic environment, kind of very abstractly means

591
00:31:15.910 --> 00:31:19.739
making our world more like this perfect world. Uh

592
00:31:19.750 --> 00:31:22.089
That's that, that's what it would mean. So of

593
00:31:22.099 --> 00:31:26.770
course, there's all sorts of enormous political questions about

594
00:31:26.780 --> 00:31:29.449
how exactly you go about doing that. I think

595
00:31:29.459 --> 00:31:32.219
in a wide range of cases, you might want

596
00:31:32.229 --> 00:31:35.189
to discuss this becomes very difficult because obviously, there's

597
00:31:35.199 --> 00:31:36.979
not going to be background agreement on what is

598
00:31:36.989 --> 00:31:38.800
true in the first place. And if there's not

599
00:31:38.810 --> 00:31:41.209
background agreement on what is true, there'll be disagreement

600
00:31:41.219 --> 00:31:44.560
on what exactly making the environment better involves. And

601
00:31:44.569 --> 00:31:47.069
that's perhaps one reason why I focus so much

602
00:31:47.079 --> 00:31:49.530
on climate change as an example in the book

603
00:31:49.540 --> 00:31:52.369
because, you know, obviously there is some disagreement, but

604
00:31:52.380 --> 00:31:57.010
there really is very widespread disagreement amongst all the

605
00:31:57.020 --> 00:32:00.689
relevant authorities that at the very least climate change

606
00:32:00.699 --> 00:32:03.530
is real and is driven by human behavior. There's

607
00:32:03.540 --> 00:32:06.010
disagreement about what to do about that fact. Um

608
00:32:06.030 --> 00:32:07.689
EVEN at the kind of the level of the

609
00:32:07.699 --> 00:32:10.810
relevant experts, um not so much about the the

610
00:32:10.819 --> 00:32:13.969
fact that this is a human driven uh phenomenon.

611
00:32:14.260 --> 00:32:15.869
So that to me kind of gives me a

612
00:32:15.880 --> 00:32:20.079
kind of workable example of what constructing a better

613
00:32:20.089 --> 00:32:22.949
environment would involve in the case of beliefs about

614
00:32:22.959 --> 00:32:26.790
climate change, it would involve making it easier, essentially

615
00:32:27.079 --> 00:32:30.739
people to access good genuine information about the climate

616
00:32:30.750 --> 00:32:35.869
and harder for them to access misinformation. And that

617
00:32:35.880 --> 00:32:38.750
doesn't necessarily mean censorship just, just to add, that

618
00:32:38.760 --> 00:32:40.790
might mean for example, and this is actually what

619
00:32:40.800 --> 00:32:43.670
the focus is on in the book, getting science

620
00:32:43.680 --> 00:32:47.750
communicators to think more to think carefully about how

621
00:32:47.760 --> 00:32:50.310
exactly they go about trying to get their message

622
00:32:50.319 --> 00:32:53.959
across. Um So this is not about um uh

623
00:32:53.969 --> 00:32:57.920
censoring uh the voices of skeptics rather, it's about

624
00:32:57.930 --> 00:32:59.959
getting people that want to put across the, the

625
00:32:59.969 --> 00:33:02.339
pro science message to think about how to do

626
00:33:02.349 --> 00:33:04.670
that effectively. Um So that's not a point about

627
00:33:04.680 --> 00:33:06.829
censorship or anything of that sort. That's more about,

628
00:33:07.040 --> 00:33:09.680
you know, how can you go about uh convincing

629
00:33:09.689 --> 00:33:11.790
people of these things that, that you think are

630
00:33:11.800 --> 00:33:14.760
obviously true and um essentially in the book, that's

631
00:33:14.770 --> 00:33:17.439
what I focus on. Uh SO far it's constructing

632
00:33:17.449 --> 00:33:20.989
a better environment, uh is concerned. Answer the question.

633
00:33:21.349 --> 00:33:24.910
Yes, I think that you've already answered this next

634
00:33:24.920 --> 00:33:28.729
question, at least partly, but uh just to address

635
00:33:28.739 --> 00:33:32.550
it directly. Now, uh one of the issues that

636
00:33:32.560 --> 00:33:34.900
stems, I guess from the fact that we live

637
00:33:34.910 --> 00:33:39.099
in this sort of imperfect epistemic environment is or

638
00:33:39.109 --> 00:33:43.849
has to do with public ignorance about important political

639
00:33:43.859 --> 00:33:48.300
and scientific issues. So how would you deal with

640
00:33:48.310 --> 00:33:48.819
that then?

641
00:33:50.229 --> 00:33:52.800
Yeah. So I think probably this is another place

642
00:33:52.810 --> 00:33:54.959
where my thinking has moved on a little bit.

643
00:33:54.969 --> 00:33:57.030
Uh FROM what I say in the book, not

644
00:33:57.040 --> 00:33:58.829
in the sense that I disagree with anything I

645
00:33:58.839 --> 00:34:00.439
say in the book, maybe just in the sense

646
00:34:00.449 --> 00:34:03.540
that I would want to add something to it.

647
00:34:04.050 --> 00:34:06.800
Um So I think the way to think about

648
00:34:06.810 --> 00:34:11.350
public ignorance of important scientific issues as a kind

649
00:34:11.360 --> 00:34:14.250
of political problem is to adopt a sort of

650
00:34:14.260 --> 00:34:17.699
if you like pluralistic response to it. So what

651
00:34:17.708 --> 00:34:22.889
you want to do essentially is um improve the

652
00:34:22.899 --> 00:34:28.478
situation. So abstractly speaking, create an environment where we

653
00:34:28.489 --> 00:34:31.429
have more true beliefs about the relevant issues. And

654
00:34:31.438 --> 00:34:34.110
of course, there are various tools one could use

655
00:34:34.120 --> 00:34:36.760
in order to go about doing that, people often

656
00:34:36.770 --> 00:34:38.679
want to talk about education. So in this case,

657
00:34:38.688 --> 00:34:41.438
we're talking about science education, they also want to

658
00:34:41.449 --> 00:34:44.520
talk about public communication strategies. So here we're talking

659
00:34:44.530 --> 00:34:47.330
about science communication strategies and then they want to

660
00:34:47.340 --> 00:34:48.889
start to talk, you know, kind of more fine

661
00:34:48.899 --> 00:34:51.750
grained about. But what exactly should these strategies look

662
00:34:51.760 --> 00:34:55.280
like? So for example, in the education context, how

663
00:34:55.290 --> 00:34:57.479
should we teach science? Should we teach it as

664
00:34:57.489 --> 00:34:59.969
kind of a body of established facts? One kind

665
00:34:59.979 --> 00:35:02.919
of question, should we do something a bit different?

666
00:35:02.929 --> 00:35:05.419
Um If we do something a bit different, what

667
00:35:05.429 --> 00:35:08.360
what should that be in the science communication context?

668
00:35:08.370 --> 00:35:11.399
You know, so like should we again, should we

669
00:35:11.409 --> 00:35:13.810
kind of focus single mindedly on getting our message

670
00:35:13.820 --> 00:35:16.310
across or rather should we try to acknowledge certain

671
00:35:16.320 --> 00:35:19.350
kinds of complexities, trusting to, to make up their

672
00:35:19.360 --> 00:35:21.530
own minds. Um So in the book, I think

673
00:35:21.540 --> 00:35:25.649
probably I adopt a slightly more myopic approach that

674
00:35:25.659 --> 00:35:27.479
I would know. So I kind of, I, I

675
00:35:27.489 --> 00:35:31.739
give the impression that roughly speaking, it's going to

676
00:35:31.750 --> 00:35:36.830
be difficult to make a kind of balanced case

677
00:35:36.840 --> 00:35:39.909
for certain scientific issues. So you should focus more

678
00:35:39.919 --> 00:35:44.100
on thinking about how you frame your message in

679
00:35:44.110 --> 00:35:47.625
the hope that that will make it easier for

680
00:35:47.635 --> 00:35:49.435
that message to get acceptance. So that's kind of

681
00:35:49.445 --> 00:35:51.135
going along the lines of maybe not trusting the

682
00:35:51.145 --> 00:35:53.754
public quite as much as you might in order

683
00:35:53.764 --> 00:35:57.354
to make sense of things, I think. No, probably

684
00:35:57.534 --> 00:35:59.584
I would say it's a bit one sided perhaps

685
00:35:59.594 --> 00:36:03.915
in certain situations. Um Actually fuller explanations are called

686
00:36:03.925 --> 00:36:08.415
for. I know increasingly think that the pandemic is

687
00:36:08.425 --> 00:36:10.705
a case in point here. So I think at

688
00:36:10.715 --> 00:36:13.074
least certain bodies did try to adopt this sort

689
00:36:13.084 --> 00:36:15.725
of information strategy where it was all very focused

690
00:36:15.735 --> 00:36:19.070
on getting people to have the right beliefs. There

691
00:36:19.080 --> 00:36:22.709
wasn't perhaps enough recognition of certain kinds of nuance

692
00:36:22.719 --> 00:36:26.800
and complexity, perhaps counter factual. But it's possible to

693
00:36:26.810 --> 00:36:29.409
me that if there had been more recognition of

694
00:36:29.419 --> 00:36:32.760
certain kinds of complexities that might have actually improved

695
00:36:32.770 --> 00:36:37.850
public uptake of certain health messages, again, that's speculation.

696
00:36:37.860 --> 00:36:40.120
But to me, it doesn't seem crazy to think

697
00:36:40.129 --> 00:36:44.570
that uh so yeah, so um to kind of

698
00:36:44.580 --> 00:36:48.909
summarize all that uh public ignorance about science is

699
00:36:48.919 --> 00:36:51.340
a complicated issue and I think it calls for

700
00:36:51.350 --> 00:36:54.570
a sort of multi proned approach. Um I guess

701
00:36:54.580 --> 00:36:55.719
the point I want to make and this is

702
00:36:55.729 --> 00:36:57.459
one of the main points in the book is

703
00:36:57.469 --> 00:37:00.979
that if you're approaching this as a problem, your

704
00:37:00.989 --> 00:37:02.550
approach has to be as I put it in

705
00:37:02.560 --> 00:37:05.370
the book, evidence based. Um So the question is

706
00:37:05.379 --> 00:37:08.679
not what might work. The question is rather based

707
00:37:08.689 --> 00:37:11.129
on the evidence that we have at our disposal,

708
00:37:11.300 --> 00:37:14.149
what does seem to work? And those are the

709
00:37:14.159 --> 00:37:17.340
sorts of strategies that you should focus on. And

710
00:37:17.350 --> 00:37:19.629
of course, that kind of goes against the ways

711
00:37:19.639 --> 00:37:22.729
in which philosophers have tended to approach these questions

712
00:37:22.739 --> 00:37:26.080
where not always and certainly not nowadays, whether there

713
00:37:26.090 --> 00:37:28.760
are other philosophers doing what I do. But you

714
00:37:28.770 --> 00:37:30.879
know, at least in the recent past, the way

715
00:37:30.889 --> 00:37:33.209
that philosophers would approach this problem would be more

716
00:37:33.219 --> 00:37:36.209
akin to, to armchair speculation to put it slightly

717
00:37:36.219 --> 00:37:36.969
provocatively,

718
00:37:38.340 --> 00:37:43.879
right? So according to these non ideal epistemology framework,

719
00:37:44.280 --> 00:37:49.959
what should be our obligations and responsibilities as Inquirer

720
00:37:49.969 --> 00:37:52.229
or epistemic agents?

721
00:37:53.260 --> 00:37:55.330
Yeah, good. So there's a nice simple answer to

722
00:37:55.340 --> 00:37:58.879
this question uh which is that you can't answer

723
00:37:58.889 --> 00:38:00.780
it in the abstract. So, so one of the

724
00:38:00.790 --> 00:38:03.169
points I want to make in the book is

725
00:38:03.179 --> 00:38:07.250
that um it's a mistake to think as I

726
00:38:07.260 --> 00:38:12.699
think many logs seem to think that one can

727
00:38:12.709 --> 00:38:17.129
actually talk about our epistemic responsibilities or obligations in

728
00:38:17.139 --> 00:38:20.020
general, rather the right view to me seems to

729
00:38:20.030 --> 00:38:24.090
be that, um, what your responsibilities and obligations actually

730
00:38:24.100 --> 00:38:26.600
are, are going to depend on certain facts about

731
00:38:26.610 --> 00:38:29.459
you. Um So for example, it might depend on

732
00:38:29.469 --> 00:38:33.530
the sort of inquiry you're engaged in. Um, SO

733
00:38:33.540 --> 00:38:37.370
think about different kinds of scientific inquiries, for example,

734
00:38:37.379 --> 00:38:41.110
um, certain scientific inquiries call for a certain kind

735
00:38:41.120 --> 00:38:44.239
of evidence, right? So perhaps you're doing a sort

736
00:38:44.250 --> 00:38:48.149
of inquiry that calls for randomized control trials. Well,

737
00:38:48.159 --> 00:38:50.360
then, you know, one of your responsibilities is to

738
00:38:50.370 --> 00:38:53.629
conduct these randomized controlled trials, right? So other sorts

739
00:38:53.639 --> 00:38:55.989
of evidence you might have are not really going

740
00:38:56.000 --> 00:39:02.330
to be um germane to your inquiry. Um So,

741
00:39:02.340 --> 00:39:04.530
so yeah, so as I said, it's not the

742
00:39:04.540 --> 00:39:06.570
sort of question that I think on my view,

743
00:39:06.580 --> 00:39:09.889
you can really answer in general rather what you

744
00:39:09.899 --> 00:39:14.010
need to do is you need to think through

745
00:39:14.020 --> 00:39:17.580
the different kinds of responsibilities you might have in

746
00:39:17.590 --> 00:39:18.919
different situations.

747
00:39:19.810 --> 00:39:22.649
Uh OK, so a more specific question then uh

748
00:39:22.659 --> 00:39:28.929
do those situations include your social situation? Does that

749
00:39:28.939 --> 00:39:29.750
also matter?

750
00:39:29.989 --> 00:39:31.850
Well, indeed, yes. So let me, let me just

751
00:39:31.860 --> 00:39:33.729
take a step back here and kind of talk

752
00:39:33.739 --> 00:39:37.060
a little bit about why you might think that

753
00:39:37.070 --> 00:39:40.469
it's important to engage with uh challenges to your

754
00:39:40.479 --> 00:39:44.540
beliefs. So this is an idea that um is

755
00:39:44.550 --> 00:39:47.350
often dated back to, to John Stuart Mill and

756
00:39:47.360 --> 00:39:52.489
his various discussions in on liberty of the importance

757
00:39:52.500 --> 00:39:54.899
of, of freedom of expression. And one thing that's

758
00:39:54.909 --> 00:39:56.949
really interesting when you read Mill is it's quite

759
00:39:56.959 --> 00:40:00.050
clear that he's not just saying that it's important

760
00:40:00.060 --> 00:40:04.850
that people are allowed to air unorthodox views. I

761
00:40:04.860 --> 00:40:07.449
mean, he thinks that, but he also thinks that

762
00:40:07.860 --> 00:40:09.639
it's important that we listen to them and we

763
00:40:09.649 --> 00:40:12.004
take them seriously that we engage with. So his

764
00:40:12.014 --> 00:40:16.885
whole argument essentially is that it is only by

765
00:40:16.895 --> 00:40:20.344
engaging with those who disagree with us that we

766
00:40:20.354 --> 00:40:22.985
if you like earn the right to, to our

767
00:40:22.995 --> 00:40:25.304
beliefs, so they kind of thought would be that

768
00:40:25.314 --> 00:40:29.655
if you can't deal with these unorthodox views, there's

769
00:40:29.665 --> 00:40:32.584
a sense in which you're not really justified in

770
00:40:32.594 --> 00:40:36.344
having your belief, it might even become what Mill

771
00:40:36.354 --> 00:40:40.159
called a dead dogma. So to me, this is

772
00:40:40.169 --> 00:40:43.790
kind of an interesting perspective because it seems obvious

773
00:40:43.800 --> 00:40:45.550
that there is something to it, right? So this

774
00:40:45.560 --> 00:40:49.139
is one reason why Mills thought has been so

775
00:40:49.149 --> 00:40:53.500
influential even hundreds of years, well, just over 100

776
00:40:53.510 --> 00:40:56.860
years later. Um So there's definitely something to this

777
00:40:56.870 --> 00:40:58.350
and I think the obvious thing to say in

778
00:40:58.360 --> 00:40:59.649
response to the and in a way I'm saying

779
00:40:59.659 --> 00:41:01.379
the obvious thing, it's just that, well, this kind

780
00:41:01.389 --> 00:41:03.659
of goes too far, right? So the fact that

781
00:41:03.669 --> 00:41:07.679
in certain situations, it seems clear that would indeed

782
00:41:07.689 --> 00:41:11.699
be important to deal with this challenge to your

783
00:41:11.709 --> 00:41:15.899
beliefs does not mean that you always should do.

784
00:41:15.909 --> 00:41:18.239
So. So kind of following Mill, I think I

785
00:41:18.250 --> 00:41:20.550
would suggest we should think about this in terms

786
00:41:20.560 --> 00:41:24.580
of the consequences are likely consequences of engagement. So

787
00:41:24.590 --> 00:41:28.500
Mill presumably is imagining a situation where the consequence

788
00:41:28.510 --> 00:41:31.320
of engagement is, I guess either a recognition that

789
00:41:31.330 --> 00:41:32.659
you were wrong in the first place, which is

790
00:41:32.669 --> 00:41:35.830
good or a kind of better understanding of why

791
00:41:35.840 --> 00:41:37.239
you're right in the first place, which is also

792
00:41:37.250 --> 00:41:39.530
good. So for Mill, it, it's good either way.

793
00:41:39.540 --> 00:41:41.510
Um So the point to me here, it's just

794
00:41:41.520 --> 00:41:43.840
that, well, that's not always what's going to happen.

795
00:41:43.889 --> 00:41:47.215
Um There are some situations where what's going to

796
00:41:47.225 --> 00:41:48.584
happen is you end up being like a muddle,

797
00:41:48.594 --> 00:41:51.725
like you're not really sure what to think. There'll

798
00:41:51.735 --> 00:41:55.995
be some situations where you end up mistakenly concluding

799
00:41:56.004 --> 00:41:57.475
that you were wrong when in fact, you were

800
00:41:57.485 --> 00:42:01.084
right. And perhaps there'll be situations where you have

801
00:42:01.094 --> 00:42:02.774
to kind of focus a little bit more on

802
00:42:02.784 --> 00:42:05.324
the public consequences of having the debate. So it

803
00:42:05.334 --> 00:42:08.594
could be that one of the bad consequences of

804
00:42:08.604 --> 00:42:12.945
having the debate, is it misleads people in your

805
00:42:12.955 --> 00:42:15.820
audience? So that's kind of, you know, programmatically various

806
00:42:15.830 --> 00:42:19.770
ways in which the sort of engagement Mill was

807
00:42:19.780 --> 00:42:24.340
championing can, can actually have adverse consequences. So, of

808
00:42:24.350 --> 00:42:26.600
course, you know, if you want to defend, defend

809
00:42:26.610 --> 00:42:28.979
Mill here, presumably you say something like this, that,

810
00:42:28.989 --> 00:42:32.040
well, Mill wasn't claiming that in every single instance,

811
00:42:32.050 --> 00:42:34.760
you would have beneficial engagement rather, he's claiming that

812
00:42:34.770 --> 00:42:37.229
as a kind of general rule. Um YOU would

813
00:42:37.239 --> 00:42:39.820
have beneficial engagement. But I think, you know, we

814
00:42:39.830 --> 00:42:41.280
can go further here and say that, well, you

815
00:42:41.290 --> 00:42:43.870
know, that's not quite right either because there are

816
00:42:43.879 --> 00:42:46.600
certain kinds of situations where I think we can

817
00:42:46.610 --> 00:42:49.290
say as a general rule, engagement is not going

818
00:42:49.300 --> 00:42:52.510
to be beneficial. So one kind of situation is

819
00:42:52.520 --> 00:42:54.500
one where this is a debate that we've had

820
00:42:54.510 --> 00:42:57.870
many times before, right? So one reason why it's

821
00:42:57.879 --> 00:43:00.860
not obvious what is gained by debating, for example,

822
00:43:00.870 --> 00:43:03.689
to take the usual example, Holocaust Deniers is that

823
00:43:03.699 --> 00:43:06.979
this debate has happened, right? There's a huge amount

824
00:43:06.989 --> 00:43:11.699
of scholarly, scholarly literature documenting that the holocaust happened.

825
00:43:12.290 --> 00:43:13.486
So to have a debate about it now uh

826
00:43:13.595 --> 00:43:18.085
seems somewhat irrelevant because you can just point to

827
00:43:18.095 --> 00:43:21.016
that literature. So the debate happened, uh that's why

828
00:43:21.025 --> 00:43:23.976
there is this huge literature and there's no need

829
00:43:23.986 --> 00:43:26.035
to go into it again. And

830
00:43:26.055 --> 00:43:28.355
in that specific, let me just ask you in

831
00:43:28.365 --> 00:43:30.936
that specific example, do you think that perhaps one

832
00:43:30.946 --> 00:43:34.736
of the things people could worry about is that

833
00:43:34.746 --> 00:43:39.572
if we keep on having that debate, it could

834
00:43:39.582 --> 00:43:43.721
legitimate uh the question, I mean, it could legitimate

835
00:43:43.731 --> 00:43:46.461
the conspiracy theory in that.

836
00:43:46.882 --> 00:43:48.402
Well, indeed, yes, this is, this is what this

837
00:43:48.412 --> 00:43:51.461
is why I said earlier, that one thing that

838
00:43:51.471 --> 00:43:54.802
can happen as a consequence of having a debate

839
00:43:54.812 --> 00:43:58.912
is as you put it uh legitimation of um

840
00:43:59.330 --> 00:44:02.919
the the other side of that debate. Um BUT

841
00:44:02.929 --> 00:44:04.689
just getting away from the Holocaust example, which I

842
00:44:04.699 --> 00:44:06.229
just used, I think it was such an obvious

843
00:44:06.239 --> 00:44:08.989
example to make my point. Here's a slightly less

844
00:44:09.000 --> 00:44:12.149
obvious example perhaps. So I think we can say

845
00:44:12.159 --> 00:44:14.659
as a kind of maybe not entirely general rule

846
00:44:14.669 --> 00:44:18.080
but a rough and ready rule. Um DIFFERENT people

847
00:44:18.300 --> 00:44:24.580
get different uh or experience. Um Sorry, uh we

848
00:44:24.590 --> 00:44:26.139
can see it as a kind of rough and

849
00:44:26.149 --> 00:44:31.870
ready rule that um the consequences of engaging the

850
00:44:31.879 --> 00:44:35.100
debates are going to vary depending on who you

851
00:44:35.110 --> 00:44:38.459
are. So, for example, if you are a well

852
00:44:38.469 --> 00:44:42.840
spoken, well credentialed academic and you engage in a

853
00:44:42.850 --> 00:44:45.520
debate with someone, then the chances are people are

854
00:44:45.530 --> 00:44:48.000
going to be very impressed with, with your arguments.

855
00:44:48.320 --> 00:44:50.580
Perhaps you're going to even impress yourself. There's very

856
00:44:50.590 --> 00:44:53.969
little chance that in engaging with all sorts of

857
00:44:53.979 --> 00:44:57.169
opponents in debate, you end up kind of misleading

858
00:44:57.179 --> 00:44:59.149
yourself into thinking that you're wrong in the first

859
00:44:59.159 --> 00:45:01.250
place. Actually, what's more likely to happen is that

860
00:45:01.260 --> 00:45:04.179
you will end up congratulating yourself on how right

861
00:45:04.189 --> 00:45:07.260
you were all along, things are different. If you

862
00:45:07.270 --> 00:45:10.409
are someone who is along all sorts of dimensions

863
00:45:10.419 --> 00:45:15.919
less secure, perhaps you've got genuine doubts in your

864
00:45:15.929 --> 00:45:21.000
own position, engaging with all sorts of unorthodox challengers

865
00:45:21.010 --> 00:45:24.510
could solidify those doubts. And that's a problem in

866
00:45:24.520 --> 00:45:29.350
situations where perhaps you're wrong to have those doubts.

867
00:45:29.360 --> 00:45:31.080
You know, it's kind of you, you're doubting that

868
00:45:31.090 --> 00:45:33.199
you're right, but actually you very much are right.

869
00:45:33.209 --> 00:45:35.070
So one example in the book that I, I

870
00:45:35.080 --> 00:45:38.949
used to make this point is uh someone in

871
00:45:38.959 --> 00:45:44.129
a workplace situation who thinks that they've faced um

872
00:45:44.830 --> 00:45:50.239
uh discrimination because of their gender. Um So they

873
00:45:50.250 --> 00:45:53.340
think this because, you know, they, they think they're

874
00:45:53.350 --> 00:45:56.350
kind of fairly good at picking up on these

875
00:45:56.360 --> 00:45:57.989
sorts of social cues. So they think that they've

876
00:45:58.000 --> 00:46:00.949
definitely faced discrimination, but they have the natural sort

877
00:46:00.959 --> 00:46:03.840
of doubts that someone in that situation might have

878
00:46:03.850 --> 00:46:06.729
one consequence for this person of kind of taking

879
00:46:06.739 --> 00:46:10.600
seriously all the men who line up to dispute

880
00:46:10.610 --> 00:46:12.679
her interpretation of the situation is that she might

881
00:46:12.689 --> 00:46:15.199
end up thinking, oh, she, she actually was wrong,

882
00:46:15.209 --> 00:46:17.520
but of course, in many situations, she was right.

883
00:46:17.530 --> 00:46:20.429
Um So these sorts of debates can knock your

884
00:46:20.439 --> 00:46:24.889
confidence in a way that means that you, you

885
00:46:24.899 --> 00:46:27.510
no longer have the belief that you started with.

886
00:46:27.689 --> 00:46:30.610
And to me that's a kind of negative epidemic

887
00:46:30.620 --> 00:46:34.260
consequence because you've gone from having a true belief

888
00:46:34.270 --> 00:46:36.389
that you've kind of picked up based on your

889
00:46:36.399 --> 00:46:40.495
reading of the various social cues to lacking that

890
00:46:40.504 --> 00:46:43.645
true belief because you have engaged with all of

891
00:46:43.655 --> 00:46:45.655
these challenges. And of course, you know, in real

892
00:46:45.705 --> 00:46:49.014
life versions of this example, many of these challengers

893
00:46:49.024 --> 00:46:52.584
have not entirely got pure motivations either, right? So

894
00:46:52.594 --> 00:46:55.219
it's kind of a all sorts of explanations why

895
00:46:55.229 --> 00:46:58.199
um people have, have sprung up to, to challenge

896
00:46:58.209 --> 00:47:01.699
your interpretation of events. So that's a situation where

897
00:47:01.830 --> 00:47:05.500
um someone has actually had a kind of net

898
00:47:05.510 --> 00:47:09.060
epidemic loss from engaging in the sort of debate

899
00:47:09.070 --> 00:47:13.020
that that Mill thinks um is always going to

900
00:47:13.030 --> 00:47:13.899
be beneficial.

901
00:47:15.004 --> 00:47:18.385
One or two more issues that you also explore

902
00:47:18.395 --> 00:47:20.135
in the book. And I would like to address

903
00:47:20.145 --> 00:47:24.135
here. So another one of the features of our

904
00:47:24.145 --> 00:47:29.264
human psychology that is non ideal epistemological speaking is

905
00:47:29.274 --> 00:47:33.905
motivated reasoning. So how would you deal with it?

906
00:47:34.155 --> 00:47:36.385
Uh ACCORDING to your framework?

907
00:47:36.864 --> 00:47:38.314
Yes, it's a, it's a good question, I guess

908
00:47:38.324 --> 00:47:40.604
the first thing to say is that so I

909
00:47:40.614 --> 00:47:44.449
said earlier that what non idealist technology means is

910
00:47:44.459 --> 00:47:48.419
we have to recognize certain dimensions of non ideality

911
00:47:48.429 --> 00:47:52.429
ways in which actual people depart from idealized versions

912
00:47:52.439 --> 00:47:54.340
there of. So one of those dimensions is very

913
00:47:54.350 --> 00:47:57.629
much a kind of susceptibility to various sorts of

914
00:47:57.639 --> 00:48:01.600
cognitive biases. So kind of ways of misapprehending the

915
00:48:01.610 --> 00:48:04.750
world, so to speak, it's motivated reasoning. It's not

916
00:48:04.760 --> 00:48:07.189
quite a cognitive bias, but kind of roughly speaking,

917
00:48:07.199 --> 00:48:09.429
you can call it a cognitive bias. So what

918
00:48:09.439 --> 00:48:13.889
this refers to is the various ways in which

919
00:48:13.899 --> 00:48:18.399
our thinking about the world is motivated by our

920
00:48:18.409 --> 00:48:20.969
desire for the world to be a certain way.

921
00:48:20.979 --> 00:48:25.899
So human beings are not dispassionate inquirer rather we

922
00:48:25.909 --> 00:48:28.100
want certain things to be true. And that can

923
00:48:28.110 --> 00:48:31.689
lead us to kind of constructive view of the

924
00:48:31.699 --> 00:48:35.260
world that aligns with what we want to be

925
00:48:35.270 --> 00:48:37.409
the case. And the important thing to say is

926
00:48:37.419 --> 00:48:38.959
that this is meant to happen in various very

927
00:48:38.969 --> 00:48:42.000
subtle ways. The idea is not that you kind

928
00:48:42.010 --> 00:48:44.820
of simple mindedly put together this imagined version of

929
00:48:44.830 --> 00:48:46.719
the world that fits with what you want the

930
00:48:46.729 --> 00:48:49.600
world to be like. That's not psychologically realistic. Um

931
00:48:49.629 --> 00:48:53.070
Rather what we do is through various means. We

932
00:48:53.080 --> 00:48:58.510
sort of fool ourselves into um having an understanding

933
00:48:58.520 --> 00:49:01.629
of, of things that is amenable to certain purposes

934
00:49:01.639 --> 00:49:03.489
that we have and we're all familiar with this,

935
00:49:03.500 --> 00:49:06.830
right? So, um, how do you go about having

936
00:49:07.030 --> 00:49:11.189
a better impression of your friends moral qualities than

937
00:49:11.199 --> 00:49:15.229
others would have not by kind of deciding to

938
00:49:15.239 --> 00:49:17.389
think well of your friend, regardless of the evidence,

939
00:49:17.399 --> 00:49:20.030
rather by gathering all the evidence that you need

940
00:49:20.040 --> 00:49:21.729
to have a good impression of your friend's moral

941
00:49:21.739 --> 00:49:25.610
qualities. So when asked about their moral character, you

942
00:49:25.620 --> 00:49:29.510
will automatically call to mind good examples. So that

943
00:49:29.520 --> 00:49:32.669
situation where they helped out someone in need as

944
00:49:32.679 --> 00:49:35.429
opposed to bad examples, that situation where instead of

945
00:49:35.439 --> 00:49:36.879
helping out someone in need, they were on a

946
00:49:36.889 --> 00:49:40.219
fancy holiday, for example, um, you know, you can,

947
00:49:40.229 --> 00:49:42.820
you construct the evidence in such a way that

948
00:49:42.830 --> 00:49:45.889
you end up with, um, the sort of view

949
00:49:45.899 --> 00:49:48.080
you want to have. And again, I want to

950
00:49:48.090 --> 00:49:51.830
further thing to emphasize is in the psychology on

951
00:49:51.840 --> 00:49:55.340
motivated reasoning. Um The idea is that we only

952
00:49:55.350 --> 00:49:57.780
do this within certain limits, right? So we can't,

953
00:49:58.100 --> 00:50:00.379
we can't have views of the world that are

954
00:50:00.389 --> 00:50:03.520
completely at odds with the facts. Um So there

955
00:50:03.530 --> 00:50:06.409
are certain limits. So for example, if your friend

956
00:50:06.419 --> 00:50:08.780
is a truly awful person, the chances are on

957
00:50:08.790 --> 00:50:11.020
some level you'll recognize that. So you can only

958
00:50:11.030 --> 00:50:12.969
stretch the truth so much, but you can still

959
00:50:12.979 --> 00:50:16.419
stretch the truth. So that's motivated reasoning. What implications

960
00:50:16.429 --> 00:50:18.830
does that have for epistemology? Well, it makes you

961
00:50:18.840 --> 00:50:24.159
ask all sorts of questions about, um, not just

962
00:50:24.169 --> 00:50:27.719
whether our beliefs are rational or justified, but also

963
00:50:27.729 --> 00:50:29.889
how to understand those beliefs in the first place.

964
00:50:29.919 --> 00:50:32.959
So what does that raise questions about whether our

965
00:50:32.969 --> 00:50:35.550
beliefs are justified? Well, kind of put simply if

966
00:50:35.560 --> 00:50:38.949
it turns out that we kind of gather evidence

967
00:50:38.959 --> 00:50:43.360
that is designed to support our favorite view of

968
00:50:43.370 --> 00:50:46.929
the world, then that doesn't sound like a way

969
00:50:46.939 --> 00:50:49.524
of, of forming justified beliefs, you know. So it's,

970
00:50:49.534 --> 00:50:51.824
it's not that we kind of go out and

971
00:50:51.834 --> 00:50:54.455
gather the evidence and then form the belief rather

972
00:50:54.465 --> 00:50:55.804
we can have in a way, almost start with

973
00:50:55.814 --> 00:50:57.824
the belief and then go looking for evidence to

974
00:50:57.834 --> 00:51:00.074
support it. Um So the problem with that, of

975
00:51:00.084 --> 00:51:02.314
course is that you might think that whatever you

976
00:51:02.324 --> 00:51:05.235
started with, you'd find the evidence for. Um, SO

977
00:51:05.245 --> 00:51:06.475
that seems to be kind of a way in

978
00:51:06.485 --> 00:51:09.905
which our beliefs are in a certain sense unjustified.

979
00:51:10.850 --> 00:51:12.810
And this also, at least to me makes me

980
00:51:12.820 --> 00:51:15.560
ask questions about who understand beliefs in the first

981
00:51:15.570 --> 00:51:18.620
place. So we do tend to think that beliefs

982
00:51:18.629 --> 00:51:20.300
are the sorts of things that we form in

983
00:51:20.310 --> 00:51:23.610
response to evidence. Right? So kind of, that's what

984
00:51:23.620 --> 00:51:26.489
a belief is. It's a kind of evidence responsive

985
00:51:26.500 --> 00:51:27.750
thing. This is the way that a lot of

986
00:51:27.760 --> 00:51:32.540
philosophers like to talk. Um But if the literature

987
00:51:32.550 --> 00:51:34.600
or motivated reasoning is to be believed, that's not

988
00:51:34.610 --> 00:51:37.889
quite right. Um Beliefs are evidence responsive in the

989
00:51:37.899 --> 00:51:40.600
sense I mentioned earlier that they can't be too

990
00:51:40.610 --> 00:51:43.959
at odds with the evidence, but they're not necessarily

991
00:51:43.969 --> 00:51:46.340
evidence responsive in the sense that they are formed

992
00:51:46.350 --> 00:51:48.840
in response to the evidence. Again, rather, it might

993
00:51:48.850 --> 00:51:50.959
be more like we, we go and gather the

994
00:51:50.969 --> 00:51:52.719
evidence that we need to support the belief we

995
00:51:52.729 --> 00:51:56.139
already have. Um So yeah, I mean, that to

996
00:51:56.149 --> 00:51:59.100
me indicates that the motivated reasoning has got all

997
00:51:59.110 --> 00:52:03.179
sorts of interesting epistemological implications that are worth thinking

998
00:52:03.189 --> 00:52:04.419
through. And that's what I try to do in

999
00:52:04.429 --> 00:52:04.739
the book.

1000
00:52:05.459 --> 00:52:08.419
And in regards to those implications, do you think

1001
00:52:08.429 --> 00:52:11.530
that keeping in mind all of what you said

1002
00:52:11.540 --> 00:52:18.699
there dealing with motivated reasoning, epistemological necessarily leads to

1003
00:52:18.709 --> 00:52:19.800
skepticism?

1004
00:52:20.669 --> 00:52:22.620
Well, first of all, no, but let me just

1005
00:52:22.629 --> 00:52:25.050
take a step back and say what we mean

1006
00:52:25.060 --> 00:52:28.949
by skepticism here. So, so in philosophy, uh certainly

1007
00:52:28.959 --> 00:52:32.590
in epistemology, when we talk about skepticism, usually people

1008
00:52:32.600 --> 00:52:36.300
assume we're talking about like Descartes skepticism. So the

1009
00:52:36.310 --> 00:52:38.750
kind of skepticism that says that we, we don't

1010
00:52:38.760 --> 00:52:42.870
know anything. So the skeptical problem in in technology

1011
00:52:42.879 --> 00:52:44.860
is usually taken to be what is called the

1012
00:52:44.870 --> 00:52:46.939
radical skeptical problem. The problem of how to show

1013
00:52:46.949 --> 00:52:49.120
that we know anything at all. So that's not

1014
00:52:49.129 --> 00:52:52.000
the sort of skepticism, that's the issue here because

1015
00:52:52.260 --> 00:52:56.500
there's nothing in the psychological literature or motivated reasoning

1016
00:52:56.510 --> 00:52:59.649
to support that sort of radical skepticism. Um So

1017
00:52:59.659 --> 00:53:03.129
the claim is not, for example, that we don't

1018
00:53:03.139 --> 00:53:06.479
know that the earth is flat or we don't

1019
00:53:06.489 --> 00:53:07.820
know that or I don't know that my hand

1020
00:53:07.830 --> 00:53:09.629
is in front of me. That's not the sorts

1021
00:53:09.639 --> 00:53:12.570
of beliefs where motivated reasoning is meant to play

1022
00:53:12.580 --> 00:53:14.449
much of a role precisely because these are not,

1023
00:53:14.459 --> 00:53:16.159
these are not the sort of beliefs that are

1024
00:53:16.169 --> 00:53:20.320
kind of relevant to our moral political identities in

1025
00:53:20.330 --> 00:53:24.610
any obvious way. Right? Um, WHEREAS, you know, uh

1026
00:53:24.620 --> 00:53:27.530
compare my belief that I've got a hand in

1027
00:53:27.540 --> 00:53:30.889
front of me with my belief that uh raising

1028
00:53:30.899 --> 00:53:33.909
taxes would be a good thing for the UK.

1029
00:53:34.179 --> 00:53:36.459
The second belief is very different to the first

1030
00:53:36.469 --> 00:53:38.679
and there's all sorts of ways in which I

1031
00:53:38.689 --> 00:53:43.909
might just go out and assemble evidence to support

1032
00:53:43.919 --> 00:53:46.449
that belief rather than taking kind of unbiased view

1033
00:53:46.459 --> 00:53:48.699
of the situation. None of that applies to my

1034
00:53:48.709 --> 00:53:50.679
belief that I've got a hand here. Right? I

1035
00:53:50.689 --> 00:53:52.300
just, I see it. I believe I've got a

1036
00:53:52.310 --> 00:53:56.405
hand, um, we're not talking about Descartes, kind of

1037
00:53:56.415 --> 00:54:00.274
skepticism. We're talking about a much more narrowly limited

1038
00:54:00.284 --> 00:54:02.695
sort of skepticism. It doesn't affect all of our

1039
00:54:02.705 --> 00:54:05.534
beliefs. But, and this is a crucial point, it

1040
00:54:05.544 --> 00:54:07.955
affects a lot of the beliefs that are most

1041
00:54:07.965 --> 00:54:11.054
important to us. Right? So, you know, moral political

1042
00:54:11.064 --> 00:54:13.879
beliefs, these are the beliefs that really matter. And

1043
00:54:13.889 --> 00:54:15.350
if it turns out that those are the beliefs

1044
00:54:15.360 --> 00:54:18.530
where perhaps we are, we are less rational than

1045
00:54:18.540 --> 00:54:22.129
we initially assumed that would I think, make for

1046
00:54:22.139 --> 00:54:25.310
at least a politically or morally interesting kind of

1047
00:54:25.320 --> 00:54:28.399
skepticism if not necessarily a kind of purely philosophically

1048
00:54:28.409 --> 00:54:31.889
interesting form. Um So your question though was does

1049
00:54:31.899 --> 00:54:35.179
dealing with motivated reasoning necessarily lead to that sort

1050
00:54:35.189 --> 00:54:36.530
of skepticism? And my answer to that is no,

1051
00:54:36.540 --> 00:54:40.689
it doesn't. Um I think it really depends on,

1052
00:54:40.699 --> 00:54:43.129
uh well, first of all, how you understand motivated

1053
00:54:43.139 --> 00:54:45.439
reasoning. And second of all, it's really going to

1054
00:54:45.449 --> 00:54:47.919
depend on you, right? So what I've said so

1055
00:54:47.929 --> 00:54:51.040
far is that humans have a tendency to kind

1056
00:54:51.050 --> 00:54:54.129
of conform their picture of certain parts of the

1057
00:54:54.139 --> 00:54:56.610
world to what they would like the world to

1058
00:54:56.620 --> 00:54:59.350
be like. But of course, people vary in terms

1059
00:54:59.360 --> 00:55:02.159
of this tendency. Um It could be that some

1060
00:55:02.169 --> 00:55:04.679
people are less prone to it than others. Um

1061
00:55:04.909 --> 00:55:07.129
It could be that even though there is an

1062
00:55:07.139 --> 00:55:09.070
element of bias in the way in which we

1063
00:55:09.080 --> 00:55:12.840
go about gathering evidence, um It's still the case

1064
00:55:12.850 --> 00:55:15.320
that we can have gathered enough evidence that our

1065
00:55:15.330 --> 00:55:17.870
beliefs are still justified by any reasonable view of

1066
00:55:17.879 --> 00:55:21.110
justification. Um So the point is not that there

1067
00:55:21.120 --> 00:55:24.719
is this ne necessarily this jump in the skepticism.

1068
00:55:24.739 --> 00:55:27.219
The point is just that it's possible. Um So

1069
00:55:27.229 --> 00:55:29.689
there's a kind of skeptical problem that we have

1070
00:55:29.699 --> 00:55:31.580
to take seriously if we start to think of

1071
00:55:31.590 --> 00:55:33.989
it motivated reasoning and just going back to radical

1072
00:55:34.000 --> 00:55:36.770
skepticism for a second. Um So people don't think

1073
00:55:36.780 --> 00:55:40.360
that radical skepticism is an interesting philosophical problem because

1074
00:55:40.370 --> 00:55:43.469
it's true. In fact, most philosophers think it's false.

1075
00:55:43.479 --> 00:55:46.229
Uh Many think is obviously false. It's an interesting

1076
00:55:46.239 --> 00:55:51.459
problem because Descartes skeptical scenarios uh pose a certain

1077
00:55:51.469 --> 00:55:56.020
kind of challenge to how we understand our knowledge

1078
00:55:56.030 --> 00:55:57.729
of the world. And the idea being, we have

1079
00:55:57.739 --> 00:55:59.949
to answer that challenge in order to be kind

1080
00:55:59.959 --> 00:56:02.110
of vindicated in thinking, we know what we think

1081
00:56:02.120 --> 00:56:04.719
we know. I wanna say the same thing here.

1082
00:56:04.729 --> 00:56:08.260
Um This stuff motivated reasoning poses a kind of

1083
00:56:08.270 --> 00:56:11.580
challenge to whether we have the sort of political

1084
00:56:11.590 --> 00:56:13.350
knowledge. For example, that we think we have a

1085
00:56:13.360 --> 00:56:16.310
rather politically relevant knowledge. Um That challenge perhaps can

1086
00:56:16.320 --> 00:56:19.169
be answered, but this is an interesting problem precisely

1087
00:56:19.179 --> 00:56:21.320
because we have to answer the challenge. Um So

1088
00:56:21.330 --> 00:56:22.300
yeah, that's the kind of way in which I'm

1089
00:56:22.310 --> 00:56:25.080
thinking about skepticism here, not as a necessarily as

1090
00:56:25.090 --> 00:56:28.260
a conclusion, more as a problem that is worth

1091
00:56:28.270 --> 00:56:28.800
thinking about.

1092
00:56:29.959 --> 00:56:32.570
Great. So I think that this is probably a

1093
00:56:32.580 --> 00:56:36.139
good point to wrap up the interview on and

1094
00:56:36.149 --> 00:56:39.989
the book is again, non ideal epistemology. I'm leaving

1095
00:56:40.000 --> 00:56:41.820
a link to it in the description of this

1096
00:56:41.830 --> 00:56:46.500
interview and Doctor mckenna, uh Robin just before we

1097
00:56:46.510 --> 00:56:49.020
go, would you like to tell people apart from

1098
00:56:49.030 --> 00:56:51.689
the book where they can find you with your

1099
00:56:51.699 --> 00:56:53.020
work on the internet?

1100
00:56:53.939 --> 00:56:56.169
Uh Well, I assume if you type my name

1101
00:56:56.179 --> 00:56:59.530
into Google, you'll find me. I've got a website,

1102
00:56:59.610 --> 00:57:04.239
uh, Robin mckenna.weeble.com, uh, not particularly good website. Um

1103
00:57:04.590 --> 00:57:06.729
If you look for Robert mckenna, you'll find my

1104
00:57:06.739 --> 00:57:10.010
University of Liverpool profile. Um I used to have

1105
00:57:10.020 --> 00:57:12.270
an active Twitter account, but I've tried to leave

1106
00:57:12.280 --> 00:57:15.459
Twitter behind to, to focus more on more uh

1107
00:57:15.629 --> 00:57:18.219
enjoyable things. Uh But yeah, look for me online.

1108
00:57:18.229 --> 00:57:19.100
You'll, you'll find me

1109
00:57:19.510 --> 00:57:22.260
great. So thank you so much again for taking

1110
00:57:22.270 --> 00:57:23.939
the time to come on the show. It's been

1111
00:57:23.949 --> 00:57:25.010
fun to talk to you.

1112
00:57:25.310 --> 00:57:26.580
Thanks for the questions, Ricardo.

1113
00:57:27.580 --> 00:57:30.310
Hi guys. Thank you for watching this interview. Until

1114
00:57:30.320 --> 00:57:32.479
the end. If you liked it, please share it.

1115
00:57:32.489 --> 00:57:35.290
Leave a like and hit the subscription button. The

1116
00:57:35.300 --> 00:57:36.949
show is brought to you by the N Lights

1117
00:57:36.959 --> 00:57:40.169
learning and development, then differently check the website at

1118
00:57:40.179 --> 00:57:43.979
N lights.com and also please consider supporting the show

1119
00:57:44.020 --> 00:57:47.139
on Patreon or paypal. I would also like to

1120
00:57:47.149 --> 00:57:49.600
give a huge thank you to my main patrons

1121
00:57:49.610 --> 00:57:53.780
and paypal supporters, Perego Larson, Jerry Muller and Frederick

1122
00:57:53.860 --> 00:57:56.709
Suno, Bernard Seche O of Alex Adam, Castle Matthew

1123
00:57:56.719 --> 00:57:59.979
Whitten Bear. No Wolf, Tim Ho Erica LJ Condors,

1124
00:58:00.000 --> 00:58:02.770
Philip Forrest Connolly. Then the Met Robert wine in

1125
00:58:02.780 --> 00:58:06.330
NAI Z Mark Nevs calling in Holbrook Field, Governor

1126
00:58:06.620 --> 00:58:10.489
Mikel Stormer Samuel Andre Francis for Agns Ferus and

1127
00:58:10.820 --> 00:58:13.860
H her meal and Lain Jung Y and the

1128
00:58:14.030 --> 00:58:18.060
Samuel K Hes Mark Smith J. Tom Hummel S

1129
00:58:18.070 --> 00:58:21.489
Friends. David Sloan Wilson. Ya dear, Roman Roach Diego,

1130
00:58:23.070 --> 00:58:26.739
Jan Punter, Romani Charlotte, Bli Nico Barba, Adam Hunt,

1131
00:58:26.959 --> 00:58:30.129
Pavlo Stassi alek medicine, Gary G Alman Sam of

1132
00:58:30.540 --> 00:58:34.989
Zed YPJ Barboa, Julian Price Edward Hall, Eden Broner

1133
00:58:36.030 --> 00:58:39.000
Douglas Fry Franca, Beto Lati W Cortez or Solis

1134
00:58:40.020 --> 00:58:45.629
Scott Zachary FTD and W Daniel Friedman, William Buckner,

1135
00:58:45.639 --> 00:58:49.679
Paul Giorgio, Luke Loki, Georgio Theophano, Chris Williams and

1136
00:58:49.689 --> 00:58:54.310
Peter Wo David Williams, the Ausa Anton Erickson Charles

1137
00:58:54.320 --> 00:58:59.649
Murray, Alex Shaw, Marie Martinez, Coralie Chevalier, Bangalore Larry

1138
00:58:59.659 --> 00:59:04.409
Dey Junior, Old Ebon, Starry Michael Bailey. Then Spur

1139
00:59:04.419 --> 00:59:08.709
by Robert Grassy Zorn, Jeff mcmahon, Jake Zul Barnabas

1140
00:59:08.729 --> 00:59:12.429
Radick, Mark Kempel, Thomas Dvor Luke Neeson, Chris Tory

1141
00:59:12.439 --> 00:59:16.659
Kimberley Johnson, Benjamin Gilbert. Jessica. No week. Linda Brendan,

1142
00:59:16.669 --> 00:59:22.149
Nicholas Carlson, Ismael Bensley Man, George Katis Valentine Steinman,

1143
00:59:22.159 --> 00:59:27.979
Perras, Kate Van Goler, Alexander Abert Liam Dan Biar,

1144
00:59:28.310 --> 00:59:33.949
Masoud Ali Mohammadi Perpendicular Jer Urla. Good enough, Gregory

1145
00:59:33.959 --> 00:59:38.260
Hastings David Pins of Sean Nelson, Mike Levin and

1146
00:59:38.290 --> 00:59:41.534
Jos Net. A special thanks to my producers is

1147
00:59:41.544 --> 00:59:44.185
our web, Jim Frank Luca Toni, Tom Veg and

1148
00:59:44.195 --> 00:59:48.645
Bernard N Cortes Dixon, Bendik Muller Thomas Trumble Catherine

1149
00:59:48.655 --> 00:59:51.814
and Patrick Tobin, John Carl, Negro, Nick Ortiz and

1150
00:59:51.824 --> 00:59:55.104
Nick Golden. And to my executive producers, Matthew Lavender,

1151
00:59:55.235 --> 00:59:58.415
Si Adrian Bogdan Knits and Rosie. Thank you for

1152
00:59:58.425 --> 00:59:58.735
all.

