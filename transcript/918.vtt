WEBVTT

1
00:00:00.079 --> 00:00:02.740
Hello, everybody. Welcome to a new episode of the

2
00:00:02.819 --> 00:00:05.639
Center. I'm your host, Ricardo Loops. And today I'm

3
00:00:05.650 --> 00:00:09.060
joined by Doctor Antoine Marie. He is a postdoctoral

4
00:00:09.069 --> 00:00:12.170
researcher at the Department of Political Science at a

5
00:00:12.220 --> 00:00:16.079
University in Denmark. Uh We've already had an interview

6
00:00:16.090 --> 00:00:17.739
on the show. I'm leaving a link to it

7
00:00:17.750 --> 00:00:20.219
in the description box of this one where we

8
00:00:20.229 --> 00:00:25.100
talked basically about misinformation, conspiracy theories and some other

9
00:00:25.110 --> 00:00:28.129
topics. And today we're going to expand a little

10
00:00:28.139 --> 00:00:30.549
bit on that and also talk about a few

11
00:00:30.559 --> 00:00:34.430
more topics like for example, how people evaluate political

12
00:00:34.439 --> 00:00:37.750
decisions, the sharing of fake news on social media,

13
00:00:37.970 --> 00:00:41.669
gender biases in hiring processes, the repression of free

14
00:00:41.680 --> 00:00:45.854
speech, political activism. And basically, as we go through

15
00:00:45.865 --> 00:00:48.915
those topics, we're going to focus mostly on the

16
00:00:48.924 --> 00:00:54.014
biases that arise from people having strong moral convictions

17
00:00:54.025 --> 00:00:57.555
and the consequences of those moral convictions. So, Antoine,

18
00:00:57.564 --> 00:01:00.075
welcome back to the show. It's always a pleasure

19
00:01:00.084 --> 00:01:00.654
to everyone.

20
00:01:01.330 --> 00:01:03.209
Thanks a lot, Ricardo and uh thanks a lot

21
00:01:03.220 --> 00:01:03.950
for having me again.

22
00:01:05.110 --> 00:01:10.809
So starting with how people evaluate political decisions. I

23
00:01:10.819 --> 00:01:15.050
mean, is it that they care more about the

24
00:01:15.059 --> 00:01:21.050
concrete impact that certain political decisions have on society

25
00:01:21.059 --> 00:01:24.809
or do they care more about the values and

26
00:01:24.819 --> 00:01:29.169
perhaps symbols that drive those same political decisions?

27
00:01:29.860 --> 00:01:33.029
Mm. So it's a big topic. I imagine you're

28
00:01:33.040 --> 00:01:35.550
referring to uh pre print that I have with

29
00:01:35.559 --> 00:01:38.180
the co-author uh I got had and brand, brand

30
00:01:38.449 --> 00:01:41.779
Freeland who was my ph advisor, it provisionally called

31
00:01:41.790 --> 00:01:45.169
intentions and efficiency and policy variations. But I would

32
00:01:45.180 --> 00:01:48.199
like to rename maybe something like people don't care

33
00:01:48.209 --> 00:01:50.400
too much about policy efficiency and they value a

34
00:01:50.410 --> 00:01:53.300
lot, the the intentions are driving them. And so

35
00:01:53.309 --> 00:01:55.370
basically what we're doing in that, in that pretty

36
00:01:55.379 --> 00:01:58.680
simple um setup is that we present participants with

37
00:01:58.690 --> 00:02:00.690
like the new little vignette that present, you know,

38
00:02:00.699 --> 00:02:03.830
the, the sort of like the decision making process

39
00:02:03.839 --> 00:02:06.120
of a politician or a CEO who's willing to

40
00:02:06.129 --> 00:02:08.740
implement a new policy for uh for instance, I

41
00:02:08.750 --> 00:02:10.839
don't know, do you know co2 emissions and you

42
00:02:10.850 --> 00:02:14.190
know, protect the environment or whatnot? We provide information

43
00:02:14.199 --> 00:02:16.889
about the level of efficiency or impact that the

44
00:02:16.899 --> 00:02:20.059
policy has its financial cost? Is it gonna cause,

45
00:02:20.070 --> 00:02:21.610
you know, the state or the or the company

46
00:02:21.619 --> 00:02:23.869
to lose vast amounts of money or to save

47
00:02:23.880 --> 00:02:26.789
vast amounts of money and we manipulate whether the

48
00:02:26.800 --> 00:02:29.190
decision maker is altruistic motivated. You know, I, I

49
00:02:29.199 --> 00:02:30.460
want to do that because I want to help

50
00:02:30.470 --> 00:02:33.110
the issue I want to protect the environment versus

51
00:02:33.119 --> 00:02:35.210
um I really don't care about the environment. All

52
00:02:35.220 --> 00:02:37.210
I'm interested in is to improve the image of

53
00:02:37.220 --> 00:02:39.869
our company or, you know, uh increase chances that

54
00:02:39.880 --> 00:02:41.220
we get re elected at the next election or

55
00:02:41.229 --> 00:02:44.399
whatnot. And so, so it's, it's a pretty simple

56
00:02:44.410 --> 00:02:46.470
setup, you know, it does, it doesn't necessarily max

57
00:02:46.479 --> 00:02:48.279
out in terms of like external validity, but it

58
00:02:48.289 --> 00:02:51.929
allows us to pretty carefully manipulate factors that are

59
00:02:52.279 --> 00:02:54.460
seemingly of interest in how people, you know, ordinary

60
00:02:54.470 --> 00:02:57.830
people form attitudes uh about whether a political, political

61
00:02:57.839 --> 00:02:59.880
decision is good or bad, you know, laudable or

62
00:02:59.889 --> 00:03:02.529
not, et cetera. And what we tend to find

63
00:03:02.539 --> 00:03:05.289
is that even when, even when the policy is,

64
00:03:05.300 --> 00:03:08.389
is described as having, you know, absolutely anecdotal positive

65
00:03:08.399 --> 00:03:12.830
impact, like less than 1% efficiency, decreasing CO2 emissions.

66
00:03:13.130 --> 00:03:15.149
And when it described as being, you know, costing

67
00:03:15.160 --> 00:03:16.850
a huge fortune to the company or to the

68
00:03:16.860 --> 00:03:19.070
state, in other words, when the policy is described

69
00:03:19.080 --> 00:03:23.119
as being vastly inefficient, just the fact that we

70
00:03:23.130 --> 00:03:26.279
described the policy as being otherwise altruistically motivated, you

71
00:03:26.289 --> 00:03:28.160
know, in the way it's being implemented by the

72
00:03:28.169 --> 00:03:31.649
minister or, or the CEO leads most participants to

73
00:03:31.660 --> 00:03:34.070
rate the policy as being more commendable or to

74
00:03:34.080 --> 00:03:37.240
or to support it more than an alternative policy

75
00:03:37.250 --> 00:03:39.960
that is described as being super, super efficient, uh

76
00:03:39.970 --> 00:03:42.809
as you know, allowing the organization to save vast

77
00:03:42.820 --> 00:03:44.970
amounts of money. But that is described as being

78
00:03:44.979 --> 00:03:47.240
implemented for reasons that have to do with the

79
00:03:47.250 --> 00:03:50.119
pursuit of, you know, a good image being re-elected

80
00:03:50.130 --> 00:03:52.360
or whatnot. And so we think that's a little

81
00:03:52.369 --> 00:03:56.199
of concern because, you know, in uh modern societies,

82
00:03:56.210 --> 00:03:59.149
many of the motivations that private actors, but also

83
00:03:59.160 --> 00:04:02.619
potentially public actors have on the market in society

84
00:04:02.979 --> 00:04:06.589
is to um uh make profits, is to, you

85
00:04:06.600 --> 00:04:10.080
know, give themselves a good image is to potentially

86
00:04:10.089 --> 00:04:12.589
uh you know, find technological solutions that will allow

87
00:04:12.600 --> 00:04:15.610
them to become richer. And so if people punish

88
00:04:15.619 --> 00:04:20.920
or do not support um policy programs uh policy

89
00:04:20.928 --> 00:04:24.019
uh initiatives, they are motivated by, you know, the

90
00:04:24.029 --> 00:04:27.179
willingness to uh make money to become richer, to

91
00:04:27.190 --> 00:04:30.350
have a successful company, even when they're actually immensely

92
00:04:30.359 --> 00:04:33.109
beneficial for society, even when they massively, you know,

93
00:04:33.119 --> 00:04:35.649
help at solving a given problem that, you know,

94
00:04:35.660 --> 00:04:38.920
humanity is facing, that's clearly problematic. And so, well,

95
00:04:39.299 --> 00:04:41.309
we're just finding that in that in, in, in

96
00:04:41.320 --> 00:04:43.299
those in those set of studies that people don't

97
00:04:43.309 --> 00:04:45.809
seem to be very consequentialist or very pragmatic in

98
00:04:45.820 --> 00:04:48.190
the way that they rate policy decisions. They seem

99
00:04:48.200 --> 00:04:50.649
to care a lot about the intentions that drive

100
00:04:50.660 --> 00:04:52.940
the poli political policies and they seem to disregard

101
00:04:52.950 --> 00:04:56.320
a lot um the the policy impact. Now I

102
00:04:56.329 --> 00:04:58.500
should specify that the way we describe the policy

103
00:04:58.510 --> 00:05:02.420
impact is not completely neutral, we provide um uh

104
00:05:02.480 --> 00:05:06.239
information about the efficiency in numerical format. We don't

105
00:05:06.250 --> 00:05:08.230
provide graphs that is not in the current version

106
00:05:08.239 --> 00:05:12.200
of the experiments. So the defense of that choice

107
00:05:12.209 --> 00:05:14.299
is to say, well, you know, very often information

108
00:05:14.309 --> 00:05:17.260
about political policies impact is provided in numerical format,

109
00:05:17.269 --> 00:05:20.140
in journals, in newspapers, etcetera. So it's fairly ecologically

110
00:05:20.149 --> 00:05:22.290
valid. But at the same time, you could say,

111
00:05:22.299 --> 00:05:24.170
well, yeah, but no surprise that people are not

112
00:05:24.179 --> 00:05:27.160
really reacting too much to, you know, uh numerical

113
00:05:27.170 --> 00:05:30.450
descriptions of, of imbalances in policy efficiency. Because the

114
00:05:30.459 --> 00:05:33.779
the mind is not particularly equipped to, you know,

115
00:05:33.920 --> 00:05:36.010
give a lot of like intuitive meaning to figures,

116
00:05:36.019 --> 00:05:38.070
to big figures to represent all those of magnitude

117
00:05:38.079 --> 00:05:40.339
clearly et Zara. And of course, I would of

118
00:05:40.350 --> 00:05:41.730
course agree with that. And that's also one of

119
00:05:41.739 --> 00:05:43.700
the arguments in the paper, we essentially say that

120
00:05:44.339 --> 00:05:47.130
people tend to deviate from the principles of pragmatism

121
00:05:47.140 --> 00:05:50.470
when assessing uh the political policy that we expose

122
00:05:50.480 --> 00:05:52.700
them to and that we present them to because

123
00:05:52.709 --> 00:05:55.459
essentially the mind is mostly shaped by natural selection

124
00:05:55.470 --> 00:05:58.130
to react to signals about, you know, the value

125
00:05:58.140 --> 00:06:01.579
as a cooperator of potential uh social actors. Be

126
00:06:01.589 --> 00:06:04.320
it the CEO be it the minister so that

127
00:06:04.329 --> 00:06:07.049
people will spontaneously attend to, they will care whether

128
00:06:07.059 --> 00:06:09.739
or not this or that politician seems to be

129
00:06:09.750 --> 00:06:12.070
a good cooperator and they will take into account

130
00:06:12.079 --> 00:06:14.869
the intentions that animates them in implementing the policy

131
00:06:14.880 --> 00:06:17.940
a lot. But as for the the the more

132
00:06:17.950 --> 00:06:20.739
abstract, the more delayed, the more macro you know,

133
00:06:20.750 --> 00:06:23.149
level uh impact of the policy, that's not something

134
00:06:23.160 --> 00:06:24.679
that the mind in two D process is very

135
00:06:24.690 --> 00:06:28.380
well unfort unfortunately, and that we think, you know,

136
00:06:28.390 --> 00:06:31.000
the the two mechanisms combined we think are leading

137
00:06:31.010 --> 00:06:33.450
people to, to become so uh so insensitive to,

138
00:06:33.459 --> 00:06:36.130
to the principles of, of pragmatic. Yeah.

139
00:06:36.140 --> 00:06:38.799
A a and also I'm not completely sure if

140
00:06:38.809 --> 00:06:41.760
this is relevant to the particular context of this

141
00:06:41.769 --> 00:06:44.799
study and what we're you were trying to understand

142
00:06:44.809 --> 00:06:48.440
there. But I, isn't it also the case that

143
00:06:48.450 --> 00:06:55.019
for many, if not most political decisions, uh people

144
00:06:55.029 --> 00:06:58.720
many times do not really have access to very

145
00:06:58.730 --> 00:07:02.459
accurate data or perhaps sometimes it's also very hard

146
00:07:02.470 --> 00:07:07.739
to predict the precise impact that specific policies will

147
00:07:07.750 --> 00:07:09.160
have on society.

148
00:07:09.570 --> 00:07:13.200
No, of course, I mean, 100% agreement. Uh That's

149
00:07:13.209 --> 00:07:17.170
obviously the case. Um YOU know, a fact like

150
00:07:17.179 --> 00:07:20.630
is that politician good or bad as a person,

151
00:07:20.640 --> 00:07:23.399
is there more character, you know, two or three

152
00:07:23.410 --> 00:07:25.609
or three in the, in the perspective of cooperating

153
00:07:25.619 --> 00:07:29.049
with them? That's almost like a fact, something that

154
00:07:29.059 --> 00:07:31.029
you can assess, you know, by direct observation, by

155
00:07:31.040 --> 00:07:33.279
interacting with people or by seeing how they talk,

156
00:07:33.290 --> 00:07:35.899
by seeing how their, you know, claims over time

157
00:07:35.910 --> 00:07:38.260
are consistent with each other, whether they do what

158
00:07:38.269 --> 00:07:39.510
they say and they say what they do or

159
00:07:39.519 --> 00:07:42.049
what not. So that seems relatively easy to assess

160
00:07:42.329 --> 00:07:46.540
but policy efficiency, not so right. It's like very

161
00:07:46.549 --> 00:07:49.910
high level facts. It's often very much delayed in

162
00:07:49.920 --> 00:07:52.070
time. You know, you will only know if um

163
00:07:52.089 --> 00:07:54.480
a political program, you know, meant to fight. Uh

164
00:07:54.489 --> 00:07:56.829
I don't know poverty in Africa or meant to

165
00:07:56.839 --> 00:08:00.420
reduce inequalities or to uh reduce unemployment or whatnot.

166
00:08:00.640 --> 00:08:02.109
You will only know if it will bear its

167
00:08:02.119 --> 00:08:05.239
fruits in the, in the midterm future, even in

168
00:08:05.250 --> 00:08:07.980
the long term future, it interacts like a zillion

169
00:08:07.989 --> 00:08:10.820
number of variables in the national economy and in

170
00:08:10.829 --> 00:08:12.790
the global economy that you have no control over,

171
00:08:13.089 --> 00:08:15.959
it's extremely hard to run randomized control trials on

172
00:08:15.970 --> 00:08:19.329
economic outcomes or, you know, public policies, everything interacts

173
00:08:19.339 --> 00:08:22.089
with, with a bit of everything. Of course, economists

174
00:08:22.100 --> 00:08:26.399
are skilled at kind of like, you know, maximizing

175
00:08:26.410 --> 00:08:30.010
uh the possibility of like answering scientifically to those

176
00:08:30.019 --> 00:08:32.710
questions and assessing policy impact. But it's always very

177
00:08:32.719 --> 00:08:35.808
difficult for everyone, including for them. And it's obviously

178
00:08:35.820 --> 00:08:37.659
not surprising that the lay citizen, you know, the

179
00:08:37.669 --> 00:08:39.130
man in the street, the woman in the street

180
00:08:39.140 --> 00:08:43.250
is not well equipped to anticipate um you know,

181
00:08:43.260 --> 00:08:45.340
what will be the, the the macros social impact

182
00:08:45.349 --> 00:08:48.250
of a policy, I mean to, to, to infer

183
00:08:48.260 --> 00:08:50.770
to, to induce any sort of like conclusion about

184
00:08:50.780 --> 00:08:52.650
whether a policy works or doesn't work. You have

185
00:08:52.659 --> 00:08:55.409
to, you would have to ideally have access to

186
00:08:55.419 --> 00:08:57.440
facts and outcomes that are distributed on the on

187
00:08:57.450 --> 00:08:59.630
the territory, which is of course impossible to do

188
00:08:59.640 --> 00:09:03.320
individually. Uh Most people are not professional scientists, uh

189
00:09:03.330 --> 00:09:07.989
social scientists, sociologists, economists. Um AND, and even assuming

190
00:09:08.000 --> 00:09:11.119
that people have access to relatively reliable policy information,

191
00:09:11.130 --> 00:09:12.780
which is already extremely hard to do, you know,

192
00:09:12.789 --> 00:09:16.580
requires representative samples, it requires the econometric method, etcetera,

193
00:09:16.599 --> 00:09:19.960
etcetera. Even assuming that people have that information very

194
00:09:19.969 --> 00:09:22.909
often that information that rigors, uh you know, opinion

195
00:09:22.919 --> 00:09:24.909
that they may come up with about whether a

196
00:09:24.919 --> 00:09:27.400
given program works or, or or doesn't work at,

197
00:09:27.409 --> 00:09:30.909
you know, advancing a given political issue. People will

198
00:09:30.919 --> 00:09:33.159
often not have the motivation to act on the,

199
00:09:33.169 --> 00:09:34.900
on the, on the basis of that. So for

200
00:09:34.909 --> 00:09:37.559
instance, I might know that it's, you know, bad

201
00:09:37.570 --> 00:09:39.619
nowadays to eat meat and to take the plane.

202
00:09:39.630 --> 00:09:41.520
And I might know very well reflectively that I'm

203
00:09:41.530 --> 00:09:43.719
supposed to stop eating meat or at least beef

204
00:09:43.729 --> 00:09:46.419
and take the plane less. Well, it's pretty hard

205
00:09:46.429 --> 00:09:49.650
from a mot visional perspective to be completely compelled

206
00:09:49.659 --> 00:09:52.210
to act in accordance with those principles. I try,

207
00:09:52.219 --> 00:09:54.169
I try my best but very often, you know,

208
00:09:54.179 --> 00:09:56.119
I'm experiencing a sort of like weakness of will

209
00:09:56.130 --> 00:09:58.460
and I will fail to meet in practice those

210
00:09:58.469 --> 00:10:00.270
sort of like more standards that I claim to

211
00:10:00.280 --> 00:10:01.650
have and that I claim to follow.

212
00:10:02.229 --> 00:10:04.570
Mhm. Uh Let me just ask you one more

213
00:10:04.580 --> 00:10:06.979
question before we move on to another topic about

214
00:10:06.989 --> 00:10:09.030
this. And this is perhaps something that I will

215
00:10:09.039 --> 00:10:11.820
also ask you when it comes to other topics

216
00:10:11.830 --> 00:10:14.690
we're going to explore here today. But when it

217
00:10:14.700 --> 00:10:18.270
comes to this particular study, I mean, do you

218
00:10:18.280 --> 00:10:23.530
expect this to apply to basically everyone, regardless of

219
00:10:23.539 --> 00:10:28.570
their, for example, political ideology or partisanship or I

220
00:10:28.580 --> 00:10:31.400
mean, is there any way, by which perhaps there

221
00:10:31.409 --> 00:10:35.200
are some differences between, for example, left and right

222
00:10:35.210 --> 00:10:38.109
liberals and conservatives or not?

223
00:10:38.809 --> 00:10:42.549
Um, IT'S a, it's a big question. I wouldn't

224
00:10:42.559 --> 00:10:45.049
claim to be able to answer this one. So

225
00:10:45.059 --> 00:10:47.289
I'm a political psychologist but I don't really claim

226
00:10:47.299 --> 00:10:49.109
to be a specialist of like, uh, you know,

227
00:10:49.119 --> 00:10:53.059
subgroup differences or, or more, more differences across, um,

228
00:10:53.849 --> 00:10:56.669
uh, yeah, across differences of partisanship or, or more

229
00:10:56.679 --> 00:11:00.090
sensitivity. I would say that a tendency to overvalue

230
00:11:00.099 --> 00:11:03.429
intentions with respect to causes that you care about,

231
00:11:03.440 --> 00:11:06.010
you know, to, to want to positively reward or,

232
00:11:06.020 --> 00:11:08.710
or, or thank, you know, politicians or social actors

233
00:11:08.719 --> 00:11:11.570
who are helping an issue versus wanting to punish

234
00:11:11.580 --> 00:11:14.429
those who are selfish or who disregard the cause

235
00:11:14.440 --> 00:11:16.005
that you care about that will tend to be

236
00:11:16.015 --> 00:11:18.614
universal, I think because it's very likely to be

237
00:11:18.625 --> 00:11:20.815
part of our sort of like universal, you know,

238
00:11:20.825 --> 00:11:24.085
social cognition package. It's like cognitive skills that you

239
00:11:24.094 --> 00:11:27.275
need to engage all the time every day and

240
00:11:27.284 --> 00:11:28.794
that you would have had to engage, you know,

241
00:11:28.804 --> 00:11:32.445
ancestrally regularly over the millennials to gauge the trustworthiness

242
00:11:32.455 --> 00:11:35.325
of potential co-operation partners. So this you should expect

243
00:11:35.335 --> 00:11:38.229
will be so much universal. Now, at the same

244
00:11:38.239 --> 00:11:40.469
time, of course, there are differences are, you know,

245
00:11:40.479 --> 00:11:44.900
relevant morally. And so it's possible that people who

246
00:11:44.909 --> 00:11:46.700
are a bit like, you know, who have a

247
00:11:46.710 --> 00:11:49.900
bit of a effective altruist sensitivity or who are

248
00:11:49.909 --> 00:11:52.030
maybe a bit more libertarian or maybe a bit

249
00:11:52.039 --> 00:11:54.669
more on the autistic spectrum or whatnot. It's possible

250
00:11:54.679 --> 00:11:57.489
that people who have those personality characteristics who are

251
00:11:57.500 --> 00:12:01.619
a little less, you know, um, elsewhere than in

252
00:12:01.630 --> 00:12:05.250
the cognitive average of the population. It's possible that

253
00:12:05.260 --> 00:12:08.210
those people may on average spontaneously or being or,

254
00:12:08.219 --> 00:12:11.419
or maybe educated to value policy impact a bit

255
00:12:11.429 --> 00:12:13.299
more than the good principles and the intentions that

256
00:12:13.309 --> 00:12:15.030
drive them, that they may be a bit more

257
00:12:15.070 --> 00:12:18.780
psychologically compatible with the principles of utilitarianism and pragmatism.

258
00:12:19.590 --> 00:12:21.229
Uh I think there's some evidence doing that, but

259
00:12:21.239 --> 00:12:22.520
I don't claim to be a, I don't claim

260
00:12:22.530 --> 00:12:26.849
to be a specialist of that. And um and

261
00:12:26.859 --> 00:12:30.559
yeah, and as regards the policy efficiency, I think,

262
00:12:30.570 --> 00:12:33.010
yeah, most people will have difficulty, you know, representing

263
00:12:33.020 --> 00:12:36.140
uh orders of magnitudes and, and, and impacts and

264
00:12:36.150 --> 00:12:38.729
to gauge policy efficiency based on figures. But of

265
00:12:38.739 --> 00:12:40.369
course, here again, there's a bit of margin of

266
00:12:40.380 --> 00:12:42.530
leeway. If you, you know, learn for many years

267
00:12:42.539 --> 00:12:44.169
that if you work for many years at the

268
00:12:44.179 --> 00:12:46.260
Ministry of the Economy or whatnot, you learn to

269
00:12:46.270 --> 00:12:48.250
many people and to handle figures and to represent

270
00:12:48.260 --> 00:12:50.059
what they mean a bit more, you have more

271
00:12:50.070 --> 00:12:52.409
of a sense of like, you know what 10

272
00:12:52.419 --> 00:12:55.640
million represents with respect to as compared to 1

273
00:12:55.650 --> 00:12:57.640
billion. And you are be able to get a

274
00:12:57.650 --> 00:12:59.010
sense of like what is the budget of the

275
00:12:59.020 --> 00:13:00.880
state? What is the budget of the company and

276
00:13:00.890 --> 00:13:02.559
that puts you in a better position to reason

277
00:13:02.570 --> 00:13:04.809
about figures. I think, I think even in my

278
00:13:04.820 --> 00:13:06.690
conference since our French president, he is probably much

279
00:13:06.700 --> 00:13:08.690
better at reasoning about figures when it comes to

280
00:13:08.849 --> 00:13:11.919
economic. Although men than I myself, just because he

281
00:13:11.929 --> 00:13:13.440
was the minister of the economy for many years.

282
00:13:13.450 --> 00:13:15.400
And he's a professional doing that. And you would

283
00:13:15.409 --> 00:13:17.820
hope that politicians are good at doing that. And

284
00:13:17.830 --> 00:13:19.320
to be honest, I don't know what my colleagues

285
00:13:19.330 --> 00:13:21.400
in political science who work on political elites are

286
00:13:21.409 --> 00:13:23.049
finding, I hope that they are finding that they

287
00:13:23.059 --> 00:13:25.229
had a decent numerical literacy, but I'm not completely

288
00:13:25.239 --> 00:13:25.859
convinced.

289
00:13:27.000 --> 00:13:30.520
Yeah, le let's see, let's see about that. So,

290
00:13:30.770 --> 00:13:33.549
OK, so in our previous conversation, we talked a

291
00:13:33.559 --> 00:13:36.460
little bit about misinformation and just to try to

292
00:13:36.469 --> 00:13:40.530
perhaps establish a bridge between our two conversations here.

293
00:13:40.840 --> 00:13:45.000
Uh WHEN it comes to sharing partisan news on

294
00:13:45.010 --> 00:13:47.900
social media specifically. So you mean

295
00:13:47.909 --> 00:13:49.929
you mean making partisan news of, of, of, of

296
00:13:49.940 --> 00:13:53.669
news finds us sorry, like you mean like sharing,

297
00:13:53.679 --> 00:13:55.900
sharing you selectively in a way that is partisan?

298
00:13:56.369 --> 00:14:01.729
Yes, exactly. Exactly. So do do strong moral convictions

299
00:14:01.739 --> 00:14:04.390
also play a role there. And if so what

300
00:14:04.400 --> 00:14:05.989
are the consequences?

301
00:14:07.510 --> 00:14:09.909
Um Yes, they do. They absolutely do. So we

302
00:14:09.919 --> 00:14:12.309
have, we have that uh that paper recently published

303
00:14:12.320 --> 00:14:15.549
in, in Pnes Nexus with um my friends Sasha

304
00:14:15.719 --> 00:14:18.000
Altai and, and Brain Street Clan again, in which

305
00:14:18.010 --> 00:14:20.580
we essentially show using you know, relatively simple online

306
00:14:20.590 --> 00:14:23.400
experiments that the more people that, that first of

307
00:14:23.409 --> 00:14:25.580
all people are, are, are disposed to engage in

308
00:14:25.590 --> 00:14:28.020
what we call my side sharing or partisan sharing.

309
00:14:28.289 --> 00:14:31.099
They report higher willingness to share, you know, news

310
00:14:31.109 --> 00:14:34.340
items either true or false that fit the ideology.

311
00:14:34.349 --> 00:14:36.500
You know, there are congruent to their side that

312
00:14:36.510 --> 00:14:38.869
for instance, portray, you know, um a social threat

313
00:14:38.880 --> 00:14:41.075
that they are likely to believe in. Let's say

314
00:14:41.085 --> 00:14:43.755
I'm a democrat, I'm gonna want to pass along

315
00:14:43.765 --> 00:14:45.224
and I'm gonna be more likely to pass along

316
00:14:45.234 --> 00:14:47.085
the news that a piece of news that says

317
00:14:47.094 --> 00:14:48.434
that, I don't know, there's a big problem but

318
00:14:48.445 --> 00:14:51.284
like uh 3d printing of guns because it's gonna

319
00:14:51.294 --> 00:14:53.354
cause, you know, the spread of, of, of, of,

320
00:14:53.364 --> 00:14:55.484
of like homemade guns in the US or whatnot.

321
00:14:55.804 --> 00:14:57.474
So people tend to share news that are con

322
00:14:57.655 --> 00:15:02.515
to their side uh plausible threats. Um Statistical facts

323
00:15:02.525 --> 00:15:04.234
that they think are true. You know, for instance,

324
00:15:04.244 --> 00:15:06.515
um you should take a bunch of like a

325
00:15:06.525 --> 00:15:09.875
liberal participants. They are fairly likely to pass along.

326
00:15:10.299 --> 00:15:12.219
Uh A news item that says that this, for

327
00:15:12.229 --> 00:15:15.520
instance, you know, five or 10% gender pay gap

328
00:15:15.530 --> 00:15:17.159
between men and women in the US, which is

329
00:15:17.169 --> 00:15:19.559
roughly true, right? But they're also pretty likely to

330
00:15:19.570 --> 00:15:21.530
want to share um a piece of fake news

331
00:15:21.539 --> 00:15:23.359
that will say that there's a 60% pay gap

332
00:15:23.369 --> 00:15:24.950
between men and women, which is false, which is

333
00:15:24.960 --> 00:15:29.880
vastly exaggerated. Right. So, yeah, news items, claims, uh

334
00:15:29.890 --> 00:15:33.359
rumors, true or false when they fit people's uh

335
00:15:33.369 --> 00:15:36.780
prior expectations and moral commitments, you know, uh political

336
00:15:36.789 --> 00:15:41.010
convictions, um, activist motivations or whatnot uh will tend

337
00:15:41.020 --> 00:15:43.080
to be passed along more in real life and

338
00:15:43.090 --> 00:15:45.760
social media. The, the, the studies that are included

339
00:15:45.770 --> 00:15:47.059
in the paper that I'm talking about are only

340
00:15:47.070 --> 00:15:48.580
on social media, but we also know that that's

341
00:15:48.590 --> 00:15:51.859
the case in, in real life. Um So, yeah,

342
00:15:51.869 --> 00:15:54.400
people have the tendency that 10 is amplified by

343
00:15:54.409 --> 00:15:56.599
uh the degree to which demoralize or that they

344
00:15:56.609 --> 00:15:58.590
deem you know, certain issue to be of like

345
00:15:58.599 --> 00:16:00.859
political priority for them. So whether, you know, they

346
00:16:00.869 --> 00:16:04.130
see a political issue, gun control abortion or, or

347
00:16:04.140 --> 00:16:06.000
the, or the ban on abortion, for instance, for,

348
00:16:06.010 --> 00:16:08.799
for conservatives as being like a core value for

349
00:16:08.809 --> 00:16:10.659
their moral identity, for instance, or are being central

350
00:16:10.669 --> 00:16:13.640
to their moral identity when people, you know, rate.

351
00:16:13.650 --> 00:16:16.169
Um I mean, report that they highly more or

352
00:16:16.179 --> 00:16:18.479
less an issue that amplifies identity to share uh

353
00:16:18.489 --> 00:16:22.549
new selectively. Uh And we also find unsurprisingly that

354
00:16:22.559 --> 00:16:24.330
people who are more extreme on an issue, even

355
00:16:24.340 --> 00:16:26.690
when they don't moralize, it are more likely to

356
00:16:26.700 --> 00:16:28.599
uh to share news related to that issue in

357
00:16:28.609 --> 00:16:31.549
ways that are, that are partisan. So what does

358
00:16:31.559 --> 00:16:34.830
that mean? Um It can mean a mix of

359
00:16:34.840 --> 00:16:39.400
several things. Um The spontaneous interpretation would simply be

360
00:16:39.409 --> 00:16:41.770
to say that, you know, the more people care

361
00:16:41.780 --> 00:16:44.640
about an issue. They want, people want to tackle

362
00:16:44.650 --> 00:16:47.010
a social problem. They want to, with the more

363
00:16:47.020 --> 00:16:48.969
they want to advance or cause the, the more

364
00:16:48.979 --> 00:16:51.770
they are to behave like little activists, like little,

365
00:16:51.780 --> 00:16:53.609
you know, militant members of a tribe. And they

366
00:16:53.619 --> 00:16:57.859
want to pass along, you know, um claims information

367
00:16:57.869 --> 00:16:59.869
that they think will mobilize other people to take

368
00:16:59.880 --> 00:17:03.159
action or that potentially can also um they, they

369
00:17:03.169 --> 00:17:05.199
can also pass along those militant claims because they

370
00:17:05.209 --> 00:17:07.780
expect that they will therefore be able to signal,

371
00:17:07.790 --> 00:17:10.150
you know, a group memberships. I think both motivations

372
00:17:10.160 --> 00:17:13.858
are taking place. I think both mechanisms uh are

373
00:17:13.868 --> 00:17:18.089
driving part sharing to some extent. Um But we

374
00:17:18.098 --> 00:17:22.579
admit that it's also unclear whether um people are

375
00:17:22.589 --> 00:17:25.118
not also simply sharing uh you know, news items

376
00:17:25.130 --> 00:17:28.140
in partition ways because they think that the message

377
00:17:28.150 --> 00:17:29.630
that the, that the claim and caption, it is

378
00:17:29.640 --> 00:17:31.920
more plausible. So there's a sort of like there's

379
00:17:31.930 --> 00:17:35.910
a sort of like, you know, um it's hard

380
00:17:35.920 --> 00:17:38.510
to tell to what extent partisan safety sharing is

381
00:17:38.520 --> 00:17:42.229
carried forward by stronger prior beliefs or strong convictions

382
00:17:42.239 --> 00:17:45.420
that the claim is true versus more motivated thinking

383
00:17:45.430 --> 00:17:48.560
or more political or, or more motivations that, that,

384
00:17:48.569 --> 00:17:50.430
that sort of like motivate people to act in

385
00:17:50.439 --> 00:17:53.109
somewhat activist ways by signaling allegiances and by trying

386
00:17:53.119 --> 00:17:55.410
to influence the the belief of others about those,

387
00:17:55.420 --> 00:17:58.880
those um those social topics potentially to, to mobilize

388
00:17:58.890 --> 00:18:01.079
them or to prompt them to act, that's a

389
00:18:01.089 --> 00:18:03.540
little unclear. Uh And in that paper, we're not

390
00:18:03.550 --> 00:18:05.810
in a position to, to tell what is the

391
00:18:05.819 --> 00:18:07.709
relative contribution of the two mechanisms. And there's a

392
00:18:07.719 --> 00:18:11.050
big debate right now in political behavior, political psychology,

393
00:18:11.060 --> 00:18:14.510
political science about the extent to which um you

394
00:18:14.520 --> 00:18:18.520
know, differences between partisans in behavior, in news consumption,

395
00:18:18.900 --> 00:18:21.500
in judgments of accuracy of a given claim in

396
00:18:21.630 --> 00:18:23.739
decisions to share news, et cetera. To what extent

397
00:18:23.750 --> 00:18:27.140
those differences between partisans between political subgroups are explained

398
00:18:27.150 --> 00:18:31.359
by, you know, political motivated thinking, instrumental, instrumental motives

399
00:18:31.369 --> 00:18:34.349
as opposed to more, uh you know, accuracy oriented,

400
00:18:34.359 --> 00:18:36.880
just differences in part of beliefs. Uh It's a

401
00:18:36.890 --> 00:18:38.640
big, it's a big debate right now. I can

402
00:18:38.650 --> 00:18:41.540
only recommend your uh your viewers to check out

403
00:18:41.550 --> 00:18:43.439
the work of Ben Tappin, which has done a

404
00:18:43.449 --> 00:18:45.119
lot of good work with the Golden Penny Cook

405
00:18:45.130 --> 00:18:47.520
and David ran about that. But they try to,

406
00:18:47.530 --> 00:18:50.560
to argue that much, many instances of what seem

407
00:18:50.569 --> 00:18:53.430
to be partisan motivated thinking might actually boil down

408
00:18:53.439 --> 00:18:55.880
to different in priors. I think, I think the

409
00:18:55.890 --> 00:18:57.900
argument that they make is pretty convincing. I still

410
00:18:57.910 --> 00:18:59.910
think that motivated thinking is real. I still think

411
00:18:59.920 --> 00:19:02.469
that it's very real. Um But it's clearly the

412
00:19:02.479 --> 00:19:04.119
case that a lot in a lot of like

413
00:19:04.130 --> 00:19:06.699
survey contexts, a lot of like social science context,

414
00:19:06.890 --> 00:19:09.599
what might be taken at face value as evidence

415
00:19:09.609 --> 00:19:11.750
of motivated thinking might actually boil down to just

416
00:19:11.760 --> 00:19:13.859
partisan disagreements in what is true in the world

417
00:19:13.869 --> 00:19:15.880
and what is plausible piece of information. You know.

418
00:19:16.739 --> 00:19:18.459
Now I think you asked me, you, you, you

419
00:19:18.469 --> 00:19:21.739
asked me about the consequences of moralizing the topic

420
00:19:21.750 --> 00:19:23.930
and, and the, the impact of uh of that

421
00:19:23.939 --> 00:19:26.770
on, on, on, on, on partisan sharing of, of

422
00:19:26.780 --> 00:19:29.160
news items. So, yeah, as I said, it amplifies

423
00:19:29.170 --> 00:19:32.949
your, your propensity to, to effectively share news that

424
00:19:32.959 --> 00:19:36.119
fit your ideology. But in the paper, we, we

425
00:19:36.130 --> 00:19:38.489
don't just describe uh we don't just register that

426
00:19:38.500 --> 00:19:41.900
fact. We also try to um test to what

427
00:19:41.910 --> 00:19:43.719
extent it is robust and to what extent it

428
00:19:43.729 --> 00:19:45.619
can be kind of like influenced or mitigated by

429
00:19:45.630 --> 00:19:49.930
interventions. And so when we manipulated the the perceived

430
00:19:49.939 --> 00:19:52.010
um audience of the sharing, is it the group?

431
00:19:52.020 --> 00:19:53.839
Is it the art group didn't seem to have

432
00:19:53.849 --> 00:19:55.589
much of an effect? But I can come back

433
00:19:55.599 --> 00:19:57.229
to that because we have another paper in which

434
00:19:57.239 --> 00:19:59.930
the my machine works. Uh We in the same

435
00:19:59.939 --> 00:20:02.000
paper with ASHA Alai and, and BR Mle, we

436
00:20:02.010 --> 00:20:05.150
also manipulated whether the people are imagining to share

437
00:20:05.160 --> 00:20:08.109
news from a anonymous personal account, didn't have any

438
00:20:08.119 --> 00:20:10.660
effect on the the patterns of sharing, still engaging

439
00:20:10.670 --> 00:20:13.750
in public but sharing. And towards the end of

440
00:20:13.760 --> 00:20:16.160
the paper, we also tested an intervention which in

441
00:20:16.170 --> 00:20:18.459
which we told participants that essentially they have a

442
00:20:18.469 --> 00:20:21.489
confirmation bias and that, that may potentially bias, you

443
00:20:21.500 --> 00:20:24.790
know, their new sharing decisions. And the last experiment

444
00:20:24.800 --> 00:20:26.630
in which we tell them that they also have

445
00:20:26.640 --> 00:20:28.699
a sharing bias that they have a propensity to

446
00:20:28.709 --> 00:20:31.550
share more stuff that align with their, with their

447
00:20:31.560 --> 00:20:33.880
ideology that tells them what they want to hear.

448
00:20:34.560 --> 00:20:39.089
And neither of those, uh, two last interventions really

449
00:20:39.099 --> 00:20:40.780
had much of an effect. I think they tended

450
00:20:40.790 --> 00:20:43.709
to reduce a little bit overall showing but small

451
00:20:43.719 --> 00:20:47.319
effect, very small effect. It didn't, uh, reduce people's

452
00:20:47.329 --> 00:20:49.939
propensity to share news selectively. It didn't make people

453
00:20:49.949 --> 00:20:52.359
more likely to share incongruent news and less likely

454
00:20:52.369 --> 00:20:54.800
to share congruent news. Ok. So what it would,

455
00:20:54.810 --> 00:20:57.189
what all that suggests, I think is that selective

456
00:20:57.199 --> 00:21:00.510
sharing and its amplification by moralization and actually extremity

457
00:21:00.520 --> 00:21:03.069
is very robust. It's something that is deeply entrenched

458
00:21:03.079 --> 00:21:07.140
in people's cognitive uh dispositions and that makes sense

459
00:21:07.150 --> 00:21:11.880
if um it's underpinned by more motivations, we know

460
00:21:11.890 --> 00:21:13.930
that more motivations are trade off insensitive, right? If

461
00:21:13.939 --> 00:21:16.560
you really care about defending abortion, if you are,

462
00:21:16.569 --> 00:21:19.920
you know, a, a convinced conservative, why would you,

463
00:21:19.930 --> 00:21:21.780
why would you swayed in your moral in your

464
00:21:21.790 --> 00:21:24.439
political fight by a small, you know, intervention, you

465
00:21:24.449 --> 00:21:26.900
know, sort of experiments, of course, that's unlikely to

466
00:21:26.910 --> 00:21:28.380
have much of an effect because, you know, we

467
00:21:28.390 --> 00:21:32.079
are touching upon deeply entrenched, um you know, identity

468
00:21:32.089 --> 00:21:35.479
relevant moral convictions. And if, and to the extent

469
00:21:35.489 --> 00:21:38.920
that the part is sharing is driven by sincere

470
00:21:38.930 --> 00:21:41.790
convictions about, you know, there being a threat associated

471
00:21:41.800 --> 00:21:45.050
to a given issue or um beliefs about the

472
00:21:45.060 --> 00:21:46.329
fact that the art group is really evil and

473
00:21:46.339 --> 00:21:49.410
really stupid. Well, here again, it's not a small

474
00:21:49.420 --> 00:21:51.380
intervention in a sort of experiment that is gonna

475
00:21:51.390 --> 00:21:54.430
sway that belief that you have acquired over the

476
00:21:54.439 --> 00:21:57.819
years, over, over, you know, many repeated trials of

477
00:21:57.829 --> 00:22:00.839
like consuming partisan media and conversations with people, etcetera,

478
00:22:00.849 --> 00:22:03.530
etcetera. So in general, it's very hard to change

479
00:22:03.540 --> 00:22:07.060
uh people's news diets, uh intuitive reactions to political

480
00:22:07.069 --> 00:22:10.930
messages simply because they have a long biographical history

481
00:22:10.989 --> 00:22:13.369
and they are deeply entrenched either in people's expectations

482
00:22:13.380 --> 00:22:15.430
about the world or their political commitments and that's

483
00:22:15.439 --> 00:22:17.270
hard to modify and surprisingly.

484
00:22:17.810 --> 00:22:20.829
Mhm. So it's, it seems to me that it's

485
00:22:20.839 --> 00:22:24.010
difficult, at least as far as we know to

486
00:22:24.020 --> 00:22:28.780
curb this tendency that most people have to share

487
00:22:28.880 --> 00:22:34.849
strongly partisan news, including sometimes fake news. Right. Yeah.

488
00:22:34.979 --> 00:22:38.930
Yeah. Yeah. Um IT is generally difficult. Um SOME

489
00:22:38.939 --> 00:22:41.650
people like uh Sasha Altai and Hugo Messi would

490
00:22:41.660 --> 00:22:45.949
even argue that, you know, should we be spending

491
00:22:45.959 --> 00:22:48.459
that much uh of our work and our, you

492
00:22:48.469 --> 00:22:50.900
know, money on trying to come up with like

493
00:22:50.959 --> 00:22:55.469
interventions and get misinformation. Um The spread of misinformation

494
00:22:55.479 --> 00:22:57.369
is to begin with not that big of a

495
00:22:57.380 --> 00:23:00.390
problem. At least in the West, people don't consume

496
00:23:00.400 --> 00:23:02.069
that much uh fake news. You know, most of

497
00:23:02.079 --> 00:23:03.849
the fake news are shared by a tiny little

498
00:23:03.859 --> 00:23:07.050
majority of Twitter users and Facebook and Facebook users

499
00:23:07.390 --> 00:23:09.369
does the work for instance of uh you know,

500
00:23:09.380 --> 00:23:12.420
um Matias Osmundsen and Michael Ben Pearson and others,

501
00:23:12.430 --> 00:23:14.560
you know, who have a American Political Science review

502
00:23:14.569 --> 00:23:17.160
that shows that most of the spreaders of part

503
00:23:17.199 --> 00:23:19.640
fake news are actually less than 1% of the

504
00:23:19.650 --> 00:23:22.500
Twitter sample that they could get access to. And

505
00:23:22.510 --> 00:23:25.069
that, that less than 1% is showing 90% of

506
00:23:25.079 --> 00:23:26.640
the fake news or something like that. So it's

507
00:23:26.910 --> 00:23:30.260
on the whole, in addition to the fact that

508
00:23:30.270 --> 00:23:32.239
few fake news are being shared in the, in

509
00:23:32.250 --> 00:23:35.410
the sample. Ok. So there's few, there's little circulation

510
00:23:35.420 --> 00:23:37.839
of misinformation overall, you know, a lot of studies

511
00:23:37.849 --> 00:23:39.859
and that that's what they find. And a very

512
00:23:39.869 --> 00:23:42.979
small minority of like pretty cynical activists seem to

513
00:23:42.989 --> 00:23:44.900
be doing the heavy lifting of sharing that information.

514
00:23:44.910 --> 00:23:49.040
So maybe we're exaggerating the seriousness of misinformation. It

515
00:23:49.050 --> 00:23:50.560
doesn't mean that it's not a problem at all.

516
00:23:50.569 --> 00:23:52.660
It is for sure a problem. It is a

517
00:23:52.670 --> 00:23:55.069
problem that, for instance, you know, people increasingly uh

518
00:23:55.079 --> 00:23:56.959
in some areas of the world are dispersed full

519
00:23:56.969 --> 00:23:59.439
of vaccines. It is a problem that the Russians

520
00:23:59.449 --> 00:24:03.189
are spreading, you know, anti-french propaganda uh in Africa

521
00:24:03.199 --> 00:24:05.839
and taking advantage of that, of those misinformation campaigns

522
00:24:05.849 --> 00:24:08.150
to send their Wagner soldiers and to increase their

523
00:24:08.160 --> 00:24:10.319
influence in West Africa. That's of course a problem.

524
00:24:10.750 --> 00:24:14.609
But as far as like the circulation of misinformation

525
00:24:14.619 --> 00:24:17.150
on social media about politics in the West is

526
00:24:17.160 --> 00:24:19.800
concerned is possible that we're exaggerating a little bit.

527
00:24:19.810 --> 00:24:22.130
Uh THE seriousness of the problem. And so what,

528
00:24:22.150 --> 00:24:24.479
what people like Sasha Alta and Yuma would say,

529
00:24:24.489 --> 00:24:25.989
and I'm, and I'm sure they're, they're right if

530
00:24:26.000 --> 00:24:28.000
they thought about it more than I have, what

531
00:24:28.010 --> 00:24:29.819
we should instead say is to try to focus

532
00:24:29.829 --> 00:24:32.160
our efforts and our time and our money on

533
00:24:32.170 --> 00:24:36.790
making people care more about trusting uh reliable sources.

534
00:24:36.800 --> 00:24:40.239
OK. Like increasing people's trust in uh the Washington

535
00:24:40.250 --> 00:24:42.550
Post, the New York Times, Le Monde le Figaro,

536
00:24:42.800 --> 00:24:45.369
uh El Pais and whatnot. OK. So the priority

537
00:24:45.380 --> 00:24:47.640
should not be in trying to fight misinformation. It's

538
00:24:47.650 --> 00:24:50.640
hard. It may, it may even backfire by reinforcing

539
00:24:50.650 --> 00:24:54.069
conspiracy theories and whatnot. The priority should be to

540
00:24:54.079 --> 00:24:56.989
try to restore a little bit trust in institutions

541
00:24:57.000 --> 00:24:58.880
and trust in journalism. And if we, and if

542
00:24:58.890 --> 00:25:01.829
we manage to increase a little bit people's trust

543
00:25:01.839 --> 00:25:04.630
in a legitimate legacy news media, if we manage

544
00:25:04.640 --> 00:25:07.430
to make them consume a bit more reliable information,

545
00:25:07.640 --> 00:25:09.869
we would diminish the amount of false beliefs and

546
00:25:09.880 --> 00:25:12.369
misinformation in society much more than we would if

547
00:25:12.380 --> 00:25:14.849
we manage to make people a bit more distrustful

548
00:25:14.859 --> 00:25:16.810
of uh you know, fake news sources and whatnot.

549
00:25:17.119 --> 00:25:18.910
At least that's the argument that they defend. So

550
00:25:18.920 --> 00:25:20.770
I don't know if it answers a bit what

551
00:25:20.780 --> 00:25:24.180
you were saying, but that's my uh interpretation of

552
00:25:24.189 --> 00:25:25.880
uh I mean, what would be said, what would

553
00:25:25.890 --> 00:25:26.780
have to be said here?

554
00:25:27.020 --> 00:25:29.900
Mhm. Yeah. And actually I have uh seal t

555
00:25:29.989 --> 00:25:32.109
on the show two or three years ago and

556
00:25:32.119 --> 00:25:35.020
when it comes to the seriousness of the sharing

557
00:25:35.030 --> 00:25:38.510
of fake news online in social media, particularly, uh

558
00:25:38.530 --> 00:25:42.469
if I remember correctly said basically the same you

559
00:25:42.479 --> 00:25:45.729
said there. And I, I think that uh you

560
00:25:45.739 --> 00:25:49.020
were focusing mostly on people who share fake news

561
00:25:49.030 --> 00:25:52.140
there. But uh it also applies to people who

562
00:25:52.150 --> 00:25:55.880
actually produce fake news and put, put it into

563
00:25:55.890 --> 00:26:00.329
circulation on the internet. It's just a tiny, tiny,

564
00:26:00.339 --> 00:26:04.069
tiny minority of people doing that. Right.

565
00:26:04.510 --> 00:26:06.489
Yeah, I imagine so. I haven't, I haven't worked

566
00:26:06.500 --> 00:26:08.560
on like, you know, the, the famous producers, like

567
00:26:08.569 --> 00:26:11.640
the websites who, uh, you know, who output fake

568
00:26:11.650 --> 00:26:14.010
news online, of course, they're pursuing, you know, clicks

569
00:26:14.020 --> 00:26:16.270
and, and attention and revenue when they're here for

570
00:26:16.280 --> 00:26:17.880
the, for the money, for the most part, they

571
00:26:17.890 --> 00:26:19.859
might also have, you know, disruptive motivations, like more

572
00:26:19.869 --> 00:26:23.439
political motivations to undermine trust in the establishment or

573
00:26:23.449 --> 00:26:25.540
in journalism or in, you know, the current politicians

574
00:26:25.550 --> 00:26:30.089
or whatnot. Um, I, I don't know, I don't

575
00:26:30.099 --> 00:26:32.959
know what I mean here again, I think, you

576
00:26:32.969 --> 00:26:35.410
know, the, the, the, the, the, the proportion of

577
00:26:35.420 --> 00:26:40.199
information producers who are intentionally cynical, intentionally spreading misinformation

578
00:26:40.209 --> 00:26:43.739
is extremely small. Uh Of course, it's absolutely problematic

579
00:26:43.750 --> 00:26:45.189
that they exist and there should be, you know,

580
00:26:45.199 --> 00:26:48.250
uh probably penal sanctions against them and, you know,

581
00:26:48.260 --> 00:26:50.099
the, the government should crack down on them at

582
00:26:50.109 --> 00:26:53.000
Zara, but it's easy to overestimate the gravity of

583
00:26:53.010 --> 00:26:58.380
the issue simply because, you know, um the mind

584
00:26:58.390 --> 00:27:01.260
is extremely uh uh you know, hyper sensitive to,

585
00:27:01.270 --> 00:27:03.219
to threat in general. So that's true of like

586
00:27:03.229 --> 00:27:05.569
citizens. That's true of journalists, that's true of politicians

587
00:27:05.579 --> 00:27:08.640
or researchers. So any information that somehow contributes to

588
00:27:08.650 --> 00:27:10.119
saying that there is something that is not working

589
00:27:10.130 --> 00:27:12.739
well in society will be a bit more, you

590
00:27:12.750 --> 00:27:15.979
know, believed a bit more uh culturally successful uh

591
00:27:15.989 --> 00:27:19.469
holding constant other factors. Of course, I'm not saying

592
00:27:19.479 --> 00:27:21.589
that you can make people believe, you know, anything

593
00:27:21.599 --> 00:27:24.189
and everything, people aren't completely good. But if the

594
00:27:24.199 --> 00:27:27.890
threat seems plausible and if it seems relevant, uh

595
00:27:27.900 --> 00:27:29.920
it's gonna spread far and wide. And so there's

596
00:27:29.930 --> 00:27:33.020
a tendency for the broad public and, and, and,

597
00:27:33.030 --> 00:27:35.729
and, and sentence themselves to, to think that it,

598
00:27:36.270 --> 00:27:38.290
to think that the, the problem of misinformation is

599
00:27:38.300 --> 00:27:41.209
very broad is very big and they indicate can

600
00:27:41.219 --> 00:27:44.060
potentially also have, you know, a career incentives, publication

601
00:27:44.069 --> 00:27:46.869
incentives to also further exa exaggerate the threat. So

602
00:27:46.880 --> 00:27:48.130
I think we have to keep that in mind.

603
00:27:48.140 --> 00:27:50.750
Yeah, like the mind is super attentive to threat,

604
00:27:50.930 --> 00:27:53.890
we easily exaggerate things. It's relevant, right? Makes us

605
00:27:53.900 --> 00:27:57.500
sound interesting in the conversation and it creates opportunities

606
00:27:57.510 --> 00:28:00.420
for publication for talks for uh you know, a

607
00:28:00.430 --> 00:28:03.770
commission that the parliament or what not. And um

608
00:28:03.780 --> 00:28:05.229
and that will, that will make sense. I mean,

609
00:28:05.239 --> 00:28:07.250
I'm not necessarily, you know, throwing stones at people

610
00:28:07.260 --> 00:28:09.689
who participate in that I have perhaps participated in

611
00:28:09.699 --> 00:28:12.349
that kind of like more panic about misinformation myself

612
00:28:12.359 --> 00:28:14.439
intentionally or not. But we should keep in mind

613
00:28:14.449 --> 00:28:16.270
that. Yeah, the, the, the problem is probably not

614
00:28:16.280 --> 00:28:19.140
as serious as the climate changes or the, the

615
00:28:19.150 --> 00:28:21.079
war in uh in Israel is.

616
00:28:21.729 --> 00:28:24.739
Mhm. So, in a related topic, uh in our

617
00:28:24.750 --> 00:28:27.949
previous conversation, we've also talked a little bit about

618
00:28:27.959 --> 00:28:31.329
conspiracy theories. And in this time, I would like

619
00:28:31.339 --> 00:28:34.510
to ask you about specifically when it comes to

620
00:28:34.520 --> 00:28:41.209
political conspiracy theories and other kinds of rebased narratives.

621
00:28:41.369 --> 00:28:47.109
Are there social functions to holding and disseminating them?

622
00:28:48.660 --> 00:28:51.109
Yeah, presumably at least that's what we argue with

623
00:28:51.119 --> 00:28:55.020
uh Mika Pearson and, and others, but Mikel Bang,

624
00:28:55.030 --> 00:28:57.099
in particular, in, in at least a couple of

625
00:28:57.109 --> 00:29:01.739
pieces that we have written together. Um So again,

626
00:29:01.750 --> 00:29:04.180
the mind is very, is very susceptible to threatening

627
00:29:04.189 --> 00:29:06.500
information, right? Uh We are a loss of earth.

628
00:29:06.510 --> 00:29:09.099
It makes sense to, you know, um get ourselves

629
00:29:09.109 --> 00:29:12.150
ready for, for threatening a threat. So it's obviously

630
00:29:12.160 --> 00:29:14.900
adaptive to overreact to information that there may be

631
00:29:14.910 --> 00:29:18.150
a social threat um in particular social threats that,

632
00:29:18.160 --> 00:29:20.400
you know, take a group based form, right? Because

633
00:29:20.410 --> 00:29:22.819
uh we humans evolved in an essential environment in

634
00:29:22.829 --> 00:29:24.979
which presumably there was a fair amount of group

635
00:29:24.989 --> 00:29:28.510
competition and group conflict. Um And so it's, it's

636
00:29:28.520 --> 00:29:31.500
adaptive to overreact to be kind of like on

637
00:29:31.510 --> 00:29:34.030
the on the lookout for potential or coordinate social

638
00:29:34.040 --> 00:29:36.670
threats directed against us, directed against our in group

639
00:29:36.680 --> 00:29:39.500
are keen and Sarah and once people have that

640
00:29:39.510 --> 00:29:43.140
predisposition to find, you know, threat based narratives, appealing

641
00:29:43.150 --> 00:29:47.400
like conspiracy theories, um potentially also some popular forms

642
00:29:47.410 --> 00:29:51.390
of like uh Marxism, anti racist discourse, anti-communist discourse,

643
00:29:51.400 --> 00:29:54.979
et cetera, much of ideologies, threat based narratives, right?

644
00:29:54.989 --> 00:29:58.420
I mean, whether you're trying to, you know, um

645
00:29:58.430 --> 00:30:02.020
motivate people to um take action against communism or

646
00:30:02.030 --> 00:30:04.199
whether you're trying to motivate your population to take

647
00:30:04.209 --> 00:30:07.819
actions against the capitalist imperialism and to endorse communism

648
00:30:08.060 --> 00:30:09.579
In all those cases, you're going to try to

649
00:30:09.589 --> 00:30:12.739
spin threat based narratives and to, you know, uh

650
00:30:12.750 --> 00:30:15.640
diffuse them in society through the journals of propaganda

651
00:30:15.650 --> 00:30:18.099
through the channels of general education, through the, the

652
00:30:18.109 --> 00:30:20.920
radio, et cetera. So threat based narratives are absolutely

653
00:30:20.930 --> 00:30:23.719
central in politics and in, in, in, in ideological

654
00:30:23.729 --> 00:30:27.069
phenomena. And so once you have that ubiquitous with

655
00:30:27.089 --> 00:30:30.369
based narratives uh that, you know, uh the mind

656
00:30:30.380 --> 00:30:34.189
responds to and finds intuitive attention grabbing and uh

657
00:30:34.199 --> 00:30:37.920
well pay attention to it becomes possible to use

658
00:30:37.930 --> 00:30:41.969
them instrumentally strategically to influence how people think of

659
00:30:41.979 --> 00:30:45.199
yourself and, and, and, and, and act. So if

660
00:30:45.209 --> 00:30:46.594
I, if I had a good, you know, threat

661
00:30:46.604 --> 00:30:49.314
based narrative that can get my group to team

662
00:30:49.324 --> 00:30:52.155
up together and to attack or defend against an

663
00:30:52.165 --> 00:30:54.694
enemy group, if I can somehow convince that, you

664
00:30:54.704 --> 00:30:56.834
know, there's a dangerous art group right there in

665
00:30:56.844 --> 00:31:00.094
the neighboring um behind the neighboring hill or in

666
00:31:00.104 --> 00:31:02.415
the neighboring nation that is poi poised to attack

667
00:31:02.425 --> 00:31:05.055
us and who wants to exterminate us. I will

668
00:31:05.064 --> 00:31:07.265
easily be able to reap the benefits of like

669
00:31:07.275 --> 00:31:09.704
mobilizing my group against that art group. OK. So

670
00:31:09.714 --> 00:31:12.984
they are mobilizing benefits in, in using instrumentally threads

671
00:31:13.055 --> 00:31:16.699
narratives. And we suspect that at least sometimes that's

672
00:31:16.780 --> 00:31:19.550
how PC theories are being used. Uh People use

673
00:31:19.560 --> 00:31:23.300
them to kind of like recruit um new individuals

674
00:31:23.310 --> 00:31:26.069
to join a political fight, a political cause that

675
00:31:26.079 --> 00:31:29.819
they care about. For instance, you know, anti-establishment uh

676
00:31:29.829 --> 00:31:32.869
uh narrative or, or some kind of like a

677
00:31:32.880 --> 00:31:35.760
more racist narrative against uh you know, the neighboring

678
00:31:35.770 --> 00:31:38.000
tribe that you, that you may be living close

679
00:31:38.010 --> 00:31:40.250
to in like the Middle East or in Africa

680
00:31:40.260 --> 00:31:42.979
or whatnot. And so, and so, yeah, it becomes

681
00:31:42.989 --> 00:31:46.310
possible to, to kind of like instrumentally agitate a

682
00:31:46.319 --> 00:31:49.479
sense of threat in other people to mobilize them

683
00:31:49.489 --> 00:31:51.689
to side with you and to take action with

684
00:31:51.699 --> 00:31:53.839
you on your side and to potentially uh yeah,

685
00:31:53.849 --> 00:31:56.420
organize collectively to, to, to thwart a potential threat.

686
00:31:57.300 --> 00:32:00.959
Um And, and there's at least another potential um

687
00:32:00.969 --> 00:32:03.780
social use that can be made of uh of

688
00:32:03.790 --> 00:32:05.739
this narrative that is also very instrumental that it

689
00:32:05.750 --> 00:32:08.300
is also a little exploitative, which is a signaling,

690
00:32:08.310 --> 00:32:10.560
of course, you know, by, by saying how much

691
00:32:10.569 --> 00:32:13.530
you hate an art group, by saying how much

692
00:32:13.540 --> 00:32:16.699
you think that the art group is uh dangerous,

693
00:32:16.709 --> 00:32:18.839
uh you know, uh conspiring against you with Zara.

694
00:32:19.050 --> 00:32:22.719
Zara, you can express a sense of loyalty to

695
00:32:22.729 --> 00:32:24.150
your own group, you can, you can by, by

696
00:32:24.160 --> 00:32:27.180
see who you hated society, you're potentially sort of

697
00:32:27.189 --> 00:32:30.130
like, you know, distinguishing yourself from a food that

698
00:32:30.140 --> 00:32:32.400
in group and also potentially neutral audiences who don't

699
00:32:32.410 --> 00:32:34.939
care about it and you're singling you, you're signing.

700
00:32:34.949 --> 00:32:37.410
So you're expressing who you're sending with. And that

701
00:32:37.420 --> 00:32:39.760
can be uh that can be useful to um

702
00:32:40.160 --> 00:32:43.189
that can be useful to uh to, to express

703
00:32:43.199 --> 00:32:45.579
to potential allies to potential recruits, you know, that

704
00:32:45.589 --> 00:32:46.910
you're standing with them, that you care about the

705
00:32:46.920 --> 00:32:52.560
interests itself. Um As you mentioned, one important thing

706
00:32:54.030 --> 00:32:57.890
about the mobilization argument, which is that this, there's

707
00:32:57.900 --> 00:33:00.609
kind of a debate in the country, sciences, of

708
00:33:00.619 --> 00:33:04.170
culture and political science and what not about the

709
00:33:04.180 --> 00:33:07.349
extent to which those um threat based narratives really

710
00:33:07.359 --> 00:33:10.109
persuade people or really motivate people to act so

711
00:33:10.119 --> 00:33:13.050
on, on a sort of like optimistic accounts when

712
00:33:13.060 --> 00:33:15.250
you spread, you know, threat based rumor or conspiracy

713
00:33:15.260 --> 00:33:17.479
theory that seems to be believable or, you know,

714
00:33:17.489 --> 00:33:20.380
a cons spiritual narrative or like a sort of

715
00:33:20.390 --> 00:33:23.050
like um ethnic rumor about the fact that, you

716
00:33:23.060 --> 00:33:25.400
know, the neighboring Tutsi are gonna want to massacre

717
00:33:25.410 --> 00:33:29.839
all your uh um Hutu fellows or whatnot under

718
00:33:29.849 --> 00:33:33.180
an optimistic argument, uh an optimistic view of the

719
00:33:33.189 --> 00:33:37.319
argument, those claims will persuade and will really get

720
00:33:37.329 --> 00:33:39.619
people to act because they're afraid of threat, which

721
00:33:39.630 --> 00:33:42.959
we really have motivational impact. I think sometimes it's

722
00:33:42.969 --> 00:33:46.619
the case. Um BUT people like Michael B, Peterson

723
00:33:46.630 --> 00:33:48.829
and maybe also Hugo Mercier have argued and I'm

724
00:33:48.839 --> 00:33:51.300
pretty convinced by that, by that argument that very

725
00:33:51.310 --> 00:33:55.000
often people are not so much convinced, you know,

726
00:33:55.010 --> 00:33:58.030
prompted to act and to take arms against the

727
00:33:58.040 --> 00:34:01.680
threat as they are more using the information as

728
00:34:01.689 --> 00:34:04.260
a way of like signaling that they are willing

729
00:34:04.270 --> 00:34:06.510
to attack, that they're ready to attack already. And

730
00:34:06.520 --> 00:34:09.280
they're using the propaganda, the threat based rumor, the

731
00:34:09.290 --> 00:34:11.600
narrative as a way of like saying, hey, I'm

732
00:34:11.610 --> 00:34:13.840
one of those guys who hate the other group.

733
00:34:13.850 --> 00:34:15.918
I'm willing to take action. And I'm saying that

734
00:34:15.929 --> 00:34:18.399
to you in order to signal that I'm ready

735
00:34:18.409 --> 00:34:21.159
to coordinate with you in a potential collective action

736
00:34:21.360 --> 00:34:23.159
that we might want to take against the other

737
00:34:23.168 --> 00:34:26.188
group. OK. So under a sort of like strong

738
00:34:26.199 --> 00:34:29.739
form of the claim, the negative information really persuades

739
00:34:29.748 --> 00:34:33.259
people to act under, under a broader exception of

740
00:34:33.268 --> 00:34:35.759
the, of the argument or, or the thesis, it

741
00:34:35.768 --> 00:34:39.009
doesn't really prompt people to act, but it helps

742
00:34:39.018 --> 00:34:41.138
people who are already motivated to act for other

743
00:34:41.148 --> 00:34:44.789
reasons, to organize collectivity, to coordinate and to prepare

744
00:34:44.799 --> 00:34:47.299
a collective action that may potentially, you know, target

745
00:34:47.309 --> 00:34:47.589
the other group.

746
00:34:48.418 --> 00:34:53.668
Yeah. Uh And if that second um interpretation of

747
00:34:53.678 --> 00:34:56.079
it is the correct one. Could it, could it

748
00:34:56.089 --> 00:34:59.928
be that in this case uh conspiracy, a political

749
00:34:59.938 --> 00:35:03.059
conspiracy theory or any other sort of threat based

750
00:35:03.069 --> 00:35:08.638
narrative could function more as a sort of justification

751
00:35:08.648 --> 00:35:13.918
to act than really uh as motivating people to

752
00:35:13.938 --> 00:35:15.039
act upon it.

753
00:35:15.469 --> 00:35:21.149
Yeah. Yeah, exactly. Um It could, it could serve,

754
00:35:21.219 --> 00:35:24.919
it could allow individuals to rationalize an already existing

755
00:35:24.929 --> 00:35:28.850
motivation to, you know, organize collective action and to

756
00:35:28.860 --> 00:35:32.260
attack or to defend against the art group. Um

757
00:35:32.270 --> 00:35:34.830
Stereotypes, you know, are not necessarily always super, super

758
00:35:34.840 --> 00:35:37.949
believable. Uh Conspiracy theories are not always super super

759
00:35:37.959 --> 00:35:42.340
believable, but just the fact that they are superficially

760
00:35:42.350 --> 00:35:47.229
plausible or that they um can be at least,

761
00:35:47.239 --> 00:35:50.330
you know, superficially endorsed by your allies may suffice

762
00:35:50.340 --> 00:35:53.679
for them to function as like postdoc rationalizations of

763
00:35:53.689 --> 00:35:56.159
an already existing disposition to hate the art group

764
00:35:56.169 --> 00:35:57.959
and to want to attack against the other group.

765
00:35:58.020 --> 00:36:00.909
Yes. Yes. And, and by, by, by, by, by,

766
00:36:00.919 --> 00:36:03.989
by expressing endorsement of the conspiracy theory or the,

767
00:36:04.000 --> 00:36:05.439
the, the ethnic rumor or whatnot, but just saying

768
00:36:05.449 --> 00:36:07.739
that you believe it or by disseminating it on

769
00:36:07.750 --> 00:36:10.540
yourself, you also signal that you're one of those

770
00:36:10.550 --> 00:36:12.120
individuals who want to act. So there's also a

771
00:36:12.129 --> 00:36:15.679
signaling value, at least it contributes to co-ordination of

772
00:36:15.689 --> 00:36:18.820
collective action. So it signals a membership, it gets

773
00:36:18.830 --> 00:36:21.969
you closer to a coordinate action that is meaningful

774
00:36:21.979 --> 00:36:23.739
in terms of collective action that may be hostile

775
00:36:23.750 --> 00:36:25.590
to the art group. And you're right, it can

776
00:36:25.600 --> 00:36:28.919
also uh rationalize under under the existing motivation to,

777
00:36:28.929 --> 00:36:29.620
to take action.

778
00:36:30.439 --> 00:36:32.810
Uh And uh I mean, just before we move

779
00:36:32.820 --> 00:36:36.080
to, we move to another topic, uh you mentioned

780
00:36:36.090 --> 00:36:39.300
there, the fact that it's still debatable to what

781
00:36:39.310 --> 00:36:45.610
extent conspiracy theories might have actual behavioral effects. I

782
00:36:45.620 --> 00:36:49.959
mean, in terms of mobilizing people to do something

783
00:36:50.120 --> 00:36:52.379
uh that also apply. I mean, this is an

784
00:36:52.389 --> 00:36:56.580
ongoing debate also that applies to things like the

785
00:36:56.590 --> 00:37:02.284
spread of misinformation, political ads during campaigns, online, et

786
00:37:02.294 --> 00:37:04.784
cetera. Right? I mean, we're still not sure to

787
00:37:04.794 --> 00:37:08.875
what extent stuff like that, particularly on social media

788
00:37:08.885 --> 00:37:13.225
and the internet more generally really translating to actual

789
00:37:13.235 --> 00:37:15.925
offline behavior. Right.

790
00:37:16.445 --> 00:37:19.645
Yeah, and, and offline behavior as well. Um I

791
00:37:19.655 --> 00:37:22.030
mean, here I can be, I can be, again,

792
00:37:22.040 --> 00:37:23.419
the echo of, you know, the work of Hugo

793
00:37:23.429 --> 00:37:24.939
me here for a second because I think it's

794
00:37:24.949 --> 00:37:27.139
very uh it's very convincing even if I sometimes

795
00:37:27.149 --> 00:37:29.169
think that Hugo is going a tad too far

796
00:37:29.179 --> 00:37:31.459
and his optimism about human rationality. But I think

797
00:37:31.469 --> 00:37:33.709
the, the general arguments are pretty, pretty strong, pretty

798
00:37:33.719 --> 00:37:37.310
valid. People are not easily gullible, they are not

799
00:37:37.320 --> 00:37:41.699
easily um fooled, they have intuitive capacities to gauge

800
00:37:41.709 --> 00:37:44.530
the coherence and consistency of new information with what

801
00:37:44.540 --> 00:37:46.459
they already believe. And they're pretty good at gauging

802
00:37:46.469 --> 00:37:50.550
whether sources of information and messages have their interests

803
00:37:50.560 --> 00:37:52.350
at heart or are trying to deceive them and

804
00:37:52.360 --> 00:37:54.080
whatnot. So that's what he calls, you know, epistemic

805
00:37:54.090 --> 00:37:56.110
vigilance or in a in a more novel formulation,

806
00:37:56.120 --> 00:37:59.320
open open visions. So I think that's very true.

807
00:37:59.350 --> 00:38:03.989
Of course, um you won't easily um you know,

808
00:38:04.290 --> 00:38:07.629
make Germans become anti Semitic if there's not an

809
00:38:07.639 --> 00:38:12.199
already pretty strong pre-existing disposition for anti-semitism in German

810
00:38:12.209 --> 00:38:16.070
culture that is, you know, plunging in roots um,

811
00:38:16.229 --> 00:38:18.530
in, you know, several centuries of, of, of, uh

812
00:38:18.540 --> 00:38:22.360
Catholic uh and, and partisan history, you won't easily

813
00:38:22.370 --> 00:38:24.530
get people to buy products that they absolutely don't

814
00:38:24.540 --> 00:38:28.959
need. You won't create helo um, new desires. Uh

815
00:38:28.969 --> 00:38:30.899
That's, I think actually, by the way, if you,

816
00:38:30.909 --> 00:38:33.709
if you allow me to, um hit a little

817
00:38:33.719 --> 00:38:35.159
bit on the standard of social scientists who are

818
00:38:35.169 --> 00:38:37.590
not cognitive and who are not evolutionary, I think

819
00:38:37.600 --> 00:38:39.280
that's often like a sort of like a misconception

820
00:38:39.290 --> 00:38:41.020
that a lot of people in traditional sociology or

821
00:38:41.030 --> 00:38:44.729
whatnot may have is that they think that capitalism

822
00:38:44.739 --> 00:38:50.229
advertisement propaganda, um it's have the capacity to generate

823
00:38:50.260 --> 00:38:54.050
non practicing desires. I think that there, that's, that's

824
00:38:54.060 --> 00:38:57.469
generally misconception, human nature is about the same for

825
00:38:57.479 --> 00:39:00.919
everybody and it's pretty deeply structured by a transaction

826
00:39:00.929 --> 00:39:05.469
pressures. You won't easily persuade people to desire commodities

827
00:39:05.479 --> 00:39:08.449
or activities that they are not evolution be predisposed

828
00:39:08.459 --> 00:39:11.820
to like. Right. So in general, what advertisement does

829
00:39:11.870 --> 00:39:13.820
is that it will exploit and obey the existing

830
00:39:13.830 --> 00:39:17.300
predisposition for, I don't know, um junk food, you

831
00:39:17.310 --> 00:39:19.850
know, fat stuff, sugary stuff. It will explode to

832
00:39:19.860 --> 00:39:22.989
pre uh predi predisposition to, you know, want to

833
00:39:23.000 --> 00:39:25.409
have sex and to um want to see, you

834
00:39:25.419 --> 00:39:27.409
know, attractive sexual partners, et Zara and he's going

835
00:39:27.419 --> 00:39:32.379
to present products or services that tap those pre-existing

836
00:39:32.389 --> 00:39:35.719
desires. Uh And, and if it manages to sell

837
00:39:35.729 --> 00:39:39.179
more new services, new goods. It's because it's making

838
00:39:39.350 --> 00:39:42.459
products that type, that tap those already existing desires,

839
00:39:42.469 --> 00:39:44.919
more salient, more visible it for them, but it's

840
00:39:44.929 --> 00:39:48.169
not creating desires from nothing. Ethnic yellow at all,

841
00:39:48.179 --> 00:39:50.969
I think. Mhm. Uh, YEAH. I don't know if

842
00:39:50.979 --> 00:39:52.840
it's, if it's answering your, your question in any

843
00:39:52.850 --> 00:39:53.169
way.

844
00:39:53.899 --> 00:39:55.080
No, no. Maybe, maybe

845
00:39:55.090 --> 00:39:57.439
a little, if you, maybe a little element that

846
00:39:57.449 --> 00:39:58.840
I think I should add about like the, the

847
00:39:58.850 --> 00:40:02.340
the potency of like propaganda and, and, and rumors

848
00:40:02.350 --> 00:40:05.209
and you know, like kind of like discourses that

849
00:40:05.219 --> 00:40:08.500
aim to persuade and to mobilize people. Um An

850
00:40:08.510 --> 00:40:11.429
argument that Hugo would make in this case and

851
00:40:11.439 --> 00:40:16.929
that I'm increasingly convinced by is that um very

852
00:40:16.939 --> 00:40:19.570
often when people say that the endorsement theories or

853
00:40:19.580 --> 00:40:22.250
that they endorse, you know, ideologies like Marxism or

854
00:40:22.260 --> 00:40:25.780
like a, a pretty radical ideology very often what

855
00:40:25.790 --> 00:40:27.989
is likely to be happening in their heads is

856
00:40:28.000 --> 00:40:30.610
that people claim to be endorsing the, the, the

857
00:40:30.620 --> 00:40:33.219
statement of the belief they ascend to it. They

858
00:40:33.229 --> 00:40:35.790
say that they believe it, but in fact, deep

859
00:40:35.800 --> 00:40:38.449
down in their and conscious other systems are, are

860
00:40:38.459 --> 00:40:41.274
like uh not really, you know, like they're not

861
00:40:41.284 --> 00:40:44.314
really taking the information super seriously to the extent

862
00:40:44.324 --> 00:40:49.304
of motivating costly action, costly action, big efforts on

863
00:40:49.314 --> 00:40:51.375
the part of the individual. So very often people

864
00:40:51.385 --> 00:40:55.375
will say that they endorse fashionable beliefs because they

865
00:40:55.385 --> 00:40:57.955
allow them to, you know, seem informed, to seem

866
00:40:57.965 --> 00:40:59.804
like they care about certain values that other people

867
00:40:59.814 --> 00:41:04.679
care about. Um, THEY'RE gonna endorse them to, um,

868
00:41:04.850 --> 00:41:07.850
to seem like, yeah, trustworthy, competent to seem like

869
00:41:07.860 --> 00:41:10.000
they care about politics, etcetera. But in fact, they

870
00:41:10.010 --> 00:41:13.510
very rarely will deviate from their, you know, personal

871
00:41:13.520 --> 00:41:16.070
comfort or like, you know, everyday ways of doing

872
00:41:16.080 --> 00:41:20.070
things and, you know, private little self interest. Um,

873
00:41:20.270 --> 00:41:22.350
ON the basis of those belies very often, the,

874
00:41:22.360 --> 00:41:24.659
the, the epistemic visions mechanisms are, it's not that

875
00:41:24.669 --> 00:41:26.419
they are like turning down the information on the

876
00:41:26.429 --> 00:41:27.989
endorsement of the narrative, but it's more like they

877
00:41:28.000 --> 00:41:30.489
are keeping it in a sort of like isolated

878
00:41:30.500 --> 00:41:33.389
co format which, you know, me be would call

879
00:41:33.399 --> 00:41:37.070
reflective without really trying to connect them with like

880
00:41:37.080 --> 00:41:40.800
costly actions that would potentially entail sacrifices for the

881
00:41:40.810 --> 00:41:43.679
individuals. And uh and also big efforts in terms

882
00:41:43.689 --> 00:41:45.959
of like mobilizing others and really, you know, like

883
00:41:45.969 --> 00:41:48.500
spending money and time on like uh political calls

884
00:41:48.510 --> 00:41:51.120
that people claim to care about. But in fact,

885
00:41:51.129 --> 00:41:54.360
don't really do much to it in general. And,

886
00:41:54.370 --> 00:41:56.800
and by the way, I'm completely, I'm completely guilty

887
00:41:56.810 --> 00:41:59.139
of the same problem myself. OK? Like I'm giving

888
00:41:59.149 --> 00:42:00.879
a bit of money every month to uh to

889
00:42:00.889 --> 00:42:03.070
give well, and I'm trying to do a few,

890
00:42:03.080 --> 00:42:06.639
like not so costly altruistic stuff. Uh But I

891
00:42:06.649 --> 00:42:10.580
really take costly action in favor of more values

892
00:42:10.590 --> 00:42:12.110
that, that, that I claim that I claim to

893
00:42:12.120 --> 00:42:14.330
care about. Uh I claim to care about the

894
00:42:14.340 --> 00:42:16.260
environment, but I'm still playing way too much and

895
00:42:16.270 --> 00:42:18.199
still eating a bit, a bit too much chicken.

896
00:42:18.510 --> 00:42:20.590
And so I'm, I'm completely guilty of the same

897
00:42:20.600 --> 00:42:22.439
mechanism that I'm describing here. Uh

898
00:42:22.449 --> 00:42:25.489
But so I, if that's true, then in this

899
00:42:25.500 --> 00:42:30.729
particular case, uh people manifesting certain specific beliefs would,

900
00:42:30.739 --> 00:42:35.129
would serve mainly a social signaling function, right? In

901
00:42:35.139 --> 00:42:40.409
terms of signaling, their, I guess, group affiliations,

902
00:42:41.219 --> 00:42:45.300
yeah, signaling. Um So I think, you know, what's

903
00:42:45.310 --> 00:42:48.610
happening very often is that we have mechanisms in

904
00:42:48.620 --> 00:42:53.629
the mind that are, that evolve for tax benefits

905
00:42:53.639 --> 00:42:56.370
that were essentially relevant, you know, getting your coalition

906
00:42:56.379 --> 00:42:59.389
to co-operate together to solve a problem or throw

907
00:42:59.399 --> 00:43:02.679
out an enemy to signal your trustworthiness to be

908
00:43:02.689 --> 00:43:04.479
kept and admitted as a group as a good

909
00:43:04.489 --> 00:43:08.290
group member and whatnot and those motivations. So those

910
00:43:08.320 --> 00:43:11.439
many divisions would have expressed themselves with like intensity

911
00:43:11.449 --> 00:43:13.820
in the past when there was like, like a

912
00:43:13.830 --> 00:43:16.080
lot of fitness interdependence. And when there was like

913
00:43:16.090 --> 00:43:17.760
a lot of like external threat on the group

914
00:43:17.770 --> 00:43:19.850
or whatnot, there are people would have, you know,

915
00:43:19.860 --> 00:43:22.639
spent a lot of energy to um get their

916
00:43:22.649 --> 00:43:24.250
group to team up together, or they would have

917
00:43:24.260 --> 00:43:26.750
spent a lot of energy to signal their devotion

918
00:43:26.760 --> 00:43:29.689
to the group by incurring pretty costly sacrifices to

919
00:43:29.699 --> 00:43:31.850
deserve their place in the group at Zara because

920
00:43:31.860 --> 00:43:34.479
back at the time, the environment was very threatening.

921
00:43:34.500 --> 00:43:36.850
But nowadays, typically, most of us would live in

922
00:43:36.860 --> 00:43:39.229
you know, relatively peaceful western societies in which there

923
00:43:39.239 --> 00:43:40.850
is no war, there is no famine, there are

924
00:43:40.860 --> 00:43:43.830
no uh natural catastrophes and whatnot. And so we

925
00:43:43.840 --> 00:43:47.179
are not in dire need to mobilize other people

926
00:43:47.360 --> 00:43:49.409
in collective action or we do, we are not

927
00:43:49.419 --> 00:43:51.409
as much in dire need to signal group memberships

928
00:43:51.790 --> 00:43:54.709
as, as we, as we used to potentially. And

929
00:43:54.719 --> 00:43:57.310
so we're gonna, we're gonna still have those mechanisms

930
00:43:57.320 --> 00:43:59.209
opening in our, in the background of our heads.

931
00:43:59.219 --> 00:44:01.229
We're still gonna be doing a little bit of,

932
00:44:01.239 --> 00:44:03.850
of like political and, and more signaling but not

933
00:44:03.860 --> 00:44:06.610
with the same intensity, not at the price of

934
00:44:06.620 --> 00:44:09.429
like as costly actions as maybe we would have

935
00:44:09.439 --> 00:44:11.830
in intra context in, in context of like group

936
00:44:11.840 --> 00:44:14.989
conflict or in or with as much intensity as

937
00:44:15.000 --> 00:44:17.639
we would in like contemporary context that are prone

938
00:44:17.649 --> 00:44:19.610
to war or where, you know, people are a

939
00:44:19.620 --> 00:44:21.979
bit more in danger. So I would say that

940
00:44:21.989 --> 00:44:25.250
the motivations are there uh But they are only

941
00:44:25.260 --> 00:44:28.129
operating in a sort of like weak diminished form

942
00:44:28.139 --> 00:44:30.340
in people's heads. But still, it's enough that they're

943
00:44:30.350 --> 00:44:32.479
operating in a sort of like, you know, uh

944
00:44:32.489 --> 00:44:37.060
minimal mode for the, for the belief that people,

945
00:44:37.070 --> 00:44:39.889
for people to endorse beliefs and to disseminate claims

946
00:44:39.899 --> 00:44:43.500
in ways that, you know, seem to, yeah, that,

947
00:44:43.510 --> 00:44:46.610
that seem that still seem to fulfill signaling functions

948
00:44:46.620 --> 00:44:49.459
and, and potentially uh mobilization functions.

949
00:44:51.000 --> 00:44:55.379
So changing topics, you have a paper where you

950
00:44:55.389 --> 00:45:01.229
explore the ways by which strongly motivate people that

951
00:45:01.239 --> 00:45:06.760
are strongly morally motivated or committed to gender equality

952
00:45:06.770 --> 00:45:11.949
in this specific case process and understand evidence regarding

953
00:45:12.080 --> 00:45:17.310
gender bias hiring processes. So, uh and the and

954
00:45:17.320 --> 00:45:21.459
of course, there are benefits to being committed to

955
00:45:21.469 --> 00:45:24.709
gender equality in this specific case. But in the

956
00:45:24.719 --> 00:45:28.510
paper, you also talk about potential costs. Could you

957
00:45:28.520 --> 00:45:30.429
explain what's going on here?

958
00:45:31.060 --> 00:45:33.780
Yeah, it's all very simple. So we have that

959
00:45:33.790 --> 00:45:35.820
uh we have that pre print paper that uh

960
00:45:35.830 --> 00:45:37.889
got rejected from many places, but I hope it

961
00:45:37.919 --> 00:45:39.830
will one day be accepted by a decent social

962
00:45:39.840 --> 00:45:45.050
psychology journal uh with um co authors um uh

963
00:45:45.060 --> 00:45:47.659
Hu Ling Xiao, who's a, who's a Chinese colleague

964
00:45:47.669 --> 00:45:50.840
and Andre Mle, my former PH advisor in which

965
00:45:50.850 --> 00:45:55.780
we look at like how people consume um simplified

966
00:45:55.790 --> 00:45:59.860
scientific accounts, uh simplified, you know, research summaries about

967
00:45:59.870 --> 00:46:02.370
the topic of gender bias in hiring in organizations

968
00:46:02.379 --> 00:46:05.830
and doctor in stem. And we look at how

969
00:46:05.840 --> 00:46:08.010
the degree of like feminism, if you will the

970
00:46:08.020 --> 00:46:10.610
de the degree to which they moralize gender equality

971
00:46:10.649 --> 00:46:14.590
influences those evaluations, the degree to which they think

972
00:46:14.600 --> 00:46:18.179
those scientific uh reports are credible, the methods are

973
00:46:18.189 --> 00:46:20.810
rigorous, you know, the the findings are convincing sr

974
00:46:20.870 --> 00:46:23.260
itself. And so there's, there's a, there's a, there's

975
00:46:23.270 --> 00:46:25.679
a PNS paper that came out in 2015, I

976
00:46:25.689 --> 00:46:28.169
think by Henry at a that showed that essentially

977
00:46:28.560 --> 00:46:32.610
when people were presented with uh scientific demonstrations using,

978
00:46:32.620 --> 00:46:35.310
you know, randomized control trials or like rigorous experimental

979
00:46:35.320 --> 00:46:38.689
methods. When people were exposed to those uh proofs

980
00:46:39.669 --> 00:46:45.010
that um having processes in academia favor uh men

981
00:46:45.020 --> 00:46:48.889
over women with equal credentials, they found that men

982
00:46:48.899 --> 00:46:51.770
were less receptive to that uh type of scientific

983
00:46:51.780 --> 00:46:54.780
result that women were more likely to believe them

984
00:46:55.070 --> 00:46:57.860
and to want to take action against, against those

985
00:46:57.870 --> 00:47:00.010
uh against those findings. OK. So they found that

986
00:47:00.020 --> 00:47:01.639
there was some kind of like a sex difference

987
00:47:01.649 --> 00:47:05.810
in how receptive or how um how truthful people

988
00:47:05.820 --> 00:47:08.969
were at that demonstration. And so here, what we

989
00:47:08.979 --> 00:47:11.979
tried to do was to was to see first

990
00:47:12.719 --> 00:47:14.949
is that sex effect, video, sex effect or is

991
00:47:14.959 --> 00:47:18.330
it more like an moral ideology effect that is

992
00:47:18.399 --> 00:47:21.290
associated or co varying with the sex difference? Is

993
00:47:21.300 --> 00:47:23.429
it possible that the sex difference is in fact

994
00:47:23.439 --> 00:47:27.129
reducible to differences in moral commitments in the degree

995
00:47:27.139 --> 00:47:30.540
to which people care about sex equality um in

996
00:47:30.550 --> 00:47:33.469
our, in our, you know, uh population of participants.

997
00:47:33.780 --> 00:47:35.300
And yes, it seems to be the, it seems

998
00:47:35.310 --> 00:47:37.000
to be the case when we control for more

999
00:47:37.010 --> 00:47:40.989
commitment very often the sex effect disappears. And when

1000
00:47:41.000 --> 00:47:43.510
we add more commitment as a covariance of people,

1001
00:47:43.520 --> 00:47:45.280
the variations of the degree to which they trust

1002
00:47:45.290 --> 00:47:48.459
the science they think is believable. Um We find

1003
00:47:48.469 --> 00:47:50.459
that the more people care about more equality, the

1004
00:47:50.469 --> 00:47:52.699
more they will trust, the more they will deem

1005
00:47:52.709 --> 00:47:58.129
plausible uh convincing those scientific reports that demonstrate experimentally

1006
00:47:58.489 --> 00:48:00.739
that uh women are being discriminated against in having

1007
00:48:00.750 --> 00:48:03.449
processes in academia. So that's, that's a bright side.

1008
00:48:03.459 --> 00:48:06.600
Ok. So we, we, we, we expand already existing

1009
00:48:06.610 --> 00:48:08.280
study. We showed that what seemed to be a

1010
00:48:08.290 --> 00:48:10.800
sex effect is actually maybe more like a moral

1011
00:48:10.810 --> 00:48:13.879
ideology effect. And we find that good news, people

1012
00:48:13.889 --> 00:48:17.419
who care more about equality will uh welcome more

1013
00:48:17.620 --> 00:48:21.080
with more receptivity, those rigorous demonstrations that we may

1014
00:48:21.090 --> 00:48:23.500
not be discriminated against. However, on the flip side,

1015
00:48:24.620 --> 00:48:27.070
we also find that people who are more uh

1016
00:48:27.080 --> 00:48:30.939
more committed to equality, uh see as more persuading,

1017
00:48:30.949 --> 00:48:35.350
as more persuasive, sorry. Um MORE bogus demonstrations or

1018
00:48:35.360 --> 00:48:37.719
conclusions that there is gender bias in having taking

1019
00:48:37.729 --> 00:48:39.280
place. So here, what we did is that we

1020
00:48:39.290 --> 00:48:43.770
substituted the rigorous uh experimental demonstration of gender bias

1021
00:48:43.780 --> 00:48:47.260
in hiring with a more observational and fishy study

1022
00:48:47.870 --> 00:48:52.080
that essentially looks at the pre hiring um proportion

1023
00:48:52.090 --> 00:48:54.060
of men and women in stem and the post

1024
00:48:54.070 --> 00:48:57.479
hiring proportions. And that finds that in fact, the

1025
00:48:57.489 --> 00:49:00.500
post hiring proportions of women is greater than the

1026
00:49:00.510 --> 00:49:02.810
pre hiring proportions. So clearly, there's no gender bias

1027
00:49:02.820 --> 00:49:05.429
in having against women. And and, and we, and

1028
00:49:05.439 --> 00:49:07.750
we visualize that graphically with like pretty clear about

1029
00:49:07.800 --> 00:49:09.850
shots and like there's no ambiguity about the fact

1030
00:49:09.860 --> 00:49:12.360
that women are really favored in having process. And

1031
00:49:12.370 --> 00:49:15.840
yet the research Army concludes that the fact that

1032
00:49:15.899 --> 00:49:19.350
women on the whole, even after hiring are still

1033
00:49:19.360 --> 00:49:21.409
a minority because they, they are a minority before

1034
00:49:21.419 --> 00:49:23.709
and after hiring, the fact that women are a

1035
00:49:23.719 --> 00:49:26.639
minority in science in the scientific labs is a

1036
00:49:26.649 --> 00:49:29.040
rigorous demonstration of the fact that, you know, there's

1037
00:49:29.050 --> 00:49:32.389
gender bias in hiring. So that's a fac conclusion

1038
00:49:32.520 --> 00:49:34.520
because people infer from the fact that women are

1039
00:49:34.530 --> 00:49:38.580
less generous, the notion that there should be inevitably

1040
00:49:38.590 --> 00:49:40.810
uh you know, gender-based in having against them, even

1041
00:49:40.820 --> 00:49:42.909
when the presented data is clearly showing that women

1042
00:49:42.919 --> 00:49:45.520
all things equal, actually favored in the recruiting recruitment

1043
00:49:45.530 --> 00:49:49.189
process. OK. And so here we find that people

1044
00:49:49.290 --> 00:49:54.090
um generally tend to do not process the contribution

1045
00:49:54.100 --> 00:49:56.669
between the result and the conclusion and the higher

1046
00:49:56.679 --> 00:49:59.669
and more commitment to the equality, the more people

1047
00:49:59.679 --> 00:50:02.330
are likely to rate that bogus scientific conclusion as

1048
00:50:02.340 --> 00:50:06.780
being, you know, convincing. OK. So overall, in summary,

1049
00:50:07.050 --> 00:50:08.879
if you care, it seems that if you care

1050
00:50:08.889 --> 00:50:10.729
more about gender equality, if you're more of a

1051
00:50:10.739 --> 00:50:13.239
feminist, you will. And that's a and that's a

1052
00:50:13.250 --> 00:50:18.229
good thing rate more positively believe more rigorous demonstration

1053
00:50:18.239 --> 00:50:20.219
of their being uh gender bias against women. But

1054
00:50:20.229 --> 00:50:22.820
you will also be more receptive and more believing

1055
00:50:22.889 --> 00:50:26.679
of bogus demonstrations that only rely on observational evidence

1056
00:50:27.040 --> 00:50:29.070
and that are even, you know, showing results that

1057
00:50:29.080 --> 00:50:32.760
are contradictory with the conclusion. So this of course,

1058
00:50:32.770 --> 00:50:35.399
does not, you know, amount to uh saying that

1059
00:50:35.409 --> 00:50:37.820
we should stop being uh caring about gender equality.

1060
00:50:37.830 --> 00:50:41.469
We should, of course uh uh keep caring about

1061
00:50:41.479 --> 00:50:43.419
that sort of like moral and political targets, a

1062
00:50:43.429 --> 00:50:45.770
very important one. But the lesson that we are

1063
00:50:45.780 --> 00:50:48.300
trying to teach here and we're not the first

1064
00:50:48.310 --> 00:50:51.290
ones to be making that argument is that um

1065
00:50:51.510 --> 00:50:54.870
when you are very committed to a cause, it

1066
00:50:54.879 --> 00:50:57.489
can potentially blend you a little bit, you know,

1067
00:50:57.500 --> 00:50:59.850
to the details of a reasoning, it can make

1068
00:50:59.860 --> 00:51:02.570
you a bit more susceptible to confirmation bias or

1069
00:51:02.580 --> 00:51:04.850
um or belief bias in the sense that if

1070
00:51:04.860 --> 00:51:06.590
you really endorse a conclusion, if you think that

1071
00:51:06.600 --> 00:51:07.899
the conclusion is true, if you think that the

1072
00:51:07.909 --> 00:51:10.610
conclusion is more desirable, it makes you a bit

1073
00:51:10.620 --> 00:51:13.070
more likely to neglect the inferential steps or the

1074
00:51:13.080 --> 00:51:15.409
reasoning that back it up and that lead to

1075
00:51:15.419 --> 00:51:17.780
it. And that's what we're trying to show essentially,

1076
00:51:17.810 --> 00:51:20.189
you know, morality binds and blinds. As Jonathan, he

1077
00:51:20.320 --> 00:51:22.580
would say it, it can make people a bit

1078
00:51:22.590 --> 00:51:25.510
more irrational uh at the margins, even even if

1079
00:51:25.520 --> 00:51:27.719
you know, the aggregate social effects are on average

1080
00:51:27.729 --> 00:51:31.290
positive, at least sometimes it's gonna bias people's reasoning

1081
00:51:31.300 --> 00:51:32.300
and we want to show that.

1082
00:51:33.040 --> 00:51:37.840
Mhm. So when it comes to these ideological commitments

1083
00:51:37.850 --> 00:51:41.540
that some people have, uh you've also been working

1084
00:51:41.550 --> 00:51:47.479
on ideological orthodoxy and specifically on the repression of

1085
00:51:47.489 --> 00:51:51.459
free speech. So what do we know about what

1086
00:51:51.469 --> 00:51:56.129
might motivate and what might be the goals that

1087
00:51:56.379 --> 00:52:01.500
a strong political activists try to pursue when repressing

1088
00:52:01.510 --> 00:52:02.520
free speech?

1089
00:52:02.860 --> 00:52:06.310
OK. So here you're referring to my work with

1090
00:52:06.320 --> 00:52:09.159
uh Mika Van Heeren um recent work, you know,

1091
00:52:09.169 --> 00:52:11.310
just published or still in the pipes at Sara.

1092
00:52:11.320 --> 00:52:14.639
So not much is is out yet. We have

1093
00:52:14.649 --> 00:52:17.090
um we have a we have a commentary article

1094
00:52:17.100 --> 00:52:20.760
on, on David Pino and our uh the alliance

1095
00:52:20.770 --> 00:52:23.459
theory of ideology that, that our commentary is called

1096
00:52:23.500 --> 00:52:26.820
speech prepress and outrage from orthodox activists as attempts

1097
00:52:26.830 --> 00:52:29.879
at fascinating mobilizations and getting service among allies. I'm

1098
00:52:29.889 --> 00:52:31.739
sorry, the title is a little long but you

1099
00:52:31.750 --> 00:52:33.629
know, your, your viewers are welcome to check it

1100
00:52:33.639 --> 00:52:36.000
out. And we have also like a longer piece

1101
00:52:36.010 --> 00:52:38.489
in preparation that is currently called the Ky Foundations

1102
00:52:38.500 --> 00:52:40.760
of Orthodoxy in which we try to expand a

1103
00:52:40.770 --> 00:52:42.850
bit those arguments in a somewhat more detailed form.

1104
00:52:43.709 --> 00:52:46.800
So, yeah, Michael and I have taken the phenomenon

1105
00:52:46.810 --> 00:52:50.199
of like political dogmatism and speech repression as our

1106
00:52:50.209 --> 00:52:52.919
sort of like object of inquiry. Recently, we've become

1107
00:52:52.929 --> 00:52:57.469
interested in that um for various reasons. Uh One

1108
00:52:57.479 --> 00:52:59.229
because we think that, you know, there's a bit

1109
00:52:59.239 --> 00:53:01.260
too much repression of speech in the world uh

1110
00:53:01.270 --> 00:53:05.350
nowadays still obviously also because um we think that

1111
00:53:05.360 --> 00:53:07.510
there's a bit too much of it in uh

1112
00:53:07.520 --> 00:53:08.979
in, in the west, in, in the US, in

1113
00:53:08.989 --> 00:53:11.800
particular, there's a somewhat, you know, concerning phenomenon of

1114
00:53:11.810 --> 00:53:14.790
cancel culture that is threatening uh universities and intellectual

1115
00:53:14.800 --> 00:53:17.550
professions that we think is pretty problematic. And of

1116
00:53:17.560 --> 00:53:19.780
course, because, you know, speech is still massively repressed

1117
00:53:19.790 --> 00:53:22.919
in authoritarian countries uh the world over and we,

1118
00:53:22.929 --> 00:53:25.110
and we told, we told ourselves that, well, it

1119
00:53:25.120 --> 00:53:28.780
seems like there's not really a, a mature cognitive

1120
00:53:28.790 --> 00:53:31.520
and evolutionary theory of like, why people are orthodox

1121
00:53:31.530 --> 00:53:34.709
or we press free speech in ideological movements, you

1122
00:53:34.719 --> 00:53:37.659
know, be they moral movements, religious movements, political movements

1123
00:53:37.669 --> 00:53:40.320
and whatnot. And so we, we told ourselves, well,

1124
00:53:40.330 --> 00:53:42.729
let's try to develop a theory of like why

1125
00:53:42.739 --> 00:53:44.479
there is, you know, the anti-communist witch hunt in

1126
00:53:44.489 --> 00:53:46.760
the fifties. Why is there, uh you know, the

1127
00:53:46.770 --> 00:53:49.800
repression and the surveillance of political dissent in China

1128
00:53:49.810 --> 00:53:52.360
right now? Why did the Catholic church, you know,

1129
00:53:52.370 --> 00:53:54.649
persecute the heretics in the middle ages and why

1130
00:53:54.659 --> 00:53:56.040
is there a cancel culture in the US right

1131
00:53:56.050 --> 00:53:57.399
now? Both on the left and on the right.

1132
00:53:58.060 --> 00:54:00.629
So that's a bit explanatory target if I, if

1133
00:54:00.639 --> 00:54:03.139
I may say and so what we say is

1134
00:54:03.149 --> 00:54:05.699
essentially pretty similar to, you know, what we've been

1135
00:54:05.709 --> 00:54:08.540
saying about the conspiracy theories and what, you know,

1136
00:54:08.550 --> 00:54:11.000
people like cosine and tube and, and, and other

1137
00:54:11.010 --> 00:54:13.489
authors have been saying about, you know, um more

1138
00:54:13.500 --> 00:54:16.479
beliefs and political beliefs, essentially, we are proposing that

1139
00:54:16.489 --> 00:54:20.320
people re speech for two main reasons. Uh Mobilization

1140
00:54:20.330 --> 00:54:23.510
goals and signaling goals. So the mobilization goal would

1141
00:54:23.520 --> 00:54:26.459
be fulfilled in the following way. If you care

1142
00:54:26.469 --> 00:54:29.000
a lot about the issue, uh you will want

1143
00:54:29.010 --> 00:54:32.050
to, you know, uh rally your life and to

1144
00:54:32.060 --> 00:54:34.689
motivate your life to, you know, create a coalition,

1145
00:54:34.699 --> 00:54:36.929
create, create a sort of like group uh behind

1146
00:54:36.939 --> 00:54:39.729
yourself with yourself to tackle the threat, right? You

1147
00:54:39.739 --> 00:54:42.169
might want to vanquish uh racism in the US.

1148
00:54:42.179 --> 00:54:44.659
For instance, you might want to, um, you know,

1149
00:54:44.669 --> 00:54:47.600
um thwart the threat of communist infiltration in the

1150
00:54:47.610 --> 00:54:49.570
US in the 19 fifties if you're like a,

1151
00:54:49.580 --> 00:54:51.810
like a ma artist or what not. And so

1152
00:54:51.820 --> 00:54:55.120
there's a sort of like intrinsic desire or need

1153
00:54:55.270 --> 00:54:57.860
to organize collective action if you want to advance

1154
00:54:57.870 --> 00:54:59.219
a cult or if you want to defeat an

1155
00:54:59.229 --> 00:55:02.360
enemy, be it also like an intellectual enemy, it

1156
00:55:02.370 --> 00:55:03.659
doesn't have to be a human out group. It

1157
00:55:03.669 --> 00:55:05.429
can be like dangerous ideas that you think are

1158
00:55:05.459 --> 00:55:08.310
exerting, you know, a corruptive influence on your society

1159
00:55:08.320 --> 00:55:11.830
or whatnot. And so in those conditions, that goal

1160
00:55:11.840 --> 00:55:15.419
of mobilization becomes to try to influence people's beliefs

1161
00:55:15.429 --> 00:55:18.770
so that they believe that uh the mobilization is,

1162
00:55:18.780 --> 00:55:21.570
is justified, is necessary. And typically, one way of

1163
00:55:21.580 --> 00:55:23.439
doing that is to tell people that there's a

1164
00:55:23.449 --> 00:55:24.979
threat because if there's a threat, you know, it's

1165
00:55:24.989 --> 00:55:27.750
gonna activate our evolved sense of like threat, avoidance.

1166
00:55:27.800 --> 00:55:30.669
It's very powerful from a motivational perspective. And so

1167
00:55:30.679 --> 00:55:34.270
often you can motivate people to gang together behind

1168
00:55:34.280 --> 00:55:37.149
you to organize for your side if you can

1169
00:55:37.159 --> 00:55:39.739
convince them that they are facing a threat. And

1170
00:55:39.750 --> 00:55:41.659
in that context, what we're saying is that people

1171
00:55:41.709 --> 00:55:45.260
activists, political leaders, ideological entrepreneurs, et cetera, et cetera,

1172
00:55:45.800 --> 00:55:48.800
they will try to, they will tend to try

1173
00:55:48.810 --> 00:55:51.209
to regulate free speech and to maybe repress free

1174
00:55:51.219 --> 00:55:53.810
speech when they think that the speech in question

1175
00:55:53.820 --> 00:55:56.419
is potentially demobilizing when, when they think that potentially

1176
00:55:56.429 --> 00:55:59.979
the information is contrarian or is kind of like

1177
00:55:59.989 --> 00:56:03.100
calling into question the legitimacy of the fight or

1178
00:56:03.110 --> 00:56:05.260
it contributes to nuance or procession of the threat

1179
00:56:05.520 --> 00:56:08.379
or contributes to uh say that the threat is

1180
00:56:08.389 --> 00:56:09.949
not that bad after all or what not, you

1181
00:56:09.959 --> 00:56:12.270
know what I mean? So like uh if I

1182
00:56:12.280 --> 00:56:14.919
take the example of, you know, the the canceling

1183
00:56:14.929 --> 00:56:19.510
of controversial speakers on us campuses uh recently by

1184
00:56:19.520 --> 00:56:21.750
the left, but also by the right, if you're

1185
00:56:21.760 --> 00:56:24.889
a staunch anti racist, you may want to deplatform

1186
00:56:24.899 --> 00:56:29.010
or to cancel the invitation or to stop um

1187
00:56:29.020 --> 00:56:31.560
the, the, the talk from taking place of a

1188
00:56:31.729 --> 00:56:34.570
biologist who's doing research on, let's say the link

1189
00:56:34.580 --> 00:56:38.639
between genetics and, and, and personality or someone who's

1190
00:56:38.649 --> 00:56:43.459
investigating potential, you know, psychological differences or IQ differences

1191
00:56:43.469 --> 00:56:47.530
between populations and whatnot because rightfully or wrongfully, I

1192
00:56:47.540 --> 00:56:49.760
don't know, it's, it's an open question. You might

1193
00:56:49.770 --> 00:56:52.709
think that those pieces of information in, if it's

1194
00:56:52.790 --> 00:56:56.379
sif research, those pieces of information may contribute to

1195
00:56:56.389 --> 00:57:00.919
normalize, to justify uh racism, not necessarily directly but

1196
00:57:00.929 --> 00:57:03.570
even indirectly as you know, making like uh maybe

1197
00:57:03.580 --> 00:57:05.750
a biological conception of human nature a bit more

1198
00:57:05.760 --> 00:57:08.530
popular or a bit more plausible and whatnot. So

1199
00:57:08.540 --> 00:57:10.639
you might, you might be motivated to be press

1200
00:57:10.649 --> 00:57:13.169
speech that you think is contrarian or susceptible to

1201
00:57:13.179 --> 00:57:17.090
undermine uh the third need for the necessity of

1202
00:57:17.100 --> 00:57:20.239
the mobilization against racism. And in particular, you will

1203
00:57:20.250 --> 00:57:22.870
try to repress speech that you think is undermining

1204
00:57:22.879 --> 00:57:25.889
people's perception of threat, right? For instance, you know,

1205
00:57:26.179 --> 00:57:29.409
research also that may downplay the notion that there

1206
00:57:29.419 --> 00:57:32.129
is a link between, you know, um the race

1207
00:57:32.139 --> 00:57:34.610
of the defendant and police violence, for instance, will

1208
00:57:34.620 --> 00:57:38.149
typically be seen as being contrarian as being dangerous

1209
00:57:38.159 --> 00:57:41.379
by anti racist activists because they fear that its

1210
00:57:41.389 --> 00:57:45.340
dissemination in society may influence people's prior beliefs towards

1211
00:57:45.350 --> 00:57:47.689
them. Believing that the, the the the problem of

1212
00:57:47.699 --> 00:57:49.520
racism is not that bad or that, you know,

1213
00:57:49.530 --> 00:57:51.860
it's it's exaggerated or whatnot and that may have

1214
00:57:51.870 --> 00:57:54.489
demobilizing effects and the same like on the right

1215
00:57:54.500 --> 00:57:58.314
people may want to repress free speech of um

1216
00:57:58.324 --> 00:58:00.155
the free expression of, for instance, you know, um

1217
00:58:00.165 --> 00:58:03.175
critical race theory or, you know, uh the notion

1218
00:58:03.185 --> 00:58:05.084
that, you know, gender roles are at least partly

1219
00:58:05.094 --> 00:58:09.314
socially constructed because they feel like dissemination of those

1220
00:58:09.524 --> 00:58:14.560
contracting messages may kind of like diminish or undermine

1221
00:58:14.570 --> 00:58:17.610
the the enthusiasm that people have for traditional Christian

1222
00:58:17.620 --> 00:58:19.139
values, et cetera. And they see that as a

1223
00:58:19.149 --> 00:58:22.270
threat. Ok. So one core function we think of

1224
00:58:22.280 --> 00:58:24.909
speech repression and speech regulation is you try to

1225
00:58:24.919 --> 00:58:29.889
control people's beliefs by intervening on information flows so

1226
00:58:29.899 --> 00:58:33.070
that people stay motivated for causes that you think

1227
00:58:33.080 --> 00:58:35.780
will bring about shared benefits in particular against, you

1228
00:58:35.790 --> 00:58:39.899
know, social threats. And of course, once there are

1229
00:58:39.909 --> 00:58:43.100
those efforts to mobilize allies for causes and against

1230
00:58:43.110 --> 00:58:46.429
enemies that creates individual incentives for people to signal

1231
00:58:46.439 --> 00:58:48.699
devotion to certain causes to those causes and to

1232
00:58:48.709 --> 00:58:52.709
the group. So once people care vastly about anti-racism,

1233
00:58:52.719 --> 00:58:55.040
once people have a moral panic about communist influence

1234
00:58:55.050 --> 00:58:57.209
in the US, in the, in the fifties, once

1235
00:58:57.219 --> 00:58:59.979
conservative America is going through a more panic about

1236
00:59:00.040 --> 00:59:03.199
critical race theory being taught at university or at

1237
00:59:03.209 --> 00:59:06.760
high school that creates individual opportunities for repetition and

1238
00:59:06.770 --> 00:59:11.100
enhancement. Uh It creates incentives for individuals to seem

1239
00:59:11.110 --> 00:59:14.919
like they're gonna be um very tough with uh

1240
00:59:14.929 --> 00:59:17.439
you know, uh control on ideas with like ideas

1241
00:59:17.449 --> 00:59:19.459
that they are in group, dislike as a way

1242
00:59:19.469 --> 00:59:22.590
of showing their degree of commitment for those ideas.

1243
00:59:22.600 --> 00:59:25.139
Um SORRY for the group. Uh AND, and, and

1244
00:59:25.149 --> 00:59:26.540
for the causes that they are trying to advance,

1245
00:59:26.830 --> 00:59:29.360
like, if I'm, let's say a po position in

1246
00:59:29.370 --> 00:59:32.360
19 fifties like Maar and there it is and

1247
00:59:32.370 --> 00:59:34.979
is a sort of like more panic against uh

1248
00:59:34.989 --> 00:59:37.590
about the influence of communism in American culture and

1249
00:59:37.600 --> 00:59:41.189
institutions. Well, I may see it as an opportunity

1250
00:59:41.199 --> 00:59:45.060
to try to erect, to emerge myself as a

1251
00:59:45.070 --> 00:59:48.129
tough politician, as a reliable politician who will defend,

1252
00:59:48.139 --> 00:59:51.629
you know, the traditional values of corporate liberal America

1253
00:59:51.639 --> 00:59:54.310
against the threat of communism. And so he quits

1254
00:59:54.959 --> 00:59:58.979
um by repressing speech um of people who are

1255
00:59:58.989 --> 01:00:03.060
suspected of Soviet sympathies. Someone like Mac mccalley can

1256
01:00:03.070 --> 01:00:06.250
emerge as someone who's committed to protect regional America

1257
01:00:06.260 --> 01:00:09.169
against the threat. And so, yeah, you, you, you

1258
01:00:09.179 --> 01:00:11.399
know, you appear tough against the contractions, you appear

1259
01:00:11.409 --> 01:00:13.500
tough against the enemies against people who disagree with

1260
01:00:13.510 --> 01:00:16.020
you. And that can potentially allow you to score

1261
01:00:16.030 --> 01:00:19.590
reputational points. Ok. So, uh,

1262
01:00:20.429 --> 01:00:22.969
uh, yeah, II, I have one more question or

1263
01:00:22.979 --> 01:00:26.530
topic to explore about political activists but, uh, still

1264
01:00:26.540 --> 01:00:29.739
on the topic of repressing free speech. I mean,

1265
01:00:29.750 --> 01:00:32.530
I've asked you a similar kind of question when

1266
01:00:32.540 --> 01:00:37.919
earlier we talked about uh spreading uh fake news

1267
01:00:37.929 --> 01:00:42.989
and all of that and uh how basically uh

1268
01:00:43.169 --> 01:00:46.959
people care more about or a apparently care more

1269
01:00:46.969 --> 01:00:52.219
about the values and symbols driving political decisions than

1270
01:00:52.590 --> 01:00:57.120
uh the concrete effects they have on society. Uh

1271
01:00:57.129 --> 01:00:59.830
I mean, in this particular case, are you also

1272
01:00:59.840 --> 01:01:04.030
interested in trying to understand perhaps if there would

1273
01:01:04.040 --> 01:01:08.429
be any ways we could try to curb the

1274
01:01:08.439 --> 01:01:11.570
repression of free speech or is that not something

1275
01:01:11.580 --> 01:01:14.260
that you are really studying?

1276
01:01:15.219 --> 01:01:17.479
Uh I mean, I'm obviously interested in that question

1277
01:01:17.489 --> 01:01:19.639
because, you know, the, the, the very fact that,

1278
01:01:19.649 --> 01:01:21.610
that I think that there's a need for a

1279
01:01:22.040 --> 01:01:25.409
g theory of free speech. Um Is it still

1280
01:01:25.419 --> 01:01:27.060
a reflection of the fact that I think that,

1281
01:01:27.070 --> 01:01:28.780
you know, the repression of free speech is at

1282
01:01:28.790 --> 01:01:31.830
least sometimes bad even if sometimes it's also desirable.

1283
01:01:32.479 --> 01:01:34.399
Um I think it's a good thing that for

1284
01:01:34.419 --> 01:01:36.429
instance, in Europe, we're trying to, you know, fight

1285
01:01:36.439 --> 01:01:39.500
uh neo Nazi propaganda by making it illegal to

1286
01:01:39.510 --> 01:01:41.719
have a swat seekers and uh and to, and

1287
01:01:41.729 --> 01:01:43.790
to sing a neo Nazi chants in the streets.

1288
01:01:44.409 --> 01:01:46.969
Um So there are cases where, I mean, I'm

1289
01:01:46.979 --> 01:01:48.790
not, I'm not, I'm not a free speech. Absolutely

1290
01:01:48.830 --> 01:01:52.500
alone. Even if I'm very pro free speech, what

1291
01:01:52.510 --> 01:01:55.889
can we do to reinforce or to strengthen free

1292
01:01:55.899 --> 01:02:00.159
speech? Um I don't know, Ricardo, I don't really

1293
01:02:00.169 --> 01:02:02.280
know. I haven't really studied that, uh that question

1294
01:02:02.290 --> 01:02:09.300
yet. Um I guess, I guess, I guess decreasing

1295
01:02:09.310 --> 01:02:11.739
the level of like polarization of society and trying

1296
01:02:11.750 --> 01:02:14.959
to make our national societies a little less conflictual

1297
01:02:14.969 --> 01:02:18.520
will help because um as we argue in our

1298
01:02:18.530 --> 01:02:23.080
work with Michael, um it seems pretty clear that

1299
01:02:23.090 --> 01:02:26.479
motivations, we press free speech co vary and are

1300
01:02:26.489 --> 01:02:30.340
sort of like upregulated, increased by perceptions of society,

1301
01:02:30.350 --> 01:02:33.540
be conflict ridden, you know. Mhm. When you think

1302
01:02:33.550 --> 01:02:35.159
that there's a dangerous out group that is threatening

1303
01:02:35.169 --> 01:02:37.250
your in group, which is like a, like a,

1304
01:02:37.260 --> 01:02:40.580
like a by definition, your perception of conflict, um

1305
01:02:40.590 --> 01:02:44.300
you will feel an increased need to um motivate

1306
01:02:44.330 --> 01:02:46.379
my group to form like a cohesive coalition or

1307
01:02:46.389 --> 01:02:49.139
to, you know, rally new life to, to, to

1308
01:02:49.149 --> 01:02:52.629
fight that threat. And um and it's to that

1309
01:02:52.639 --> 01:02:54.500
extent that people is to the extent that people

1310
01:02:54.510 --> 01:02:57.219
feel that they have benefits in mobilizing, that they

1311
01:02:57.229 --> 01:02:59.639
will feel that they also have benefits in repressing

1312
01:02:59.649 --> 01:03:02.560
speech that they think could be demobilizing, right? So

1313
01:03:02.870 --> 01:03:06.689
the more society is harmonious uh and the more,

1314
01:03:06.699 --> 01:03:09.360
you know, the different subgroups that compose society can

1315
01:03:09.370 --> 01:03:12.250
live their lives fairly freely. And the more uh

1316
01:03:12.280 --> 01:03:14.360
the, the less inequality maybe there is and the

1317
01:03:14.370 --> 01:03:18.459
less effective polarization there is, the less it should

1318
01:03:18.469 --> 01:03:22.159
be a problem. Um The, the, the less you

1319
01:03:22.169 --> 01:03:25.659
should have motivations from activists on, on either side

1320
01:03:25.669 --> 01:03:27.080
to want to be press and to police the

1321
01:03:27.090 --> 01:03:31.850
speech of others because yeah, again, like um motivation

1322
01:03:31.860 --> 01:03:35.959
to repress speech are impossible to dissociate from threat,

1323
01:03:35.969 --> 01:03:39.600
perceptions and motivations to, to motivate your own group

1324
01:03:39.610 --> 01:03:41.639
to do stuff in the world and often what

1325
01:03:41.649 --> 01:03:46.100
motivates. Um Yeah, that, that, that mobilization effort is

1326
01:03:46.110 --> 01:03:48.320
perception of threat. So I would say that, yeah,

1327
01:03:48.330 --> 01:03:50.010
you have to make societies a bit more peaceful

1328
01:03:50.020 --> 01:03:53.379
and a little less polarized as for more. So

1329
01:03:53.389 --> 01:03:55.100
that's of course, very difficult, right? Uh It's a

1330
01:03:55.110 --> 01:03:58.310
sort of like, you know, um basic fundamental uh

1331
01:03:58.320 --> 01:04:02.030
program for policymakers and uh you know, it involves,

1332
01:04:02.040 --> 01:04:03.840
you know, maybe uh changing a bit the economy,

1333
01:04:03.850 --> 01:04:06.830
it involves changing. Uh MAYBE the way the the

1334
01:04:06.840 --> 01:04:08.750
news media are functioning to make them a little

1335
01:04:08.760 --> 01:04:11.149
less uh to make them uh to make them

1336
01:04:11.159 --> 01:04:13.889
a little less polarizing, a little less partisan. It

1337
01:04:13.899 --> 01:04:17.120
calls for a number of reforms in many, many

1338
01:04:17.129 --> 01:04:20.629
different sectors of society. Uh I'm not here to

1339
01:04:20.639 --> 01:04:23.350
make, you know, specific policy recommendations. That's something that

1340
01:04:23.360 --> 01:04:24.870
I hope to maybe think a bit more about

1341
01:04:24.879 --> 01:04:28.060
in the future Michael, uh, might have recommendations to

1342
01:04:28.070 --> 01:04:29.790
that effect. I think he's giving a talk about

1343
01:04:29.800 --> 01:04:33.580
the future of free speech in, um, in Sweden,

1344
01:04:33.590 --> 01:04:35.699
uh, early December. That may be of interest to

1345
01:04:35.709 --> 01:04:38.229
some of your viewers. Uh, WELL, it will probably

1346
01:04:38.239 --> 01:04:39.570
be too late by the time your podcast is

1347
01:04:39.580 --> 01:04:42.580
out. But, yeah, a lot of people who are

1348
01:04:42.590 --> 01:04:44.719
more or less influential, are interested in how to

1349
01:04:44.729 --> 01:04:49.500
foor free speech. Uh, I think, um Jacob, um

1350
01:04:49.510 --> 01:04:52.340
what is his name? Uh Meena who wrote like

1351
01:04:52.350 --> 01:04:55.729
a History of Free Speech, um probably also has

1352
01:04:55.739 --> 01:04:58.850
pretty interesting positive recommendations about that. He's got a

1353
01:04:58.860 --> 01:05:01.669
very interesting book called um Free Speech or History

1354
01:05:01.679 --> 01:05:04.620
from Socrates to uh to I forget what that

1355
01:05:04.629 --> 01:05:06.129
is really good and that I can recommend your

1356
01:05:06.139 --> 01:05:09.270
viewers to, to check out. Uh But yeah, I'm

1357
01:05:09.280 --> 01:05:11.590
not, I'm not gonna venture into like specific, you

1358
01:05:11.600 --> 01:05:14.020
know, policy recommendations in terms of like what to

1359
01:05:14.030 --> 01:05:17.340
do to enforce free speech. Um I don't really

1360
01:05:17.350 --> 01:05:19.510
know but, but for sure that value needs to

1361
01:05:19.520 --> 01:05:22.739
be protected and uh and it's a cardinal value

1362
01:05:22.750 --> 01:05:25.520
of, of us um scientists and I assume, you

1363
01:05:25.530 --> 01:05:28.100
know, most regional liberals of, of, of, of, of

1364
01:05:28.110 --> 01:05:28.939
the West right now?

1365
01:05:29.280 --> 01:05:31.989
Great. So, uh would you like to tell people

1366
01:05:32.000 --> 01:05:36.229
just before we go the kinds of work you're

1367
01:05:36.239 --> 01:05:38.810
doing on right now, what kinds of topics you're

1368
01:05:38.820 --> 01:05:40.510
working on? And, and by the way, I didn't

1369
01:05:40.520 --> 01:05:43.060
mention this at the beginning in our in, in

1370
01:05:43.070 --> 01:05:46.320
the introduction. But in the near future, you will

1371
01:05:46.330 --> 01:05:49.409
also be moving to one stick to Jean Nico.

1372
01:05:50.229 --> 01:05:52.709
Yeah, that's it. Yeah. Yeah. Yeah. So my, my

1373
01:05:52.719 --> 01:05:56.219
contract at university is reaching its end in um

1374
01:05:56.229 --> 01:05:59.169
end of March 24 and I have a new

1375
01:05:59.179 --> 01:06:02.939
job at the Envision Nico in Paris. Uh I'll

1376
01:06:02.949 --> 01:06:06.760
be part of the um evolution and the social

1377
01:06:06.770 --> 01:06:09.629
team. So evolution and social cognition and my P

1378
01:06:09.639 --> 01:06:11.080
I is going to be Olivia Mohan, who's a

1379
01:06:11.090 --> 01:06:14.520
cultural evolutionist, who's mostly been uh been doing work

1380
01:06:14.530 --> 01:06:17.949
on like uh the cult evolution of uh of,

1381
01:06:17.959 --> 01:06:22.550
of uh scriptural systems of languages. But he's interested

1382
01:06:22.560 --> 01:06:26.520
in cultural conservatism. Why do people want to prefer

1383
01:06:26.530 --> 01:06:29.120
the status quo in many cases with respect to

1384
01:06:29.129 --> 01:06:33.429
like, you know, preserving uh cultural traditions, preferring um

1385
01:06:33.479 --> 01:06:35.699
the status quo in, in between group relationships in

1386
01:06:35.709 --> 01:06:38.010
politics, etcetera. And so, yeah, we're going to try

1387
01:06:38.020 --> 01:06:40.149
to explore like what, what motivations people may have

1388
01:06:40.159 --> 01:06:42.570
in, in, in, in preferring the status quo. And

1389
01:06:42.580 --> 01:06:44.159
so I'll be back in various and working from

1390
01:06:44.169 --> 01:06:47.860
France, studying from uh yeah, the spring 24 and

1391
01:06:47.870 --> 01:06:49.719
as, as well as my current work. Well, I

1392
01:06:49.729 --> 01:06:51.219
would say that many of the papers that have

1393
01:06:51.229 --> 01:06:52.850
been talk uh that I've been talking about today

1394
01:06:52.860 --> 01:06:55.689
are not necessarily published yet. Uh So I'm still

1395
01:06:55.699 --> 01:06:58.159
like working on them a little bit. Uh And

1396
01:06:58.169 --> 01:07:00.610
in particular, what occupies most of my time right

1397
01:07:00.620 --> 01:07:04.510
now is our work with Michael Mikel Bang um

1398
01:07:04.520 --> 01:07:07.020
about orthodoxy and speech repression. I'm trying to write

1399
01:07:07.030 --> 01:07:09.719
a relatively long theory piece about, about that and

1400
01:07:09.729 --> 01:07:12.580
the social functions of mobilizations and, and, and signaling

1401
01:07:12.590 --> 01:07:15.350
that we think it fulfills. Um And so that's

1402
01:07:15.360 --> 01:07:16.989
my main piece of work right now. Yes.

1403
01:07:17.590 --> 01:07:20.310
Uh And by the way, where can people find

1404
01:07:20.320 --> 01:07:21.489
you on the internet?

1405
01:07:21.770 --> 01:07:24.439
Uh They can find me on Twitter. Uh Of

1406
01:07:24.449 --> 01:07:29.159
course, my handle is um a capital A uh

1407
01:07:29.169 --> 01:07:35.560
underscore Marie um Ma Rie underscore S CIA Marie.

1408
01:07:35.580 --> 01:07:38.899
I like science and also have a website that

1409
01:07:38.909 --> 01:07:42.090
I need to um that maybe you, you would

1410
01:07:42.100 --> 01:07:44.020
be able to uh to cite another video, I

1411
01:07:44.030 --> 01:07:47.600
guess in some websites. Um What I and you

1412
01:07:47.610 --> 01:07:49.449
can shoot me an email at Antoine dot Marie

1413
01:07:49.459 --> 01:07:52.090
dot sc I at gmail.com if they also want

1414
01:07:52.100 --> 01:07:54.719
to. Of course, and I'm on Facebook and you

1415
01:07:54.729 --> 01:07:56.399
know, find the ball through many channels.

1416
01:07:56.939 --> 01:08:00.000
Great. So look Antoine. Thank you so much for

1417
01:08:00.010 --> 01:08:02.250
coming on the show again, as I said at

1418
01:08:02.260 --> 01:08:04.139
the beginning of the show, it was great. Fun

1419
01:08:04.149 --> 01:08:04.800
to talk to you.

1420
01:08:05.340 --> 01:08:06.879
Thanks a lot. Have a good day. See

1421
01:08:06.889 --> 01:08:10.540
you. Hi guys. Thank you for watching this interview.

1422
01:08:10.550 --> 01:08:12.879
Until the end. If you liked it, please share

1423
01:08:12.889 --> 01:08:15.360
it. Leave a like and hit the subscription button.

1424
01:08:15.689 --> 01:08:17.200
The show is brought to you by the N

1425
01:08:17.209 --> 01:08:20.640
Lights learning and development. Then differently check the website

1426
01:08:20.649 --> 01:08:24.040
at N lights.com and also please consider supporting the

1427
01:08:24.049 --> 01:08:27.580
show on Patreon or paypal. I would also like

1428
01:08:27.589 --> 01:08:29.479
to give a huge thank you to my main

1429
01:08:29.490 --> 01:08:33.790
patrons and Paypal supporters, Perera Larson, Jerry Muller and

1430
01:08:33.839 --> 01:08:36.959
Frederick Suno, Bernard Seche O of Alex Adam, Castle

1431
01:08:36.970 --> 01:08:40.040
Matthew Whitten Bear. No wolf, Tim Ho Erica LJ

1432
01:08:40.129 --> 01:08:43.309
Connors, Philip Forrest Connolly. Then the Met Robert Wine

1433
01:08:43.337 --> 01:08:46.929
in NAI Z Mar Nevs calling in Hobel Governor

1434
01:08:47.219 --> 01:08:51.087
Mikel Stormer Samuel Andre Francis for Agns Ferger and

1435
01:08:51.417 --> 01:08:54.457
H her me and Lain Jung Y and the

1436
01:08:54.627 --> 01:08:58.337
Samuel K Hes Mark Smith J Tom Hummel S

1437
01:08:58.679 --> 01:09:03.229
Friends, David Sloan Wilson, ya de ro ro Diego,

1438
01:09:03.669 --> 01:09:07.339
Jan Punter Romani Charlotte, Bli Nico Barba, Adam Hunt

1439
01:09:07.560 --> 01:09:10.779
Pavlo Stassi Nale medicine, Gary G Alman, Sam Ofri

1440
01:09:11.370 --> 01:09:15.589
and YPJ Barboa, Julian Price Edward Hall, Eden Broner

1441
01:09:16.629 --> 01:09:21.470
Douglas Fry Franca Beto Lati Cortez Solis Scott Zachary

1442
01:09:21.490 --> 01:09:27.939
ftdw Daniel Friedman, William Buckner, Paul Giorgio, Luke Loki,

1443
01:09:28.419 --> 01:09:32.720
Georgio Theophano. Chris Williams and Peter Wo David Williams

1444
01:09:32.729 --> 01:09:36.770
Di Costa Anton Erickson Charles Murray, Alex Shaw, Marie

1445
01:09:36.779 --> 01:09:42.399
Martinez, Coralie Chevalier, Bangalore Larry Dey junior, Old Ebon,

1446
01:09:42.877 --> 01:09:46.298
Starry Michael Bailey. Then Spur by Robert Grassy Zorn.

1447
01:09:46.519 --> 01:09:51.068
Jeff mcmahon, Jake Zul Barnabas Radis Mark Kemple Thomas

1448
01:09:51.078 --> 01:09:55.548
Dvor Luke Neeson, Chris to Kimberley Johnson, Benjamin Gilbert,

1449
01:09:55.559 --> 01:09:59.829
Jessica, Nicky Linda Brendan Nicholas Carlson Ismael Bensley Man

1450
01:10:00.290 --> 01:10:05.529
George Katis, Valentine Steinman, Perros, Kate Von Goler, Alexander

1451
01:10:06.509 --> 01:10:12.129
Albert Liam Dan Biar Masoud Ali Mohammadi Perpendicular J

1452
01:10:12.160 --> 01:10:16.100
Ner Urla. Good enough Gregory Hastings David Pins of

1453
01:10:16.450 --> 01:10:20.674
Sean Nelson, Mike Levin and Jos Net. A special

1454
01:10:20.685 --> 01:10:23.185
thanks to my producers is our web, Jim Frank

1455
01:10:23.194 --> 01:10:26.785
Luca Toni, Tom Veg and Bernard N Cortes Dixon

1456
01:10:26.794 --> 01:10:30.464
Bendik Muller Thomas Trumble, Catherine and Patrick Tobin, John

1457
01:10:30.475 --> 01:10:33.625
Carl, Negro, Nick Ortiz and Nick Golden. And to

1458
01:10:33.634 --> 01:10:37.774
my executive producers, Matthew lavender, Si Adrian Bogdan Knits

1459
01:10:37.825 --> 01:10:39.334
and Rosie. Thank you for all.

