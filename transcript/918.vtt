WEBVTT

1
00:00:00.079 --> 00:00:02.559
Hello, everybody. Welcome to a new episode of

2
00:00:02.569 --> 00:00:04.980
the Center. I'm your host, Ricardo Loops.

3
00:00:04.988 --> 00:00:07.960
And today I'm joined by Doctor Antoine Marie. He

4
00:00:07.969 --> 00:00:11.669
is a postdoctoral researcher at the Department of Political Science

5
00:00:11.679 --> 00:00:15.538
at a University in Denmark. Uh We've already had

6
00:00:15.550 --> 00:00:17.530
an interview on the show. I'm leaving a link

7
00:00:17.539 --> 00:00:20.010
to it in the description box of this one where

8
00:00:20.019 --> 00:00:24.870
we talked basically about misinformation, conspiracy theories and some

9
00:00:24.879 --> 00:00:27.539
other topics. And today we're going to expand a

10
00:00:27.548 --> 00:00:30.548
little bit on that and also talk about a few

11
00:00:30.559 --> 00:00:34.429
more topics like for example, how people evaluate political

12
00:00:34.439 --> 00:00:37.750
decisions, the sharing of fake news on social media

13
00:00:37.969 --> 00:00:41.408
, gender biases in hiring processes, the repression of

14
00:00:41.418 --> 00:00:45.173
free speech, political activism. And basically, as

15
00:00:45.185 --> 00:00:48.014
we go through those topics, we're going to focus

16
00:00:48.024 --> 00:00:52.804
mostly on the biases that arise from people having strong

17
00:00:52.814 --> 00:00:56.723
moral convictions and the consequences of those moral convictions.

18
00:00:56.734 --> 00:00:58.993
So, Antoine, welcome back to the show.

19
00:00:59.005 --> 00:01:00.789
It's always a pleasure to everyone. Thanks a lot

20
00:01:00.798 --> 00:01:03.618
, Ricardo and uh thanks a lot for having me

21
00:01:03.630 --> 00:01:10.790
again. So starting with how people evaluate political decisions

22
00:01:10.799 --> 00:01:14.209
. I mean, is it that they care more

23
00:01:14.219 --> 00:01:19.469
about the concrete impact that certain political decisions have on

24
00:01:19.680 --> 00:01:23.808
society or do they care more about the values and

25
00:01:23.819 --> 00:01:30.400
perhaps symbols that drive those same political decisions? Mm

26
00:01:30.430 --> 00:01:33.028
. So it's a big topic. I imagine you're

27
00:01:33.040 --> 00:01:34.599
referring to uh pre print that I have with the

28
00:01:34.609 --> 00:01:38.519
co-author uh I got had and brand, brand Freeland

29
00:01:38.528 --> 00:01:42.689
who was my ph advisor, it provisionally called intentions

30
00:01:42.698 --> 00:01:45.409
and efficiency and policy variations. But I would like

31
00:01:45.418 --> 00:01:48.609
to rename maybe something like people don't care too much

32
00:01:48.620 --> 00:01:51.058
about policy efficiency and they value a lot, the

33
00:01:51.069 --> 00:01:53.870
the intentions are driving them. And so basically what

34
00:01:53.879 --> 00:01:56.049
we're doing in that, in that pretty simple um

35
00:01:56.058 --> 00:01:59.129
setup is that we present participants with like the new

36
00:01:59.138 --> 00:02:00.099
little vignette that present, you know, the,

37
00:02:00.588 --> 00:02:04.000
the sort of like the decision making process of a

38
00:02:04.010 --> 00:02:06.799
politician or a CEO who's willing to implement a new

39
00:02:06.808 --> 00:02:08.058
policy for uh for instance, I don't know,

40
00:02:08.139 --> 00:02:10.199
do you know co2 emissions and you know, protect

41
00:02:10.210 --> 00:02:15.169
the environment or whatnot? We provide information about the

42
00:02:15.179 --> 00:02:17.210
level of efficiency or impact that the policy has its

43
00:02:17.219 --> 00:02:20.169
financial cost? Is it gonna cause, you know

44
00:02:20.179 --> 00:02:22.199
, the state or the or the company to lose

45
00:02:22.210 --> 00:02:23.379
vast amounts of money or to save vast amounts of

46
00:02:23.389 --> 00:02:28.258
money and we manipulate whether the decision maker is altruistic

47
00:02:28.270 --> 00:02:29.479
motivated. You know, I, I want to

48
00:02:29.490 --> 00:02:30.929
do that because I want to help the issue I

49
00:02:30.939 --> 00:02:34.069
want to protect the environment versus um I really don't

50
00:02:34.080 --> 00:02:36.000
care about the environment. All I'm interested in is

51
00:02:36.008 --> 00:02:38.349
to improve the image of our company or, you

52
00:02:38.360 --> 00:02:39.520
know, uh increase chances that we get re elected

53
00:02:39.528 --> 00:02:43.278
at the next election or whatnot. And so,

54
00:02:43.288 --> 00:02:44.860
so it's, it's a pretty simple setup, you

55
00:02:44.868 --> 00:02:46.758
know, it does, it doesn't necessarily max out

56
00:02:46.770 --> 00:02:47.659
in terms of like external validity, but it allows

57
00:02:47.669 --> 00:02:52.929
us to pretty carefully manipulate factors that are seemingly of

58
00:02:52.939 --> 00:02:53.729
interest in how people, you know, ordinary people

59
00:02:53.740 --> 00:02:58.099
form attitudes uh about whether a political, political decision

60
00:02:58.110 --> 00:02:59.879
is good or bad, you know, laudable or

61
00:02:59.889 --> 00:03:01.270
not, et cetera. And what we tend to

62
00:03:01.278 --> 00:03:05.189
find is that even when, even when the policy

63
00:03:05.199 --> 00:03:06.758
is, is described as having, you know,

64
00:03:06.770 --> 00:03:09.929
absolutely anecdotal positive impact, like less than 1% efficiency

65
00:03:10.088 --> 00:03:14.250
, decreasing CO2 emissions. And when it described as

66
00:03:14.258 --> 00:03:15.118
being, you know, costing a huge fortune to

67
00:03:15.129 --> 00:03:17.129
the company or to the state, in other words

68
00:03:17.139 --> 00:03:20.558
, when the policy is described as being vastly inefficient

69
00:03:21.588 --> 00:03:23.838
, just the fact that we described the policy as

70
00:03:23.849 --> 00:03:27.020
being otherwise altruistically motivated, you know, in the

71
00:03:27.028 --> 00:03:29.118
way it's being implemented by the minister or, or

72
00:03:29.129 --> 00:03:31.308
the CEO leads most participants to rate the policy as

73
00:03:31.319 --> 00:03:35.558
being more commendable or to or to support it more

74
00:03:35.569 --> 00:03:38.659
than an alternative policy that is described as being super

75
00:03:38.669 --> 00:03:40.099
, super efficient, uh as you know, allowing

76
00:03:40.110 --> 00:03:44.050
the organization to save vast amounts of money. But

77
00:03:44.058 --> 00:03:46.740
that is described as being implemented for reasons that have

78
00:03:46.750 --> 00:03:47.319
to do with the pursuit of, you know,

79
00:03:47.360 --> 00:03:51.360
a good image being re-elected or whatnot. And so

80
00:03:51.368 --> 00:03:53.629
we think that's a little of concern because, you

81
00:03:53.639 --> 00:03:55.838
know, in uh modern societies, many of the

82
00:03:55.849 --> 00:04:00.379
motivations that private actors, but also potentially public actors

83
00:04:00.389 --> 00:04:03.729
have on the market in society is to um uh

84
00:04:03.740 --> 00:04:05.960
make profits, is to, you know, give

85
00:04:05.969 --> 00:04:10.520
themselves a good image is to potentially uh you know

86
00:04:10.569 --> 00:04:13.069
, find technological solutions that will allow them to become

87
00:04:13.080 --> 00:04:15.439
richer. And so if people punish or do not

88
00:04:15.449 --> 00:04:21.428
support um policy programs uh policy uh initiatives, they

89
00:04:21.439 --> 00:04:25.088
are motivated by, you know, the willingness to

90
00:04:25.100 --> 00:04:27.420
uh make money to become richer, to have a

91
00:04:27.428 --> 00:04:30.910
successful company, even when they're actually immensely beneficial for

92
00:04:30.920 --> 00:04:32.108
society, even when they massively, you know,

93
00:04:32.119 --> 00:04:34.649
help at solving a given problem that, you know

94
00:04:34.660 --> 00:04:38.528
, humanity is facing, that's clearly problematic. And

95
00:04:38.540 --> 00:04:40.829
so, well, we're just finding that in that

96
00:04:40.838 --> 00:04:42.338
in, in, in those in those set of

97
00:04:42.350 --> 00:04:44.939
studies that people don't seem to be very consequentialist or

98
00:04:44.949 --> 00:04:46.350
very pragmatic in the way that they rate policy decisions

99
00:04:46.809 --> 00:04:49.250
. They seem to care a lot about the intentions

100
00:04:49.259 --> 00:04:51.309
that drive the poli political policies and they seem to

101
00:04:51.319 --> 00:04:56.290
disregard a lot um the the policy impact. Now

102
00:04:56.298 --> 00:04:58.500
I should specify that the way we describe the policy

103
00:04:58.509 --> 00:05:01.420
impact is not completely neutral, we provide um uh

104
00:05:01.480 --> 00:05:05.238
information about the efficiency in numerical format. We don't

105
00:05:05.250 --> 00:05:08.350
provide graphs that is not in the current version of

106
00:05:08.358 --> 00:05:12.290
the experiments. So the defense of that choice is

107
00:05:12.298 --> 00:05:13.809
to say, well, you know, very often

108
00:05:13.819 --> 00:05:16.259
information about political policies impact is provided in numerical format

109
00:05:16.269 --> 00:05:18.059
, in journals, in newspapers, etcetera. So

110
00:05:18.069 --> 00:05:20.869
it's fairly ecologically valid. But at the same time

111
00:05:20.879 --> 00:05:21.980
, you could say, well, yeah, but

112
00:05:21.988 --> 00:05:25.269
no surprise that people are not really reacting too much

113
00:05:25.278 --> 00:05:28.220
to, you know, uh numerical descriptions of,

114
00:05:28.230 --> 00:05:30.178
of imbalances in policy efficiency. Because the the mind

115
00:05:30.189 --> 00:05:33.298
is not particularly equipped to, you know, give

116
00:05:33.309 --> 00:05:35.069
a lot of like intuitive meaning to figures, to

117
00:05:35.079 --> 00:05:38.519
big figures to represent all those of magnitude clearly et

118
00:05:38.528 --> 00:05:40.528
Zara. And of course, I would of course

119
00:05:40.540 --> 00:05:41.798
agree with that. And that's also one of the

120
00:05:41.809 --> 00:05:44.720
arguments in the paper, we essentially say that people

121
00:05:44.730 --> 00:05:47.250
tend to deviate from the principles of pragmatism when assessing

122
00:05:47.629 --> 00:05:50.000
uh the political policy that we expose them to and

123
00:05:50.009 --> 00:05:53.639
that we present them to because essentially the mind is

124
00:05:53.649 --> 00:05:57.548
mostly shaped by natural selection to react to signals about

125
00:05:57.559 --> 00:05:59.309
, you know, the value as a cooperator of

126
00:05:59.319 --> 00:06:01.338
potential uh social actors. Be it the CEO be

127
00:06:01.350 --> 00:06:05.100
it the minister so that people will spontaneously attend to

128
00:06:05.108 --> 00:06:08.699
, they will care whether or not this or that

129
00:06:08.709 --> 00:06:11.149
politician seems to be a good cooperator and they will

130
00:06:11.160 --> 00:06:14.350
take into account the intentions that animates them in implementing

131
00:06:14.358 --> 00:06:16.379
the policy a lot. But as for the the

132
00:06:16.389 --> 00:06:18.980
the more abstract, the more delayed, the more

133
00:06:18.988 --> 00:06:21.509
macro you know, level uh impact of the policy

134
00:06:21.519 --> 00:06:23.939
, that's not something that the mind in two D

135
00:06:23.949 --> 00:06:27.970
process is very well unfort unfortunately, and that we

136
00:06:27.980 --> 00:06:29.778
think, you know, the the two mechanisms combined

137
00:06:29.790 --> 00:06:31.379
we think are leading people to, to become so

138
00:06:31.389 --> 00:06:33.389
uh so insensitive to, to the principles of,

139
00:06:33.399 --> 00:06:36.540
of pragmatic. Yeah. A a and also I'm

140
00:06:36.548 --> 00:06:40.928
not completely sure if this is relevant to the particular

141
00:06:40.939 --> 00:06:44.048
context of this study and what we're you were trying

142
00:06:44.059 --> 00:06:46.920
to understand there. But I, isn't it also

143
00:06:46.928 --> 00:06:51.199
the case that for many, if not most political

144
00:06:51.209 --> 00:06:57.119
decisions, uh people many times do not really have

145
00:06:57.129 --> 00:07:00.608
access to very accurate data or perhaps sometimes it's also

146
00:07:00.619 --> 00:07:06.519
very hard to predict the precise impact that specific policies

147
00:07:06.528 --> 00:07:10.439
will have on society. No, of course,

148
00:07:10.449 --> 00:07:13.689
I mean, 100% agreement. Uh That's obviously the

149
00:07:13.699 --> 00:07:16.639
case. Um you know, a fact like is

150
00:07:16.649 --> 00:07:19.858
that politician good or bad as a person, is

151
00:07:19.869 --> 00:07:23.399
there more character, you know, two or three

152
00:07:23.410 --> 00:07:25.608
or three in the, in the perspective of cooperating

153
00:07:25.619 --> 00:07:28.949
with them? That's almost like a fact, something

154
00:07:28.959 --> 00:07:30.389
that you can assess, you know, by direct

155
00:07:30.399 --> 00:07:31.660
observation, by interacting with people or by seeing how

156
00:07:31.670 --> 00:07:33.850
they talk, by seeing how their, you know

157
00:07:33.858 --> 00:07:36.329
, claims over time are consistent with each other,

158
00:07:36.338 --> 00:07:39.129
whether they do what they say and they say what

159
00:07:39.139 --> 00:07:41.309
they do or what not. So that seems relatively

160
00:07:41.319 --> 00:07:44.949
easy to assess but policy efficiency, not so right

161
00:07:44.959 --> 00:07:47.858
. It's like very high level facts. It's often

162
00:07:47.869 --> 00:07:49.569
very much delayed in time. You know, you

163
00:07:49.579 --> 00:07:53.519
will only know if um a political program, you

164
00:07:53.528 --> 00:07:54.819
know, meant to fight. Uh I don't know

165
00:07:54.829 --> 00:07:58.769
poverty in Africa or meant to reduce inequalities or to

166
00:07:58.778 --> 00:08:01.350
uh reduce unemployment or whatnot. You will only know

167
00:08:01.358 --> 00:08:03.108
if it will bear its fruits in the, in

168
00:08:03.119 --> 00:08:05.358
the midterm future, even in the long term future

169
00:08:05.369 --> 00:08:09.298
, it interacts like a zillion number of variables in

170
00:08:09.309 --> 00:08:11.699
the national economy and in the global economy that you

171
00:08:11.709 --> 00:08:13.528
have no control over, it's extremely hard to run

172
00:08:13.699 --> 00:08:16.040
randomized control trials on economic outcomes or, you know

173
00:08:16.048 --> 00:08:18.850
, public policies, everything interacts with, with a

174
00:08:18.858 --> 00:08:22.869
bit of everything. Of course, economists are skilled

175
00:08:22.879 --> 00:08:26.220
at kind of like, you know, maximizing uh

176
00:08:26.588 --> 00:08:31.019
the possibility of like answering scientifically to those questions and

177
00:08:31.028 --> 00:08:33.048
assessing policy impact. But it's always very difficult for

178
00:08:33.058 --> 00:08:35.940
everyone, including for them. And it's obviously not

179
00:08:35.950 --> 00:08:37.658
surprising that the lay citizen, you know, the

180
00:08:37.668 --> 00:08:39.129
man in the street, the woman in the street

181
00:08:39.139 --> 00:08:43.250
is not well equipped to anticipate um you know,

182
00:08:43.259 --> 00:08:45.340
what will be the, the the macros social impact

183
00:08:45.349 --> 00:08:46.570
of a policy, I mean to, to,

184
00:08:46.580 --> 00:08:48.879
to infer to, to induce any sort of like

185
00:08:48.889 --> 00:08:52.320
conclusion about whether a policy works or doesn't work.

186
00:08:52.330 --> 00:08:54.570
You have to, you would have to ideally have

187
00:08:54.580 --> 00:08:56.308
access to facts and outcomes that are distributed on the

188
00:08:56.320 --> 00:08:58.450
on the territory, which is of course impossible to

189
00:08:58.460 --> 00:09:03.298
do individually. Uh Most people are not professional scientists

190
00:09:03.308 --> 00:09:05.678
, uh social scientists, sociologists, economists. Um

191
00:09:07.029 --> 00:09:09.239
and, and even assuming that people have access to

192
00:09:09.250 --> 00:09:11.200
relatively reliable policy information, which is already extremely hard

193
00:09:11.210 --> 00:09:13.469
to do, you know, requires representative samples,

194
00:09:13.479 --> 00:09:16.918
it requires the econometric method, etcetera, etcetera.

195
00:09:16.139 --> 00:09:20.408
Even assuming that people have that information very often that

196
00:09:20.418 --> 00:09:22.019
information that rigors, uh you know, opinion that

197
00:09:22.029 --> 00:09:24.779
they may come up with about whether a given program

198
00:09:24.788 --> 00:09:26.529
works or, or or doesn't work at, you

199
00:09:26.538 --> 00:09:30.908
know, advancing a given political issue. People will

200
00:09:30.918 --> 00:09:33.158
often not have the motivation to act on the,

201
00:09:33.168 --> 00:09:33.729
on the, on the basis of that. So

202
00:09:33.739 --> 00:09:37.080
for instance, I might know that it's, you

203
00:09:37.090 --> 00:09:39.109
know, bad nowadays to eat meat and to take

204
00:09:39.119 --> 00:09:41.288
the plane. And I might know very well reflectively

205
00:09:41.298 --> 00:09:43.298
that I'm supposed to stop eating meat or at least

206
00:09:43.308 --> 00:09:45.700
beef and take the plane less. Well, it's

207
00:09:45.710 --> 00:09:48.928
pretty hard from a mot visional perspective to be completely

208
00:09:48.940 --> 00:09:50.710
compelled to act in accordance with those principles. I

209
00:09:50.719 --> 00:09:52.889
try, I try my best but very often,

210
00:09:52.899 --> 00:09:54.678
you know, I'm experiencing a sort of like weakness

211
00:09:54.690 --> 00:09:58.210
of will and I will fail to meet in practice

212
00:09:58.219 --> 00:10:00.269
those sort of like more standards that I claim to

213
00:10:00.279 --> 00:10:01.769
have and that I claim to follow. Mhm.

214
00:10:01.989 --> 00:10:05.308
Uh Let me just ask you one more question before

215
00:10:05.320 --> 00:10:07.369
we move on to another topic about this. And

216
00:10:07.379 --> 00:10:09.048
this is perhaps something that I will also ask you

217
00:10:09.058 --> 00:10:11.840
when it comes to other topics we're going to explore

218
00:10:11.849 --> 00:10:16.029
here today. But when it comes to this particular

219
00:10:16.038 --> 00:10:18.729
study, I mean, do you expect this to

220
00:10:18.739 --> 00:10:22.989
apply to basically everyone, regardless of their, for

221
00:10:24.000 --> 00:10:28.989
example, political ideology or partisanship or I mean,

222
00:10:28.000 --> 00:10:31.639
is there any way, by which perhaps there are

223
00:10:31.649 --> 00:10:35.200
some differences between, for example, left and right

224
00:10:35.210 --> 00:10:39.320
liberals and conservatives or not? Um, it's a

225
00:10:39.330 --> 00:10:41.908
, it's a big question. I wouldn't claim to

226
00:10:41.918 --> 00:10:45.298
be able to answer this one. So I'm a

227
00:10:45.308 --> 00:10:46.538
political psychologist but I don't really claim to be a

228
00:10:46.548 --> 00:10:48.658
specialist of like, uh, you know, subgroup

229
00:10:48.668 --> 00:10:52.599
differences or, or more, more differences across,

230
00:10:52.609 --> 00:10:56.149
um, uh, yeah, across differences of partisanship

231
00:10:56.158 --> 00:10:58.710
or, or more sensitivity. I would say that

232
00:10:58.719 --> 00:11:01.798
a tendency to overvalue intentions with respect to causes that

233
00:11:01.808 --> 00:11:03.139
you care about, you know, to, to

234
00:11:03.149 --> 00:11:07.109
want to positively reward or, or, or thank

235
00:11:07.119 --> 00:11:09.038
, you know, politicians or social actors who are

236
00:11:09.048 --> 00:11:11.479
helping an issue versus wanting to punish those who are

237
00:11:11.668 --> 00:11:15.178
selfish or who disregard the cause that you care about

238
00:11:15.190 --> 00:11:16.104
that will tend to be universal, I think because

239
00:11:16.114 --> 00:11:18.413
it's very likely to be part of our sort of

240
00:11:18.423 --> 00:11:22.205
like universal, you know, social cognition package.

241
00:11:22.494 --> 00:11:24.494
It's like cognitive skills that you need to engage all

242
00:11:24.504 --> 00:11:28.004
the time every day and that you would have had

243
00:11:28.014 --> 00:11:30.335
to engage, you know, ancestrally regularly over the

244
00:11:30.344 --> 00:11:33.803
millennials to gauge the trustworthiness of potential co-operation partners.

245
00:11:33.815 --> 00:11:35.764
So this you should expect will be so much universal

246
00:11:37.529 --> 00:11:37.830
. Now, at the same time, of course

247
00:11:37.840 --> 00:11:39.989
, there are differences are, you know, relevant

248
00:11:41.000 --> 00:11:45.129
morally. And so it's possible that people who are

249
00:11:45.139 --> 00:11:46.700
a bit like, you know, who have a

250
00:11:46.710 --> 00:11:50.239
bit of a effective altruist sensitivity or who are maybe

251
00:11:50.250 --> 00:11:52.259
a bit more libertarian or maybe a bit more on

252
00:11:52.269 --> 00:11:54.469
the autistic spectrum or whatnot. It's possible that people

253
00:11:54.479 --> 00:11:58.229
who have those personality characteristics who are a little less

254
00:11:58.239 --> 00:12:01.099
, you know, um, elsewhere than in the

255
00:12:01.109 --> 00:12:05.428
cognitive average of the population. It's possible that those

256
00:12:05.440 --> 00:12:07.418
people may on average spontaneously or being or, or

257
00:12:07.428 --> 00:12:11.798
maybe educated to value policy impact a bit more than

258
00:12:11.808 --> 00:12:13.759
the good principles and the intentions that drive them,

259
00:12:13.769 --> 00:12:16.619
that they may be a bit more psychologically compatible with

260
00:12:16.629 --> 00:12:20.129
the principles of utilitarianism and pragmatism. Uh I think

261
00:12:20.139 --> 00:12:20.678
there's some evidence doing that, but I don't claim

262
00:12:20.690 --> 00:12:22.889
to be a, I don't claim to be a

263
00:12:22.899 --> 00:12:26.099
specialist of that. And um and yeah, and

264
00:12:26.109 --> 00:12:30.649
as regards the policy efficiency, I think, yeah

265
00:12:30.658 --> 00:12:31.090
, most people will have difficulty, you know,

266
00:12:31.099 --> 00:12:35.250
representing uh orders of magnitudes and, and, and

267
00:12:35.259 --> 00:12:37.450
impacts and to gauge policy efficiency based on figures.

268
00:12:37.460 --> 00:12:39.918
But of course, here again, there's a bit

269
00:12:39.928 --> 00:12:41.529
of margin of leeway. If you, you know

270
00:12:41.538 --> 00:12:43.408
, learn for many years that if you work for

271
00:12:43.418 --> 00:12:45.710
many years at the Ministry of the Economy or whatnot

272
00:12:45.719 --> 00:12:46.658
, you learn to many people and to handle figures

273
00:12:46.668 --> 00:12:48.509
and to represent what they mean a bit more,

274
00:12:48.519 --> 00:12:50.369
you have more of a sense of like, you

275
00:12:50.379 --> 00:12:54.129
know what 10 million represents with respect to as compared

276
00:12:54.139 --> 00:12:56.418
to 1 billion. And you are be able to

277
00:12:56.428 --> 00:12:58.918
get a sense of like what is the budget of

278
00:12:58.928 --> 00:13:00.658
the state? What is the budget of the company

279
00:13:00.690 --> 00:13:01.558
and that puts you in a better position to reason

280
00:13:01.570 --> 00:13:03.668
about figures. I think, I think even in

281
00:13:03.678 --> 00:13:05.440
my conference since our French president, he is probably

282
00:13:05.450 --> 00:13:07.690
much better at reasoning about figures when it comes to

283
00:13:07.849 --> 00:13:11.788
economic. Although men than I myself, just because

284
00:13:11.798 --> 00:13:13.440
he was the minister of the economy for many years

285
00:13:13.450 --> 00:13:15.269
. And he's a professional doing that. And you

286
00:13:15.279 --> 00:13:16.168
would hope that politicians are good at doing that.

287
00:13:16.509 --> 00:13:18.908
And to be honest, I don't know what my

288
00:13:18.918 --> 00:13:20.399
colleagues in political science who work on political elites are

289
00:13:20.408 --> 00:13:22.048
finding, I hope that they are finding that they

290
00:13:22.058 --> 00:13:24.229
had a decent numerical literacy, but I'm not completely

291
00:13:24.239 --> 00:13:28.769
convinced. Yeah, le let's see, let's see

292
00:13:28.779 --> 00:13:31.869
about that. So, OK, so in our

293
00:13:31.879 --> 00:13:35.399
previous conversation, we talked a little bit about misinformation

294
00:13:35.408 --> 00:13:37.869
and just to try to perhaps establish a bridge between

295
00:13:37.879 --> 00:13:41.279
our two conversations here. Uh when it comes to

296
00:13:41.288 --> 00:13:46.729
sharing partisan news on social media specifically. So you

297
00:13:46.739 --> 00:13:48.479
mean you mean making partisan news of, of,

298
00:13:48.489 --> 00:13:52.678
of, of news finds us sorry, like you

299
00:13:52.690 --> 00:13:54.080
mean like sharing, sharing you selectively in a way

300
00:13:54.090 --> 00:13:56.869
that is partisan? Yes, exactly. Exactly.

301
00:13:58.070 --> 00:14:01.798
So do do strong moral convictions also play a role

302
00:14:01.808 --> 00:14:05.989
there. And if so what are the consequences?

303
00:14:07.509 --> 00:14:09.658
Um Yes, they do. They absolutely do.

304
00:14:09.668 --> 00:14:11.548
So we have, we have that uh that paper

305
00:14:11.558 --> 00:14:13.619
recently published in, in Pnes Nexus with um my

306
00:14:13.629 --> 00:14:16.320
friends Sasha Altai and, and Brain Street Clan again

307
00:14:16.330 --> 00:14:18.558
, in which we essentially show using you know,

308
00:14:18.570 --> 00:14:22.879
relatively simple online experiments that the more people that,

309
00:14:22.889 --> 00:14:24.320
that first of all people are, are, are

310
00:14:24.330 --> 00:14:26.928
disposed to engage in what we call my side sharing

311
00:14:26.940 --> 00:14:30.298
or partisan sharing. They report higher willingness to share

312
00:14:30.308 --> 00:14:31.469
, you know, news items either true or false

313
00:14:31.479 --> 00:14:33.798
that fit the ideology. You know, there are

314
00:14:33.808 --> 00:14:37.399
congruent to their side that for instance, portray,

315
00:14:37.408 --> 00:14:39.109
you know, um a social threat that they are

316
00:14:39.119 --> 00:14:41.075
likely to believe in. Let's say I'm a democrat

317
00:14:41.264 --> 00:14:43.144
, I'm gonna want to pass along and I'm gonna

318
00:14:43.154 --> 00:14:46.033
be more likely to pass along the news that a

319
00:14:46.043 --> 00:14:46.524
piece of news that says that, I don't know

320
00:14:46.533 --> 00:14:50.154
, there's a big problem but like uh 3d printing

321
00:14:50.163 --> 00:14:50.715
of guns because it's gonna cause, you know,

322
00:14:50.724 --> 00:14:52.354
the spread of, of, of, of,

323
00:14:52.364 --> 00:14:54.484
of like homemade guns in the US or whatnot.

324
00:14:54.803 --> 00:14:56.774
So people tend to share news that are con to

325
00:14:56.783 --> 00:15:01.634
their side uh plausible threats. Um Statistical facts that

326
00:15:01.644 --> 00:15:03.234
they think are true. You know, for instance

327
00:15:03.244 --> 00:15:05.514
, um you should take a bunch of like a

328
00:15:05.524 --> 00:15:09.875
liberal participants. They are fairly likely to pass along

329
00:15:09.298 --> 00:15:11.918
. Uh a news item that says that this,

330
00:15:11.928 --> 00:15:15.158
for instance, you know, five or 10% gender

331
00:15:15.168 --> 00:15:16.840
pay gap between men and women in the US,

332
00:15:16.849 --> 00:15:18.989
which is roughly true, right? But they're also

333
00:15:18.000 --> 00:15:20.239
pretty likely to want to share um a piece of

334
00:15:20.250 --> 00:15:22.178
fake news that will say that there's a 60% pay

335
00:15:22.190 --> 00:15:24.729
gap between men and women, which is false,

336
00:15:24.739 --> 00:15:26.798
which is vastly exaggerated. Right. So, yeah

337
00:15:26.808 --> 00:15:31.288
, news items, claims, uh rumors, true

338
00:15:31.298 --> 00:15:33.950
or false when they fit people's uh prior expectations and

339
00:15:33.960 --> 00:15:37.558
moral commitments, you know, uh political convictions,

340
00:15:37.570 --> 00:15:41.139
um, activist motivations or whatnot uh will tend to

341
00:15:41.149 --> 00:15:43.879
be passed along more in real life and social media

342
00:15:43.940 --> 00:15:45.759
. The, the, the studies that are included

343
00:15:45.769 --> 00:15:46.190
in the paper that I'm talking about are only on

344
00:15:46.200 --> 00:15:48.649
social media, but we also know that that's the

345
00:15:48.658 --> 00:15:50.719
case in, in real life. Um So,

346
00:15:50.729 --> 00:15:52.979
yeah, people have the tendency that 10 is amplified

347
00:15:52.989 --> 00:15:56.599
by uh the degree to which demoralize or that they

348
00:15:56.609 --> 00:15:58.590
deem you know, certain issue to be of like

349
00:15:58.599 --> 00:16:00.678
political priority for them. So whether, you know

350
00:16:00.690 --> 00:16:03.389
, they see a political issue, gun control abortion

351
00:16:03.580 --> 00:16:04.340
or, or the, or the ban on abortion

352
00:16:04.349 --> 00:16:07.639
, for instance, for, for conservatives as being

353
00:16:07.649 --> 00:16:08.619
like a core value for their moral identity, for

354
00:16:08.629 --> 00:16:11.678
instance, or are being central to their moral identity

355
00:16:11.129 --> 00:16:14.820
when people, you know, rate. Um I

356
00:16:14.830 --> 00:16:15.418
mean, report that they highly more or less an

357
00:16:15.428 --> 00:16:18.759
issue that amplifies identity to share uh new selectively.

358
00:16:19.269 --> 00:16:22.979
Uh And we also find unsurprisingly that people who are

359
00:16:22.989 --> 00:16:23.729
more extreme on an issue, even when they don't

360
00:16:23.739 --> 00:16:26.229
moralize, it are more likely to uh to share

361
00:16:26.239 --> 00:16:29.048
news related to that issue in ways that are,

362
00:16:29.058 --> 00:16:32.048
that are partisan. So what does that mean?

363
00:16:32.379 --> 00:16:34.710
Um It can mean a mix of several things.

364
00:16:34.719 --> 00:16:40.359
Um The spontaneous interpretation would simply be to say that

365
00:16:40.369 --> 00:16:41.269
, you know, the more people care about an

366
00:16:41.279 --> 00:16:44.658
issue. They want, people want to tackle a

367
00:16:44.668 --> 00:16:47.009
social problem. They want to, with the more

368
00:16:47.019 --> 00:16:48.969
they want to advance or cause the, the more

369
00:16:48.979 --> 00:16:51.769
they are to behave like little activists, like little

370
00:16:51.779 --> 00:16:52.440
, you know, militant members of a tribe.

371
00:16:52.450 --> 00:16:55.899
And they want to pass along, you know,

372
00:16:56.139 --> 00:16:59.558
um claims information that they think will mobilize other people

373
00:16:59.570 --> 00:17:03.119
to take action or that potentially can also um they

374
00:17:03.129 --> 00:17:04.118
, they can also pass along those militant claims because

375
00:17:04.130 --> 00:17:07.779
they expect that they will therefore be able to signal

376
00:17:07.789 --> 00:17:08.420
, you know, a group memberships. I think

377
00:17:08.430 --> 00:17:11.670
both motivations are taking place. I think both mechanisms

378
00:17:12.309 --> 00:17:17.250
uh are driving part sharing to some extent. Um

379
00:17:17.259 --> 00:17:22.439
But we admit that it's also unclear whether um people

380
00:17:22.449 --> 00:17:23.759
are not also simply sharing uh you know, news

381
00:17:23.769 --> 00:17:27.140
items in partition ways because they think that the message

382
00:17:27.150 --> 00:17:29.549
that the, that the claim and caption, it

383
00:17:29.559 --> 00:17:30.539
is more plausible. So there's a sort of like

384
00:17:30.670 --> 00:17:33.449
there's a sort of like, you know, um

385
00:17:34.309 --> 00:17:37.108
it's hard to tell to what extent partisan safety sharing

386
00:17:37.118 --> 00:17:41.229
is carried forward by stronger prior beliefs or strong convictions

387
00:17:41.239 --> 00:17:45.529
that the claim is true versus more motivated thinking or

388
00:17:45.539 --> 00:17:48.559
more political or, or more motivations that, that

389
00:17:48.568 --> 00:17:49.430
, that sort of like motivate people to act in

390
00:17:49.439 --> 00:17:52.269
somewhat activist ways by signaling allegiances and by trying to

391
00:17:52.279 --> 00:17:56.219
influence the the belief of others about those, those

392
00:17:56.229 --> 00:17:59.029
um those social topics potentially to, to mobilize them

393
00:17:59.039 --> 00:18:00.338
or to prompt them to act, that's a little

394
00:18:00.348 --> 00:18:03.539
unclear. Uh And in that paper, we're not

395
00:18:03.549 --> 00:18:04.809
in a position to, to tell what is the

396
00:18:04.818 --> 00:18:07.709
relative contribution of the two mechanisms. And there's a

397
00:18:07.719 --> 00:18:11.049
big debate right now in political behavior, political psychology

398
00:18:11.059 --> 00:18:14.509
, political science about the extent to which um you

399
00:18:14.519 --> 00:18:17.618
know, differences between partisans in behavior, in news

400
00:18:17.630 --> 00:18:21.088
consumption, in judgments of accuracy of a given claim

401
00:18:21.098 --> 00:18:22.309
in decisions to share news, et cetera. To

402
00:18:22.318 --> 00:18:26.509
what extent those differences between partisans between political subgroups are

403
00:18:26.519 --> 00:18:29.509
explained by, you know, political motivated thinking,

404
00:18:29.519 --> 00:18:32.979
instrumental, instrumental motives as opposed to more, uh

405
00:18:32.989 --> 00:18:34.410
you know, accuracy oriented, just differences in part

406
00:18:34.430 --> 00:18:37.400
of beliefs. Uh It's a big, it's a

407
00:18:37.410 --> 00:18:38.670
big debate right now. I can only recommend your

408
00:18:38.680 --> 00:18:41.189
uh your viewers to check out the work of Ben

409
00:18:41.199 --> 00:18:44.118
Tappin, which has done a lot of good work

410
00:18:44.130 --> 00:18:45.279
with the Golden Penny Cook and David ran about that

411
00:18:45.519 --> 00:18:48.199
. But they try to, to argue that much

412
00:18:48.209 --> 00:18:51.890
, many instances of what seem to be partisan motivated

413
00:18:51.900 --> 00:18:53.719
thinking might actually boil down to different in priors.

414
00:18:55.150 --> 00:18:56.640
I think, I think the argument that they make

415
00:18:56.650 --> 00:18:57.949
is pretty convincing. I still think that motivated thinking

416
00:18:57.959 --> 00:19:00.868
is real. I still think that it's very real

417
00:19:00.170 --> 00:19:03.509
. Um But it's clearly the case that a lot

418
00:19:03.519 --> 00:19:04.250
in a lot of like survey contexts, a lot

419
00:19:04.259 --> 00:19:07.088
of like social science context, what might be taken

420
00:19:07.098 --> 00:19:10.890
at face value as evidence of motivated thinking might actually

421
00:19:10.900 --> 00:19:12.410
boil down to just partisan disagreements in what is true

422
00:19:12.420 --> 00:19:15.469
in the world and what is plausible piece of information

423
00:19:15.479 --> 00:19:17.729
. You know. Now I think you asked me

424
00:19:17.739 --> 00:19:18.000
, you, you, you asked me about the

425
00:19:18.009 --> 00:19:22.219
consequences of moralizing the topic and, and the,

426
00:19:22.368 --> 00:19:23.759
the impact of uh of that on, on,

427
00:19:23.769 --> 00:19:26.769
on, on, on partisan sharing of, of

428
00:19:26.779 --> 00:19:27.459
news items. So, yeah, as I said

429
00:19:27.469 --> 00:19:30.170
, it amplifies your, your propensity to, to

430
00:19:30.180 --> 00:19:34.439
effectively share news that fit your ideology. But in

431
00:19:34.449 --> 00:19:37.029
the paper, we, we don't just describe uh

432
00:19:37.039 --> 00:19:38.299
we don't just register that fact. We also try

433
00:19:38.309 --> 00:19:42.219
to um test to what extent it is robust and

434
00:19:42.229 --> 00:19:44.969
to what extent it can be kind of like influenced

435
00:19:44.979 --> 00:19:48.348
or mitigated by interventions. And so when we manipulated

436
00:19:48.358 --> 00:19:51.578
the the perceived um audience of the sharing, is

437
00:19:51.588 --> 00:19:52.509
it the group? Is it the art group didn't

438
00:19:52.519 --> 00:19:55.059
seem to have much of an effect? But I

439
00:19:55.068 --> 00:19:56.930
can come back to that because we have another paper

440
00:19:56.939 --> 00:19:59.608
in which the my machine works. Uh We in

441
00:19:59.618 --> 00:20:00.348
the same paper with ASHA Alai and, and BR

442
00:20:00.489 --> 00:20:03.809
Mle, we also manipulated whether the people are imagining

443
00:20:03.818 --> 00:20:07.848
to share news from a anonymous personal account, didn't

444
00:20:07.858 --> 00:20:08.750
have any effect on the the patterns of sharing,

445
00:20:08.759 --> 00:20:12.578
still engaging in public but sharing. And towards the

446
00:20:12.588 --> 00:20:15.670
end of the paper, we also tested an intervention

447
00:20:15.680 --> 00:20:18.439
which in which we told participants that essentially they have

448
00:20:18.449 --> 00:20:21.348
a confirmation bias and that, that may potentially bias

449
00:20:21.358 --> 00:20:23.039
, you know, their new sharing decisions. And

450
00:20:23.049 --> 00:20:25.719
the last experiment in which we tell them that they

451
00:20:25.729 --> 00:20:27.529
also have a sharing bias that they have a propensity

452
00:20:27.539 --> 00:20:30.380
to share more stuff that align with their, with

453
00:20:30.390 --> 00:20:33.880
their ideology that tells them what they want to hear

454
00:20:33.559 --> 00:20:37.239
. And neither of those, uh, two last

455
00:20:37.250 --> 00:20:40.420
interventions really had much of an effect. I think

456
00:20:40.430 --> 00:20:42.338
they tended to reduce a little bit overall showing but

457
00:20:42.348 --> 00:20:45.160
small effect, very small effect. It didn't,

458
00:20:45.170 --> 00:20:48.868
uh, reduce people's propensity to share news selectively.

459
00:20:48.880 --> 00:20:51.809
It didn't make people more likely to share incongruent news

460
00:20:51.818 --> 00:20:53.229
and less likely to share congruent news. Ok.

461
00:20:53.239 --> 00:20:55.789
So what it would, what all that suggests,

462
00:20:55.799 --> 00:20:57.828
I think is that selective sharing and its amplification by

463
00:20:57.838 --> 00:21:02.009
moralization and actually extremity is very robust. It's something

464
00:21:02.019 --> 00:21:06.328
that is deeply entrenched in people's cognitive uh dispositions and

465
00:21:06.338 --> 00:21:11.630
that makes sense if um it's underpinned by more motivations

466
00:21:11.640 --> 00:21:12.650
, we know that more motivations are trade off insensitive

467
00:21:12.660 --> 00:21:15.130
, right? If you really care about defending abortion

468
00:21:15.140 --> 00:21:15.848
, if you are, you know, a,

469
00:21:15.858 --> 00:21:19.328
a convinced conservative, why would you, why would

470
00:21:19.338 --> 00:21:22.818
you swayed in your moral in your political fight by

471
00:21:22.828 --> 00:21:23.549
a small, you know, intervention, you know

472
00:21:23.559 --> 00:21:26.809
, sort of experiments, of course, that's unlikely

473
00:21:26.818 --> 00:21:27.189
to have much of an effect because, you know

474
00:21:27.199 --> 00:21:30.500
, we are touching upon deeply entrenched, um you

475
00:21:30.509 --> 00:21:33.670
know, identity relevant moral convictions. And if,

476
00:21:33.680 --> 00:21:37.269
and to the extent that the part is sharing is

477
00:21:37.279 --> 00:21:40.209
driven by sincere convictions about, you know, there

478
00:21:40.219 --> 00:21:44.098
being a threat associated to a given issue or um

479
00:21:44.289 --> 00:21:45.920
beliefs about the fact that the art group is really

480
00:21:45.930 --> 00:21:48.309
evil and really stupid. Well, here again,

481
00:21:48.689 --> 00:21:49.939
it's not a small intervention in a sort of experiment

482
00:21:49.949 --> 00:21:53.029
that is gonna sway that belief that you have acquired

483
00:21:53.039 --> 00:21:56.000
over the years, over, over, you know

484
00:21:56.009 --> 00:21:59.509
, many repeated trials of like consuming partisan media and

485
00:21:59.519 --> 00:22:00.969
conversations with people, etcetera, etcetera. So in

486
00:22:00.979 --> 00:22:03.809
general, it's very hard to change uh people's news

487
00:22:03.818 --> 00:22:07.598
diets, uh intuitive reactions to political messages simply because

488
00:22:07.608 --> 00:22:11.719
they have a long biographical history and they are deeply

489
00:22:11.729 --> 00:22:14.289
entrenched either in people's expectations about the world or their

490
00:22:14.299 --> 00:22:17.269
political commitments and that's hard to modify and surprisingly.

491
00:22:17.809 --> 00:22:19.529
Mhm. So it's, it seems to me that

492
00:22:19.539 --> 00:22:22.549
it's difficult, at least as far as we know

493
00:22:22.559 --> 00:22:27.779
to curb this tendency that most people have to share

494
00:22:27.880 --> 00:22:33.390
strongly partisan news, including sometimes fake news. Right

495
00:22:33.459 --> 00:22:36.759
. Yeah. Yeah. Yeah. Um it is

496
00:22:36.769 --> 00:22:40.568
generally difficult. Um some people like uh Sasha Altai

497
00:22:40.680 --> 00:22:42.789
and Hugo Messi would even argue that, you know

498
00:22:44.799 --> 00:22:47.400
, should we be spending that much uh of our

499
00:22:47.410 --> 00:22:48.729
work and our, you know, money on trying

500
00:22:48.739 --> 00:22:52.818
to come up with like interventions and get misinformation.

501
00:22:52.529 --> 00:22:56.930
Um The spread of misinformation is to begin with not

502
00:22:56.939 --> 00:22:57.549
that big of a problem. At least in the

503
00:22:57.559 --> 00:23:00.549
West, people don't consume that much uh fake news

504
00:23:00.559 --> 00:23:02.848
. You know, most of the fake news are

505
00:23:02.858 --> 00:23:04.269
shared by a tiny little majority of Twitter users and

506
00:23:04.279 --> 00:23:07.828
Facebook and Facebook users does the work for instance of

507
00:23:07.838 --> 00:23:11.250
uh you know, um Matias Osmundsen and Michael Ben

508
00:23:11.318 --> 00:23:12.000
Pearson and others, you know, who have a

509
00:23:12.009 --> 00:23:15.229
American Political Science review that shows that most of the

510
00:23:15.239 --> 00:23:18.439
spreaders of part fake news are actually less than 1%

511
00:23:18.449 --> 00:23:22.000
of the Twitter sample that they could get access to

512
00:23:22.269 --> 00:23:23.088
. And that, that less than 1% is showing

513
00:23:23.098 --> 00:23:26.199
90% of the fake news or something like that.

514
00:23:26.209 --> 00:23:29.719
So it's on the whole, in addition to the

515
00:23:29.729 --> 00:23:32.180
fact that few fake news are being shared in the

516
00:23:32.189 --> 00:23:33.500
, in the sample. Ok. So there's few

517
00:23:33.509 --> 00:23:37.049
, there's little circulation of misinformation overall, you know

518
00:23:37.059 --> 00:23:37.680
, a lot of studies and that that's what they

519
00:23:37.689 --> 00:23:41.699
find. And a very small minority of like pretty

520
00:23:41.709 --> 00:23:42.989
cynical activists seem to be doing the heavy lifting of

521
00:23:44.000 --> 00:23:47.779
sharing that information. So maybe we're exaggerating the seriousness

522
00:23:47.789 --> 00:23:48.750
of misinformation. It doesn't mean that it's not a

523
00:23:48.759 --> 00:23:52.059
problem at all. It is for sure a problem

524
00:23:52.259 --> 00:23:52.529
. It is a problem that, for instance,

525
00:23:52.539 --> 00:23:55.759
you know, people increasingly uh in some areas of

526
00:23:55.769 --> 00:23:57.279
the world are dispersed full of vaccines. It is

527
00:23:57.289 --> 00:24:00.269
a problem that the Russians are spreading, you know

528
00:24:00.400 --> 00:24:03.108
, anti-french propaganda uh in Africa and taking advantage of

529
00:24:03.118 --> 00:24:06.910
that, of those misinformation campaigns to send their Wagner

530
00:24:06.920 --> 00:24:08.239
soldiers and to increase their influence in West Africa.

531
00:24:08.250 --> 00:24:11.818
That's of course a problem. But as far as

532
00:24:11.828 --> 00:24:15.559
like the circulation of misinformation on social media about politics

533
00:24:15.568 --> 00:24:18.259
in the West is concerned is possible that we're exaggerating

534
00:24:18.269 --> 00:24:21.199
a little bit. Uh the seriousness of the problem

535
00:24:21.439 --> 00:24:22.150
. And so what, what people like Sasha Alta

536
00:24:22.439 --> 00:24:23.949
and Yuma would say, and I'm, and I'm

537
00:24:23.959 --> 00:24:26.660
sure they're, they're right if they thought about it

538
00:24:26.670 --> 00:24:27.828
more than I have, what we should instead say

539
00:24:27.838 --> 00:24:30.868
is to try to focus our efforts and our time

540
00:24:30.880 --> 00:24:34.150
and our money on making people care more about trusting

541
00:24:34.279 --> 00:24:37.868
uh reliable sources. OK. Like increasing people's trust

542
00:24:37.880 --> 00:24:41.299
in uh the Washington Post, the New York Times

543
00:24:41.309 --> 00:24:42.630
, Le Monde le Figaro, uh El Pais and

544
00:24:42.640 --> 00:24:45.939
whatnot. OK. So the priority should not be

545
00:24:45.949 --> 00:24:48.439
in trying to fight misinformation. It's hard. It

546
00:24:48.449 --> 00:24:51.410
may, it may even backfire by reinforcing conspiracy theories

547
00:24:51.420 --> 00:24:53.430
and whatnot. The priority should be to try to

548
00:24:53.439 --> 00:24:56.449
restore a little bit trust in institutions and trust in

549
00:24:56.459 --> 00:24:59.289
journalism. And if we, and if we manage

550
00:24:59.299 --> 00:25:03.068
to increase a little bit people's trust in a legitimate

551
00:25:03.078 --> 00:25:04.469
legacy news media, if we manage to make them

552
00:25:04.479 --> 00:25:07.588
consume a bit more reliable information, we would diminish

553
00:25:07.598 --> 00:25:11.309
the amount of false beliefs and misinformation in society much

554
00:25:11.318 --> 00:25:12.838
more than we would if we manage to make people

555
00:25:12.848 --> 00:25:15.828
a bit more distrustful of uh you know, fake

556
00:25:15.838 --> 00:25:17.989
news sources and whatnot. At least that's the argument

557
00:25:18.000 --> 00:25:18.459
that they defend. So I don't know if it

558
00:25:18.469 --> 00:25:21.959
answers a bit what you were saying, but that's

559
00:25:21.969 --> 00:25:25.019
my uh interpretation of uh I mean, what would

560
00:25:25.029 --> 00:25:26.779
be said, what would have to be said here

561
00:25:26.019 --> 00:25:29.098
? Mhm. Yeah. And actually I have uh

562
00:25:29.108 --> 00:25:30.939
seal t on the show two or three years ago

563
00:25:30.949 --> 00:25:34.019
and when it comes to the seriousness of the sharing

564
00:25:34.029 --> 00:25:37.430
of fake news online in social media, particularly,

565
00:25:37.500 --> 00:25:41.469
uh if I remember correctly said basically the same you

566
00:25:41.479 --> 00:25:45.348
said there. And I, I think that uh

567
00:25:45.358 --> 00:25:48.019
you were focusing mostly on people who share fake news

568
00:25:48.029 --> 00:25:52.140
there. But uh it also applies to people who

569
00:25:52.150 --> 00:25:55.880
actually produce fake news and put, put it into

570
00:25:55.890 --> 00:25:59.660
circulation on the internet. It's just a tiny,

571
00:25:59.670 --> 00:26:03.068
tiny, tiny minority of people doing that. Right

572
00:26:03.509 --> 00:26:04.828
. Yeah, I imagine so. I haven't,

573
00:26:04.838 --> 00:26:07.209
I haven't worked on like, you know, the

574
00:26:07.219 --> 00:26:08.858
, the famous producers, like the websites who,

575
00:26:08.868 --> 00:26:11.318
uh, you know, who output fake news online

576
00:26:11.328 --> 00:26:12.459
, of course, they're pursuing, you know,

577
00:26:12.469 --> 00:26:15.118
clicks and, and attention and revenue when they're here

578
00:26:15.130 --> 00:26:17.380
for the, for the money, for the most

579
00:26:17.390 --> 00:26:18.608
part, they might also have, you know,

580
00:26:18.618 --> 00:26:22.400
disruptive motivations, like more political motivations to undermine trust

581
00:26:22.410 --> 00:26:23.660
in the establishment or in journalism or in, you

582
00:26:23.670 --> 00:26:26.709
know, the current politicians or whatnot. Um,

583
00:26:27.598 --> 00:26:30.509
I, I don't know, I don't know what

584
00:26:30.930 --> 00:26:33.049
I mean here again, I think, you know

585
00:26:33.059 --> 00:26:33.250
, the, the, the, the, the

586
00:26:33.259 --> 00:26:37.920
, the proportion of information producers who are intentionally cynical

587
00:26:37.930 --> 00:26:41.199
, intentionally spreading misinformation is extremely small. Uh Of

588
00:26:41.209 --> 00:26:44.650
course, it's absolutely problematic that they exist and there

589
00:26:44.660 --> 00:26:47.170
should be, you know, uh probably penal sanctions

590
00:26:47.180 --> 00:26:48.489
against them and, you know, the, the

591
00:26:48.500 --> 00:26:49.739
government should crack down on them at Zara, but

592
00:26:51.170 --> 00:26:53.969
it's easy to overestimate the gravity of the issue simply

593
00:26:53.979 --> 00:26:59.699
because, you know, um the mind is extremely

594
00:26:59.709 --> 00:27:00.410
uh uh you know, hyper sensitive to, to

595
00:27:00.420 --> 00:27:03.759
threat in general. So that's true of like citizens

596
00:27:03.769 --> 00:27:04.568
. That's true of journalists, that's true of politicians

597
00:27:04.578 --> 00:27:07.640
or researchers. So any information that somehow contributes to

598
00:27:07.650 --> 00:27:10.318
saying that there is something that is not working well

599
00:27:10.328 --> 00:27:11.779
in society will be a bit more, you know

600
00:27:11.789 --> 00:27:15.719
, believed a bit more uh culturally successful uh holding

601
00:27:15.729 --> 00:27:18.469
constant other factors. Of course, I'm not saying

602
00:27:18.479 --> 00:27:21.009
that you can make people believe, you know,

603
00:27:21.019 --> 00:27:22.519
anything and everything, people aren't completely good. But

604
00:27:22.750 --> 00:27:26.868
if the threat seems plausible and if it seems relevant

605
00:27:26.880 --> 00:27:29.608
, uh it's gonna spread far and wide. And

606
00:27:29.618 --> 00:27:32.469
so there's a tendency for the broad public and,

607
00:27:32.479 --> 00:27:33.568
and, and, and, and sentence themselves to

608
00:27:33.799 --> 00:27:37.049
, to think that it, to think that the

609
00:27:37.059 --> 00:27:38.130
, the problem of misinformation is very broad is very

610
00:27:38.140 --> 00:27:41.180
big and they indicate can potentially also have, you

611
00:27:41.189 --> 00:27:45.000
know, a career incentives, publication incentives to also

612
00:27:45.009 --> 00:27:47.229
further exa exaggerate the threat. So I think we

613
00:27:47.239 --> 00:27:48.719
have to keep that in mind. Yeah, like

614
00:27:48.729 --> 00:27:51.500
the mind is super attentive to threat, we easily

615
00:27:51.509 --> 00:27:52.890
exaggerate things. It's relevant, right? Makes us

616
00:27:52.900 --> 00:27:56.598
sound interesting in the conversation and it creates opportunities for

617
00:27:56.608 --> 00:28:00.068
publication for talks for uh you know, a commission

618
00:28:00.078 --> 00:28:03.949
that the parliament or what not. And um and

619
00:28:03.959 --> 00:28:04.229
that will, that will make sense. I mean

620
00:28:04.239 --> 00:28:06.959
, I'm not necessarily, you know, throwing stones

621
00:28:06.969 --> 00:28:08.578
at people who participate in that I have perhaps participated

622
00:28:08.588 --> 00:28:11.348
in that kind of like more panic about misinformation myself

623
00:28:11.358 --> 00:28:14.439
intentionally or not. But we should keep in mind

624
00:28:14.449 --> 00:28:15.779
that. Yeah, the, the, the problem

625
00:28:15.789 --> 00:28:18.809
is probably not as serious as the climate changes or

626
00:28:18.818 --> 00:28:21.078
the, the war in uh in Israel is.

627
00:28:21.729 --> 00:28:23.430
Mhm. So, in a related topic, uh

628
00:28:23.439 --> 00:28:26.368
in our previous conversation, we've also talked a little

629
00:28:26.380 --> 00:28:30.979
bit about conspiracy theories. And in this time,

630
00:28:30.989 --> 00:28:33.019
I would like to ask you about specifically when it

631
00:28:33.029 --> 00:28:38.479
comes to political conspiracy theories and other kinds of rebased

632
00:28:38.979 --> 00:28:45.838
narratives. Are there social functions to holding and disseminating

633
00:28:45.848 --> 00:28:49.519
them? Yeah, presumably at least that's what we

634
00:28:49.529 --> 00:28:52.890
argue with uh Mika Pearson and, and others,

635
00:28:52.900 --> 00:28:55.959
but Mikel Bang, in particular, in, in

636
00:28:55.969 --> 00:28:57.049
at least a couple of pieces that we have written

637
00:28:57.059 --> 00:29:02.618
together. Um So again, the mind is very

638
00:29:02.630 --> 00:29:04.219
, is very susceptible to threatening information, right?

639
00:29:04.229 --> 00:29:06.910
Uh We are a loss of earth. It makes

640
00:29:06.920 --> 00:29:08.358
sense to, you know, um get ourselves ready

641
00:29:08.368 --> 00:29:11.150
for, for threatening a threat. So it's obviously

642
00:29:11.160 --> 00:29:14.920
adaptive to overreact to information that there may be a

643
00:29:14.930 --> 00:29:18.189
social threat um in particular social threats that, you

644
00:29:18.199 --> 00:29:18.939
know, take a group based form, right?

645
00:29:18.949 --> 00:29:22.818
Because uh we humans evolved in an essential environment in

646
00:29:22.828 --> 00:29:25.479
which presumably there was a fair amount of group competition

647
00:29:25.489 --> 00:29:27.509
and group conflict. Um And so it's, it's

648
00:29:27.519 --> 00:29:30.630
adaptive to overreact to be kind of like on the

649
00:29:30.640 --> 00:29:33.709
on the lookout for potential or coordinate social threats directed

650
00:29:33.719 --> 00:29:36.969
against us, directed against our in group are keen

651
00:29:36.979 --> 00:29:41.630
and Sarah and once people have that predisposition to find

652
00:29:41.640 --> 00:29:42.269
, you know, threat based narratives, appealing like

653
00:29:42.279 --> 00:29:47.608
conspiracy theories, um potentially also some popular forms of

654
00:29:47.618 --> 00:29:51.390
like uh Marxism, anti racist discourse, anti-communist discourse

655
00:29:51.400 --> 00:29:53.229
, et cetera, much of ideologies, threat based

656
00:29:53.239 --> 00:29:56.670
narratives, right? I mean, whether you're trying

657
00:29:56.680 --> 00:29:59.699
to, you know, um motivate people to um

658
00:29:59.709 --> 00:30:03.219
take action against communism or whether you're trying to motivate

659
00:30:03.229 --> 00:30:06.680
your population to take actions against the capitalist imperialism and

660
00:30:06.689 --> 00:30:08.068
to endorse communism In all those cases, you're going

661
00:30:08.078 --> 00:30:11.769
to try to spin threat based narratives and to,

662
00:30:11.779 --> 00:30:14.559
you know, uh diffuse them in society through the

663
00:30:14.568 --> 00:30:17.479
journals of propaganda through the channels of general education,

664
00:30:17.489 --> 00:30:18.588
through the, the radio, et cetera. So

665
00:30:18.608 --> 00:30:22.779
threat based narratives are absolutely central in politics and in

666
00:30:22.789 --> 00:30:23.769
, in, in, in ideological phenomena. And

667
00:30:23.779 --> 00:30:27.789
so once you have that ubiquitous with based narratives uh

668
00:30:27.799 --> 00:30:30.009
that, you know, uh the mind responds to

669
00:30:30.019 --> 00:30:34.170
and finds intuitive attention grabbing and uh well pay attention

670
00:30:34.180 --> 00:30:38.789
to it becomes possible to use them instrumentally strategically to

671
00:30:38.799 --> 00:30:42.118
influence how people think of yourself and, and,

672
00:30:42.130 --> 00:30:45.219
and, and, and act. So if I

673
00:30:45.229 --> 00:30:45.250
, if I had a good, you know,

674
00:30:45.354 --> 00:30:48.314
threat based narrative that can get my group to team

675
00:30:48.324 --> 00:30:52.453
up together and to attack or defend against an enemy

676
00:30:52.463 --> 00:30:53.693
group, if I can somehow convince that, you

677
00:30:53.703 --> 00:30:56.834
know, there's a dangerous art group right there in

678
00:30:56.844 --> 00:31:00.213
the neighboring um behind the neighboring hill or in the

679
00:31:00.223 --> 00:31:02.594
neighboring nation that is poi poised to attack us and

680
00:31:02.604 --> 00:31:04.614
who wants to exterminate us. I will easily be

681
00:31:04.625 --> 00:31:07.344
able to reap the benefits of like mobilizing my group

682
00:31:07.354 --> 00:31:08.884
against that art group. OK. So they are

683
00:31:08.894 --> 00:31:12.743
mobilizing benefits in, in using instrumentally threads narratives.

684
00:31:14.035 --> 00:31:17.309
And we suspect that at least sometimes that's how PC

685
00:31:17.318 --> 00:31:18.949
theories are being used. Uh People use them to

686
00:31:18.959 --> 00:31:22.799
kind of like recruit um new individuals to join a

687
00:31:22.809 --> 00:31:26.680
political fight, a political cause that they care about

688
00:31:26.689 --> 00:31:29.858
. For instance, you know, anti-establishment uh uh

689
00:31:30.430 --> 00:31:33.118
narrative or, or some kind of like a more

690
00:31:33.130 --> 00:31:36.078
racist narrative against uh you know, the neighboring tribe

691
00:31:36.088 --> 00:31:37.209
that you, that you may be living close to

692
00:31:37.219 --> 00:31:40.779
in like the Middle East or in Africa or whatnot

693
00:31:41.719 --> 00:31:41.680
. And so, and so, yeah, it

694
00:31:41.689 --> 00:31:45.289
becomes possible to, to kind of like instrumentally agitate

695
00:31:45.299 --> 00:31:48.479
a sense of threat in other people to mobilize them

696
00:31:48.489 --> 00:31:51.789
to side with you and to take action with you

697
00:31:51.799 --> 00:31:53.170
on your side and to potentially uh yeah, organize

698
00:31:53.229 --> 00:31:56.420
collectively to, to, to thwart a potential threat

699
00:31:56.299 --> 00:32:00.400
. Um And, and there's at least another potential

700
00:32:00.410 --> 00:32:02.779
um social use that can be made of uh of

701
00:32:02.789 --> 00:32:05.838
this narrative that is also very instrumental that it is

702
00:32:05.848 --> 00:32:07.299
also a little exploitative, which is a signaling,

703
00:32:07.309 --> 00:32:09.078
of course, you know, by, by saying

704
00:32:09.088 --> 00:32:12.358
how much you hate an art group, by saying

705
00:32:12.900 --> 00:32:15.479
how much you think that the art group is uh

706
00:32:15.489 --> 00:32:17.430
dangerous, uh you know, uh conspiring against you

707
00:32:17.439 --> 00:32:21.969
with Zara. Zara, you can express a sense

708
00:32:21.979 --> 00:32:22.500
of loyalty to your own group, you can,

709
00:32:22.509 --> 00:32:24.858
you can by, by see who you hated society

710
00:32:25.150 --> 00:32:27.578
, you're potentially sort of like, you know,

711
00:32:27.588 --> 00:32:30.818
distinguishing yourself from a food that in group and also

712
00:32:30.828 --> 00:32:32.608
potentially neutral audiences who don't care about it and you're

713
00:32:32.618 --> 00:32:35.930
singling you, you're signing. So you're expressing who

714
00:32:35.939 --> 00:32:37.239
you're sending with. And that can be uh that

715
00:32:37.250 --> 00:32:40.809
can be useful to um that can be useful to

716
00:32:40.838 --> 00:32:44.489
uh to, to express to potential allies to potential

717
00:32:44.500 --> 00:32:45.269
recruits, you know, that you're standing with them

718
00:32:45.279 --> 00:32:49.078
, that you care about the interests itself. Um

719
00:32:49.910 --> 00:32:54.009
As you mentioned, one important thing about the mobilization

720
00:32:54.019 --> 00:32:58.189
argument, which is that this, there's kind of

721
00:32:58.199 --> 00:33:00.279
a debate in the country, sciences, of culture

722
00:33:00.289 --> 00:33:04.689
and political science and what not about the extent to

723
00:33:04.699 --> 00:33:07.209
which those um threat based narratives really persuade people or

724
00:33:07.219 --> 00:33:09.578
really motivate people to act so on, on a

725
00:33:09.588 --> 00:33:13.769
sort of like optimistic accounts when you spread, you

726
00:33:13.779 --> 00:33:15.880
know, threat based rumor or conspiracy theory that seems

727
00:33:15.890 --> 00:33:16.838
to be believable or, you know, a cons

728
00:33:16.848 --> 00:33:21.318
spiritual narrative or like a sort of like um ethnic

729
00:33:21.328 --> 00:33:22.259
rumor about the fact that, you know, the

730
00:33:22.269 --> 00:33:25.150
neighboring Tutsi are gonna want to massacre all your uh

731
00:33:25.618 --> 00:33:30.160
um Hutu fellows or whatnot under an optimistic argument,

732
00:33:30.500 --> 00:33:35.019
uh an optimistic view of the argument, those claims

733
00:33:35.029 --> 00:33:37.328
will persuade and will really get people to act because

734
00:33:37.338 --> 00:33:39.979
they're afraid of threat, which we really have motivational

735
00:33:39.989 --> 00:33:44.670
impact. I think sometimes it's the case. Um

736
00:33:44.680 --> 00:33:46.229
but people like Michael B, Peterson and maybe also

737
00:33:46.239 --> 00:33:49.900
Hugo Mercier have argued and I'm pretty convinced by that

738
00:33:49.920 --> 00:33:52.608
, by that argument that very often people are not

739
00:33:52.618 --> 00:33:55.269
so much convinced, you know, prompted to act

740
00:33:55.279 --> 00:33:59.509
and to take arms against the threat as they are

741
00:33:59.519 --> 00:34:01.969
more using the information as a way of like signaling

742
00:34:01.979 --> 00:34:05.279
that they are willing to attack, that they're ready

743
00:34:05.289 --> 00:34:07.858
to attack already. And they're using the propaganda,

744
00:34:07.989 --> 00:34:09.168
the threat based rumor, the narrative as a way

745
00:34:09.179 --> 00:34:12.030
of like saying, hey, I'm one of those

746
00:34:12.039 --> 00:34:14.389
guys who hate the other group. I'm willing to

747
00:34:14.398 --> 00:34:15.510
take action. And I'm saying that to you in

748
00:34:15.519 --> 00:34:19.559
order to signal that I'm ready to coordinate with you

749
00:34:19.570 --> 00:34:22.418
in a potential collective action that we might want to

750
00:34:22.429 --> 00:34:24.340
take against the other group. OK. So under

751
00:34:24.349 --> 00:34:27.239
a sort of like strong form of the claim,

752
00:34:27.677 --> 00:34:30.509
the negative information really persuades people to act under,

753
00:34:30.518 --> 00:34:34.358
under a broader exception of the, of the argument

754
00:34:34.367 --> 00:34:36.728
or, or the thesis, it doesn't really prompt

755
00:34:36.739 --> 00:34:38.528
people to act, but it helps people who are

756
00:34:38.539 --> 00:34:42.918
already motivated to act for other reasons, to organize

757
00:34:42.927 --> 00:34:45.778
collectivity, to coordinate and to prepare a collective action

758
00:34:45.887 --> 00:34:46.438
that may potentially, you know, target the other

759
00:34:46.447 --> 00:34:52.478
group. Yeah. Uh And if that second um

760
00:34:52.489 --> 00:34:54.478
interpretation of it is the correct one. Could it

761
00:34:54.489 --> 00:34:59.148
, could it be that in this case uh conspiracy

762
00:34:59.168 --> 00:35:01.349
, a political conspiracy theory or any other sort of

763
00:35:01.358 --> 00:35:06.878
threat based narrative could function more as a sort of

764
00:35:06.958 --> 00:35:13.918
justification to act than really uh as motivating people to

765
00:35:13.938 --> 00:35:16.090
act upon it. Yeah. Yeah, exactly.

766
00:35:16.148 --> 00:35:21.599
Um It could, it could serve, it could

767
00:35:21.610 --> 00:35:25.260
allow individuals to rationalize an already existing motivation to,

768
00:35:25.269 --> 00:35:29.269
you know, organize collective action and to attack or

769
00:35:29.280 --> 00:35:32.099
to defend against the art group. Um Stereotypes,

770
00:35:32.110 --> 00:35:34.829
you know, are not necessarily always super, super

771
00:35:34.840 --> 00:35:37.949
believable. Uh Conspiracy theories are not always super super

772
00:35:37.958 --> 00:35:42.340
believable, but just the fact that they are superficially

773
00:35:42.349 --> 00:35:46.228
plausible or that they um can be at least,

774
00:35:46.239 --> 00:35:50.329
you know, superficially endorsed by your allies may suffice

775
00:35:50.340 --> 00:35:52.789
for them to function as like postdoc rationalizations of an

776
00:35:52.800 --> 00:35:55.429
already existing disposition to hate the art group and to

777
00:35:55.438 --> 00:35:58.418
want to attack against the other group. Yes.

778
00:35:58.438 --> 00:36:00.269
Yes. And, and by, by, by

779
00:36:00.280 --> 00:36:01.550
, by, by, by expressing endorsement of the

780
00:36:01.559 --> 00:36:04.648
conspiracy theory or the, the, the ethnic rumor

781
00:36:04.659 --> 00:36:06.079
or whatnot, but just saying that you believe it

782
00:36:06.090 --> 00:36:08.179
or by disseminating it on yourself, you also signal

783
00:36:08.188 --> 00:36:10.628
that you're one of those individuals who want to act

784
00:36:10.639 --> 00:36:13.469
. So there's also a signaling value, at least

785
00:36:13.719 --> 00:36:15.929
it contributes to co-ordination of collective action. So it

786
00:36:15.938 --> 00:36:20.239
signals a membership, it gets you closer to a

787
00:36:20.250 --> 00:36:22.010
coordinate action that is meaningful in terms of collective action

788
00:36:22.019 --> 00:36:23.929
that may be hostile to the art group. And

789
00:36:23.938 --> 00:36:27.429
you're right, it can also uh rationalize under under

790
00:36:27.599 --> 00:36:30.458
the existing motivation to, to take action. Uh

791
00:36:30.469 --> 00:36:32.030
And uh I mean, just before we move to

792
00:36:32.239 --> 00:36:36.079
, we move to another topic, uh you mentioned

793
00:36:36.090 --> 00:36:38.300
there, the fact that it's still debatable to what

794
00:36:38.309 --> 00:36:45.610
extent conspiracy theories might have actual behavioral effects. I

795
00:36:45.619 --> 00:36:49.958
mean, in terms of mobilizing people to do something

796
00:36:50.119 --> 00:36:52.260
uh that also apply. I mean, this is

797
00:36:52.269 --> 00:36:55.579
an ongoing debate also that applies to things like the

798
00:36:55.590 --> 00:37:01.175
spread of misinformation, political ads during campaigns, online

799
00:37:01.184 --> 00:37:02.414
, et cetera. Right? I mean, we're

800
00:37:02.425 --> 00:37:07.043
still not sure to what extent stuff like that,

801
00:37:07.054 --> 00:37:10.054
particularly on social media and the internet more generally really

802
00:37:10.063 --> 00:37:15.715
translating to actual offline behavior. Right. Yeah,

803
00:37:15.724 --> 00:37:19.644
and, and offline behavior as well. Um I

804
00:37:19.655 --> 00:37:21.750
mean, here I can be, I can be

805
00:37:21.760 --> 00:37:22.769
, again, the echo of, you know,

806
00:37:22.780 --> 00:37:23.590
the work of Hugo me here for a second because

807
00:37:23.599 --> 00:37:25.590
I think it's very uh it's very convincing even if

808
00:37:25.599 --> 00:37:28.769
I sometimes think that Hugo is going a tad too

809
00:37:28.780 --> 00:37:30.199
far and his optimism about human rationality. But I

810
00:37:30.208 --> 00:37:32.219
think the, the general arguments are pretty, pretty

811
00:37:32.228 --> 00:37:36.688
strong, pretty valid. People are not easily gullible

812
00:37:36.699 --> 00:37:39.110
, they are not easily um fooled, they have

813
00:37:39.119 --> 00:37:43.668
intuitive capacities to gauge the coherence and consistency of new

814
00:37:43.679 --> 00:37:45.769
information with what they already believe. And they're pretty

815
00:37:45.780 --> 00:37:50.039
good at gauging whether sources of information and messages have

816
00:37:50.050 --> 00:37:52.239
their interests at heart or are trying to deceive them

817
00:37:52.250 --> 00:37:52.570
and whatnot. So that's what he calls, you

818
00:37:52.579 --> 00:37:54.289
know, epistemic vigilance or in a in a more

819
00:37:54.300 --> 00:37:58.550
novel formulation, open open visions. So I think

820
00:37:58.559 --> 00:38:00.889
that's very true. Of course, um you won't

821
00:38:00.898 --> 00:38:06.769
easily um you know, make Germans become anti Semitic

822
00:38:06.780 --> 00:38:09.958
if there's not an already pretty strong pre-existing disposition for

823
00:38:09.969 --> 00:38:13.510
anti-semitism in German culture that is, you know,

824
00:38:13.519 --> 00:38:15.708
plunging in roots um, in, you know,

825
00:38:15.719 --> 00:38:19.438
several centuries of, of, of, uh Catholic

826
00:38:19.449 --> 00:38:22.360
uh and, and partisan history, you won't easily

827
00:38:22.369 --> 00:38:23.909
get people to buy products that they absolutely don't need

828
00:38:23.918 --> 00:38:28.369
. You won't create helo um, new desires.

829
00:38:28.949 --> 00:38:30.628
Uh That's, I think actually, by the way

830
00:38:30.639 --> 00:38:31.128
, if you, if you allow me to,

831
00:38:31.280 --> 00:38:34.590
um hit a little bit on the standard of social

832
00:38:34.599 --> 00:38:37.039
scientists who are not cognitive and who are not evolutionary

833
00:38:37.250 --> 00:38:37.599
, I think that's often like a sort of like

834
00:38:37.610 --> 00:38:39.958
a misconception that a lot of people in traditional sociology

835
00:38:39.969 --> 00:38:44.728
or whatnot may have is that they think that capitalism

836
00:38:44.739 --> 00:38:50.228
advertisement propaganda, um it's have the capacity to generate

837
00:38:50.260 --> 00:38:52.719
non practicing desires. I think that there, that's

838
00:38:52.728 --> 00:38:57.159
, that's generally misconception, human nature is about the

839
00:38:57.168 --> 00:39:00.300
same for everybody and it's pretty deeply structured by a

840
00:39:00.699 --> 00:39:04.659
transaction pressures. You won't easily persuade people to desire

841
00:39:04.668 --> 00:39:07.449
commodities or activities that they are not evolution be predisposed

842
00:39:07.458 --> 00:39:09.599
to like. Right. So in general, what

843
00:39:09.610 --> 00:39:13.418
advertisement does is that it will exploit and obey the

844
00:39:13.429 --> 00:39:15.750
existing predisposition for, I don't know, um junk

845
00:39:15.760 --> 00:39:19.050
food, you know, fat stuff, sugary stuff

846
00:39:19.059 --> 00:39:22.478
. It will explode to pre uh predi predisposition to

847
00:39:22.489 --> 00:39:23.188
, you know, want to have sex and to

848
00:39:23.199 --> 00:39:25.099
um want to see, you know, attractive sexual

849
00:39:25.110 --> 00:39:28.570
partners, et Zara and he's going to present products

850
00:39:28.728 --> 00:39:34.418
or services that tap those pre-existing desires. Uh And

851
00:39:34.429 --> 00:39:36.769
, and if it manages to sell more new services

852
00:39:36.780 --> 00:39:39.208
, new goods. It's because it's making products that

853
00:39:39.219 --> 00:39:42.708
type, that tap those already existing desires, more

854
00:39:42.719 --> 00:39:44.918
salient, more visible it for them, but it's

855
00:39:44.929 --> 00:39:47.168
not creating desires from nothing. Ethnic yellow at all

856
00:39:47.179 --> 00:39:50.610
, I think. Mhm. Uh, yeah.

857
00:39:50.619 --> 00:39:52.128
I don't know if it's, if it's answering your

858
00:39:52.139 --> 00:39:53.418
, your question in any way. No, no

859
00:39:53.478 --> 00:39:55.119
. Maybe, maybe a little, if you,

860
00:39:55.340 --> 00:39:58.219
maybe a little element that I think I should add

861
00:39:58.228 --> 00:40:00.978
about like the, the the potency of like propaganda

862
00:40:00.989 --> 00:40:02.110
and, and, and rumors and you know,

863
00:40:02.489 --> 00:40:06.079
like kind of like discourses that aim to persuade and

864
00:40:06.090 --> 00:40:09.099
to mobilize people. Um An argument that Hugo would

865
00:40:09.110 --> 00:40:12.978
make in this case and that I'm increasingly convinced by

866
00:40:12.989 --> 00:40:17.250
is that um very often when people say that the

867
00:40:17.260 --> 00:40:20.590
endorsement theories or that they endorse, you know,

868
00:40:20.599 --> 00:40:22.208
ideologies like Marxism or like a, a pretty radical

869
00:40:22.219 --> 00:40:27.228
ideology very often what is likely to be happening in

870
00:40:27.239 --> 00:40:30.208
their heads is that people claim to be endorsing the

871
00:40:30.219 --> 00:40:31.579
, the, the statement of the belief they ascend

872
00:40:31.590 --> 00:40:34.269
to it. They say that they believe it,

873
00:40:34.449 --> 00:40:37.250
but in fact, deep down in their and conscious

874
00:40:37.500 --> 00:40:39.074
other systems are, are like uh not really,

875
00:40:39.083 --> 00:40:42.875
you know, like they're not really taking the information

876
00:40:42.885 --> 00:40:45.644
super seriously to the extent of motivating costly action,

877
00:40:45.773 --> 00:40:49.753
costly action, big efforts on the part of the

878
00:40:49.764 --> 00:40:52.394
individual. So very often people will say that they

879
00:40:52.405 --> 00:40:55.264
endorse fashionable beliefs because they allow them to, you

880
00:40:55.273 --> 00:40:58.514
know, seem informed, to seem like they care

881
00:40:58.523 --> 00:41:00.719
about certain values that other people care about. Um

882
00:41:01.159 --> 00:41:05.019
, they're gonna endorse them to, um, to

883
00:41:05.030 --> 00:41:07.719
seem like, yeah, trustworthy, competent to seem

884
00:41:07.728 --> 00:41:08.699
like they care about politics, etcetera. But in

885
00:41:08.708 --> 00:41:12.889
fact, they very rarely will deviate from their,

886
00:41:12.898 --> 00:41:14.780
you know, personal comfort or like, you know

887
00:41:14.789 --> 00:41:15.918
, everyday ways of doing things and, you know

888
00:41:15.929 --> 00:41:20.668
, private little self interest. Um, on the

889
00:41:20.679 --> 00:41:22.389
basis of those belies very often, the, the

890
00:41:22.398 --> 00:41:23.659
, the epistemic visions mechanisms are, it's not that

891
00:41:23.668 --> 00:41:25.869
they are like turning down the information on the endorsement

892
00:41:25.878 --> 00:41:28.280
of the narrative, but it's more like they are

893
00:41:28.590 --> 00:41:30.418
keeping it in a sort of like isolated co format

894
00:41:30.429 --> 00:41:34.148
which, you know, me be would call reflective

895
00:41:34.539 --> 00:41:37.139
without really trying to connect them with like costly actions

896
00:41:37.148 --> 00:41:42.519
that would potentially entail sacrifices for the individuals. And

897
00:41:42.530 --> 00:41:44.708
uh and also big efforts in terms of like mobilizing

898
00:41:44.719 --> 00:41:45.840
others and really, you know, like spending money

899
00:41:45.849 --> 00:41:50.188
and time on like uh political calls that people claim

900
00:41:50.199 --> 00:41:52.050
to care about. But in fact, don't really

901
00:41:52.099 --> 00:41:53.599
do much to it in general. And, and

902
00:41:53.610 --> 00:41:55.800
by the way, I'm completely, I'm completely guilty

903
00:41:55.809 --> 00:41:58.860
of the same problem myself. OK? Like I'm

904
00:41:58.869 --> 00:42:00.878
giving a bit of money every month to uh to

905
00:42:00.889 --> 00:42:02.070
give well, and I'm trying to do a few

906
00:42:02.079 --> 00:42:06.610
, like not so costly altruistic stuff. Uh But

907
00:42:06.619 --> 00:42:09.579
I really take costly action in favor of more values

908
00:42:09.590 --> 00:42:12.010
that, that, that I claim that I claim

909
00:42:12.019 --> 00:42:14.280
to care about. Uh I claim to care about

910
00:42:14.289 --> 00:42:15.159
the environment, but I'm still playing way too much

911
00:42:15.168 --> 00:42:16.809
and still eating a bit, a bit too much

912
00:42:16.820 --> 00:42:20.250
chicken. And so I'm, I'm completely guilty of

913
00:42:20.260 --> 00:42:22.219
the same mechanism that I'm describing here. Uh But

914
00:42:22.228 --> 00:42:24.489
so I, if that's true, then in this

915
00:42:24.500 --> 00:42:30.728
particular case, uh people manifesting certain specific beliefs would

916
00:42:30.739 --> 00:42:35.079
, would serve mainly a social signaling function, right

917
00:42:35.090 --> 00:42:37.610
? In terms of signaling, their, I guess

918
00:42:37.619 --> 00:42:44.510
, group affiliations, yeah, signaling. Um So

919
00:42:44.519 --> 00:42:45.050
I think, you know, what's happening very often

920
00:42:45.059 --> 00:42:51.340
is that we have mechanisms in the mind that are

921
00:42:51.349 --> 00:42:54.019
, that evolve for tax benefits that were essentially relevant

922
00:42:54.030 --> 00:42:57.769
, you know, getting your coalition to co-operate together

923
00:42:57.780 --> 00:43:00.969
to solve a problem or throw out an enemy to

924
00:43:00.978 --> 00:43:02.699
signal your trustworthiness to be kept and admitted as a

925
00:43:02.708 --> 00:43:06.458
group as a good group member and whatnot and those

926
00:43:06.469 --> 00:43:09.500
motivations. So those many divisions would have expressed themselves

927
00:43:09.510 --> 00:43:13.610
with like intensity in the past when there was like

928
00:43:13.619 --> 00:43:15.550
, like a lot of fitness interdependence. And when

929
00:43:15.559 --> 00:43:16.438
there was like a lot of like external threat on

930
00:43:16.449 --> 00:43:19.760
the group or whatnot, there are people would have

931
00:43:19.769 --> 00:43:21.309
, you know, spent a lot of energy to

932
00:43:21.590 --> 00:43:22.949
um get their group to team up together, or

933
00:43:22.958 --> 00:43:25.110
they would have spent a lot of energy to signal

934
00:43:25.119 --> 00:43:29.550
their devotion to the group by incurring pretty costly sacrifices

935
00:43:29.559 --> 00:43:30.849
to deserve their place in the group at Zara because

936
00:43:30.860 --> 00:43:34.478
back at the time, the environment was very threatening

937
00:43:34.500 --> 00:43:36.519
. But nowadays, typically, most of us would

938
00:43:36.530 --> 00:43:37.869
live in you know, relatively peaceful western societies in

939
00:43:37.878 --> 00:43:39.570
which there is no war, there is no famine

940
00:43:39.579 --> 00:43:43.119
, there are no uh natural catastrophes and whatnot.

941
00:43:43.128 --> 00:43:45.208
And so we are not in dire need to mobilize

942
00:43:45.219 --> 00:43:49.179
other people in collective action or we do, we

943
00:43:49.188 --> 00:43:50.728
are not as much in dire need to signal group

944
00:43:50.739 --> 00:43:52.369
memberships as, as we, as we used to

945
00:43:52.378 --> 00:43:55.119
potentially. And so we're gonna, we're gonna still

946
00:43:55.128 --> 00:43:58.559
have those mechanisms opening in our, in the background

947
00:43:58.570 --> 00:44:00.579
of our heads. We're still gonna be doing a

948
00:44:00.590 --> 00:44:01.389
little bit of, of like political and, and

949
00:44:01.398 --> 00:44:05.829
more signaling but not with the same intensity, not

950
00:44:05.840 --> 00:44:07.398
at the price of like as costly actions as maybe

951
00:44:07.409 --> 00:44:10.489
we would have in intra context in, in context

952
00:44:10.500 --> 00:44:14.289
of like group conflict or in or with as much

953
00:44:14.300 --> 00:44:16.280
intensity as we would in like contemporary context that are

954
00:44:16.289 --> 00:44:19.478
prone to war or where, you know, people

955
00:44:19.489 --> 00:44:21.719
are a bit more in danger. So I would

956
00:44:21.728 --> 00:44:23.780
say that the motivations are there uh But they are

957
00:44:23.789 --> 00:44:28.128
only operating in a sort of like weak diminished form

958
00:44:28.139 --> 00:44:30.239
in people's heads. But still, it's enough that

959
00:44:30.250 --> 00:44:31.179
they're operating in a sort of like, you know

960
00:44:31.469 --> 00:44:36.260
, uh minimal mode for the, for the belief

961
00:44:36.269 --> 00:44:37.860
that people, for people to endorse beliefs and to

962
00:44:37.869 --> 00:44:42.659
disseminate claims in ways that, you know, seem

963
00:44:42.668 --> 00:44:44.610
to, yeah, that, that seem that still

964
00:44:44.619 --> 00:44:46.769
seem to fulfill signaling functions and, and potentially uh

965
00:44:46.780 --> 00:44:52.530
mobilization functions. So changing topics, you have a

966
00:44:52.539 --> 00:45:00.590
paper where you explore the ways by which strongly motivate

967
00:45:00.599 --> 00:45:05.708
people that are strongly morally motivated or committed to gender

968
00:45:05.949 --> 00:45:10.949
equality in this specific case process and understand evidence regarding

969
00:45:12.079 --> 00:45:15.958
gender bias hiring processes. So, uh and the

970
00:45:15.969 --> 00:45:21.228
and of course, there are benefits to being committed

971
00:45:21.239 --> 00:45:23.570
to gender equality in this specific case. But in

972
00:45:23.579 --> 00:45:28.070
the paper, you also talk about potential costs.

973
00:45:28.079 --> 00:45:30.269
Could you explain what's going on here? Yeah,

974
00:45:30.739 --> 00:45:32.800
it's all very simple. So we have that uh

975
00:45:32.809 --> 00:45:36.668
we have that pre print paper that uh got rejected

976
00:45:36.679 --> 00:45:37.239
from many places, but I hope it will one

977
00:45:37.250 --> 00:45:40.159
day be accepted by a decent social psychology journal uh

978
00:45:40.168 --> 00:45:45.878
with um co authors um uh Hu Ling Xiao,

979
00:45:45.050 --> 00:45:47.329
who's a, who's a Chinese colleague and Andre Mle

980
00:45:47.559 --> 00:45:51.469
, my former PH advisor in which we look at

981
00:45:51.478 --> 00:45:57.418
like how people consume um simplified scientific accounts, uh

982
00:45:57.429 --> 00:46:00.329
simplified, you know, research summaries about the topic

983
00:46:00.340 --> 00:46:01.938
of gender bias in hiring in organizations and doctor in

984
00:46:01.949 --> 00:46:06.469
stem. And we look at how the degree of

985
00:46:06.478 --> 00:46:07.530
like feminism, if you will the de the degree

986
00:46:07.539 --> 00:46:13.369
to which they moralize gender equality influences those evaluations,

987
00:46:13.378 --> 00:46:15.250
the degree to which they think those scientific uh reports

988
00:46:15.260 --> 00:46:17.978
are credible, the methods are rigorous, you know

989
00:46:17.989 --> 00:46:21.889
, the the findings are convincing sr itself. And

990
00:46:21.898 --> 00:46:22.260
so there's, there's a, there's a, there's

991
00:46:22.269 --> 00:46:24.679
a PNS paper that came out in 2015, I

992
00:46:24.688 --> 00:46:29.019
think by Henry at a that showed that essentially when

993
00:46:29.030 --> 00:46:31.688
people were presented with uh scientific demonstrations using, you

994
00:46:31.699 --> 00:46:36.030
know, randomized control trials or like rigorous experimental methods

995
00:46:36.228 --> 00:46:39.329
. When people were exposed to those uh proofs that

996
00:46:39.340 --> 00:46:45.739
um having processes in academia favor uh men over women

997
00:46:45.978 --> 00:46:49.458
with equal credentials, they found that men were less

998
00:46:49.469 --> 00:46:52.050
receptive to that uh type of scientific result that women

999
00:46:52.059 --> 00:46:54.769
were more likely to believe them and to want to

1000
00:46:54.780 --> 00:46:58.760
take action against, against those uh against those findings

1001
00:46:58.769 --> 00:47:00.409
. OK. So they found that there was some

1002
00:47:00.418 --> 00:47:01.769
kind of like a sex difference in how receptive or

1003
00:47:01.780 --> 00:47:07.010
how um how truthful people were at that demonstration.

1004
00:47:07.159 --> 00:47:09.059
And so here, what we tried to do was

1005
00:47:09.070 --> 00:47:13.860
to was to see first is that sex effect,

1006
00:47:13.869 --> 00:47:15.559
video, sex effect or is it more like an

1007
00:47:15.449 --> 00:47:20.059
moral ideology effect that is associated or co varying with

1008
00:47:20.070 --> 00:47:22.389
the sex difference? Is it possible that the sex

1009
00:47:22.398 --> 00:47:25.418
difference is in fact reducible to differences in moral commitments

1010
00:47:25.429 --> 00:47:29.119
in the degree to which people care about sex equality

1011
00:47:29.409 --> 00:47:30.500
um in our, in our, you know,

1012
00:47:30.510 --> 00:47:34.449
uh population of participants. And yes, it seems

1013
00:47:34.458 --> 00:47:35.909
to be the, it seems to be the case

1014
00:47:35.918 --> 00:47:37.978
when we control for more commitment very often the sex

1015
00:47:37.989 --> 00:47:42.478
effect disappears. And when we add more commitment as

1016
00:47:42.489 --> 00:47:44.619
a covariance of people, the variations of the degree

1017
00:47:44.628 --> 00:47:45.719
to which they trust the science they think is believable

1018
00:47:46.438 --> 00:47:49.769
. Um We find that the more people care about

1019
00:47:49.780 --> 00:47:51.659
more equality, the more they will trust, the

1020
00:47:51.668 --> 00:47:55.398
more they will deem plausible uh convincing those scientific reports

1021
00:47:55.409 --> 00:48:00.458
that demonstrate experimentally that uh women are being discriminated against

1022
00:48:00.469 --> 00:48:01.889
in having processes in academia. So that's, that's

1023
00:48:01.898 --> 00:48:04.889
a bright side. Ok. So we, we

1024
00:48:04.898 --> 00:48:07.179
, we, we expand already existing study. We

1025
00:48:07.188 --> 00:48:08.208
showed that what seemed to be a sex effect is

1026
00:48:08.219 --> 00:48:12.228
actually maybe more like a moral ideology effect. And

1027
00:48:12.239 --> 00:48:14.369
we find that good news, people who care more

1028
00:48:14.378 --> 00:48:19.329
about equality will uh welcome more with more receptivity,

1029
00:48:19.340 --> 00:48:22.219
those rigorous demonstrations that we may not be discriminated against

1030
00:48:22.300 --> 00:48:24.139
. However, on the flip side, we also

1031
00:48:24.148 --> 00:48:28.030
find that people who are more uh more committed to

1032
00:48:28.039 --> 00:48:30.239
equality, uh see as more persuading, as more

1033
00:48:30.250 --> 00:48:36.070
persuasive, sorry. Um more bogus demonstrations or conclusions

1034
00:48:36.079 --> 00:48:37.139
that there is gender bias in having taking place.

1035
00:48:37.148 --> 00:48:39.119
So here, what we did is that we substituted

1036
00:48:39.128 --> 00:48:44.219
the rigorous uh experimental demonstration of gender bias in hiring

1037
00:48:44.228 --> 00:48:49.199
with a more observational and fishy study that essentially looks

1038
00:48:49.208 --> 00:48:52.668
at the pre hiring um proportion of men and women

1039
00:48:52.679 --> 00:48:54.849
in stem and the post hiring proportions. And that

1040
00:48:54.860 --> 00:48:58.699
finds that in fact, the post hiring proportions of

1041
00:48:58.708 --> 00:49:00.500
women is greater than the pre hiring proportions. So

1042
00:49:00.510 --> 00:49:02.668
clearly, there's no gender bias in having against women

1043
00:49:04.458 --> 00:49:06.059
. And and, and we, and we visualize

1044
00:49:06.070 --> 00:49:07.438
that graphically with like pretty clear about shots and like

1045
00:49:07.449 --> 00:49:09.500
there's no ambiguity about the fact that women are really

1046
00:49:09.510 --> 00:49:13.168
favored in having process. And yet the research Army

1047
00:49:13.228 --> 00:49:16.869
concludes that the fact that women on the whole,

1048
00:49:16.878 --> 00:49:20.360
even after hiring are still a minority because they,

1049
00:49:20.369 --> 00:49:22.800
they are a minority before and after hiring, the

1050
00:49:22.809 --> 00:49:24.329
fact that women are a minority in science in the

1051
00:49:24.340 --> 00:49:28.519
scientific labs is a rigorous demonstration of the fact that

1052
00:49:28.530 --> 00:49:30.260
, you know, there's gender bias in hiring.

1053
00:49:30.050 --> 00:49:32.789
So that's a fac conclusion because people infer from the

1054
00:49:32.800 --> 00:49:36.989
fact that women are less generous, the notion that

1055
00:49:37.000 --> 00:49:38.530
there should be inevitably uh you know, gender-based in

1056
00:49:38.539 --> 00:49:40.809
having against them, even when the presented data is

1057
00:49:40.820 --> 00:49:44.309
clearly showing that women all things equal, actually favored

1058
00:49:44.320 --> 00:49:46.300
in the recruiting recruitment process. OK. And so

1059
00:49:46.309 --> 00:49:52.619
here we find that people um generally tend to do

1060
00:49:52.628 --> 00:49:54.469
not process the contribution between the result and the conclusion

1061
00:49:54.829 --> 00:49:58.510
and the higher and more commitment to the equality,

1062
00:49:58.860 --> 00:50:00.789
the more people are likely to rate that bogus scientific

1063
00:50:00.809 --> 00:50:04.639
conclusion as being, you know, convincing. OK

1064
00:50:04.809 --> 00:50:07.800
. So overall, in summary, if you care

1065
00:50:07.809 --> 00:50:08.628
, it seems that if you care more about gender

1066
00:50:08.639 --> 00:50:10.860
equality, if you're more of a feminist, you

1067
00:50:10.869 --> 00:50:13.969
will. And that's a and that's a good thing

1068
00:50:14.090 --> 00:50:19.039
rate more positively believe more rigorous demonstration of their being

1069
00:50:19.050 --> 00:50:20.949
uh gender bias against women. But you will also

1070
00:50:20.958 --> 00:50:23.418
be more receptive and more believing of bogus demonstrations that

1071
00:50:23.429 --> 00:50:28.119
only rely on observational evidence and that are even,

1072
00:50:28.128 --> 00:50:30.059
you know, showing results that are contradictory with the

1073
00:50:30.070 --> 00:50:32.280
conclusion. So this of course, does not,

1074
00:50:32.289 --> 00:50:35.800
you know, amount to uh saying that we should

1075
00:50:35.809 --> 00:50:37.199
stop being uh caring about gender equality. We should

1076
00:50:37.208 --> 00:50:40.820
, of course uh uh keep caring about that sort

1077
00:50:40.829 --> 00:50:43.989
of like moral and political targets, a very important

1078
00:50:44.000 --> 00:50:45.199
one. But the lesson that we are trying to

1079
00:50:45.208 --> 00:50:47.719
teach here and we're not the first ones to be

1080
00:50:47.728 --> 00:50:52.429
making that argument is that um when you are very

1081
00:50:52.438 --> 00:50:55.800
committed to a cause, it can potentially blend you

1082
00:50:55.809 --> 00:50:58.208
a little bit, you know, to the details

1083
00:50:58.219 --> 00:51:00.059
of a reasoning, it can make you a bit

1084
00:51:00.070 --> 00:51:02.898
more susceptible to confirmation bias or um or belief bias

1085
00:51:02.909 --> 00:51:06.128
in the sense that if you really endorse a conclusion

1086
00:51:06.139 --> 00:51:07.418
, if you think that the conclusion is true,

1087
00:51:07.429 --> 00:51:08.389
if you think that the conclusion is more desirable,

1088
00:51:08.760 --> 00:51:10.840
it makes you a bit more likely to neglect the

1089
00:51:10.849 --> 00:51:14.878
inferential steps or the reasoning that back it up and

1090
00:51:14.889 --> 00:51:15.610
that lead to it. And that's what we're trying

1091
00:51:15.619 --> 00:51:19.139
to show essentially, you know, morality binds and

1092
00:51:19.148 --> 00:51:20.949
blinds. As Jonathan, he would say it,

1093
00:51:21.438 --> 00:51:22.769
it can make people a bit more irrational uh at

1094
00:51:22.780 --> 00:51:25.159
the margins, even even if you know, the

1095
00:51:25.168 --> 00:51:29.188
aggregate social effects are on average positive, at least

1096
00:51:29.199 --> 00:51:30.889
sometimes it's gonna bias people's reasoning and we want to

1097
00:51:30.898 --> 00:51:35.849
show that. Mhm. So when it comes to

1098
00:51:35.860 --> 00:51:38.708
these ideological commitments that some people have, uh you've

1099
00:51:38.719 --> 00:51:45.139
also been working on ideological orthodoxy and specifically on the

1100
00:51:45.148 --> 00:51:50.389
repression of free speech. So what do we know

1101
00:51:50.398 --> 00:51:54.570
about what might motivate and what might be the goals

1102
00:51:54.579 --> 00:52:00.500
that a strong political activists try to pursue when repressing

1103
00:52:00.510 --> 00:52:05.059
free speech? OK. So here you're referring to

1104
00:52:05.070 --> 00:52:08.079
my work with uh Mika Van Heeren um recent work

1105
00:52:08.090 --> 00:52:09.599
, you know, just published or still in the

1106
00:52:09.610 --> 00:52:12.878
pipes at Sara. So not much is is out

1107
00:52:12.889 --> 00:52:15.938
yet. We have um we have a we have

1108
00:52:15.949 --> 00:52:17.989
a commentary article on, on David Pino and our

1109
00:52:19.530 --> 00:52:22.300
uh the alliance theory of ideology that, that our

1110
00:52:22.309 --> 00:52:25.148
commentary is called speech prepress and outrage from orthodox activists

1111
00:52:25.159 --> 00:52:29.590
as attempts at fascinating mobilizations and getting service among allies

1112
00:52:29.688 --> 00:52:30.250
. I'm sorry, the title is a little long

1113
00:52:30.260 --> 00:52:32.199
but you know, your, your viewers are welcome

1114
00:52:32.208 --> 00:52:35.320
to check it out. And we have also like

1115
00:52:35.329 --> 00:52:37.750
a longer piece in preparation that is currently called the

1116
00:52:37.760 --> 00:52:39.739
Ky Foundations of Orthodoxy in which we try to expand

1117
00:52:39.750 --> 00:52:42.849
a bit those arguments in a somewhat more detailed form

1118
00:52:43.708 --> 00:52:45.929
. So, yeah, Michael and I have taken

1119
00:52:45.938 --> 00:52:50.168
the phenomenon of like political dogmatism and speech repression as

1120
00:52:50.179 --> 00:52:52.539
our sort of like object of inquiry. Recently,

1121
00:52:52.550 --> 00:52:55.909
we've become interested in that um for various reasons.

1122
00:52:55.918 --> 00:52:58.728
Uh One because we think that, you know,

1123
00:52:58.739 --> 00:53:00.869
there's a bit too much repression of speech in the

1124
00:53:00.878 --> 00:53:05.250
world uh nowadays still obviously also because um we think

1125
00:53:05.260 --> 00:53:07.510
that there's a bit too much of it in uh

1126
00:53:07.519 --> 00:53:07.639
in, in the west, in, in the

1127
00:53:07.648 --> 00:53:09.188
US, in particular, there's a somewhat, you

1128
00:53:09.199 --> 00:53:13.139
know, concerning phenomenon of cancel culture that is threatening

1129
00:53:13.199 --> 00:53:15.099
uh universities and intellectual professions that we think is pretty

1130
00:53:15.110 --> 00:53:17.199
problematic. And of course, because, you know

1131
00:53:17.208 --> 00:53:21.208
, speech is still massively repressed in authoritarian countries uh

1132
00:53:21.219 --> 00:53:22.489
the world over and we, and we told,

1133
00:53:22.500 --> 00:53:24.519
we told ourselves that, well, it seems like

1134
00:53:24.530 --> 00:53:29.349
there's not really a, a mature cognitive and evolutionary

1135
00:53:29.360 --> 00:53:30.708
theory of like, why people are orthodox or we

1136
00:53:30.719 --> 00:53:34.809
press free speech in ideological movements, you know,

1137
00:53:34.820 --> 00:53:37.659
be they moral movements, religious movements, political movements

1138
00:53:37.668 --> 00:53:39.179
and whatnot. And so we, we told ourselves

1139
00:53:39.188 --> 00:53:40.809
, well, let's try to develop a theory of

1140
00:53:40.820 --> 00:53:43.978
like why there is, you know, the anti-communist

1141
00:53:43.989 --> 00:53:45.070
witch hunt in the fifties. Why is there,

1142
00:53:45.239 --> 00:53:47.559
uh you know, the repression and the surveillance of

1143
00:53:47.570 --> 00:53:51.478
political dissent in China right now? Why did the

1144
00:53:51.489 --> 00:53:52.458
Catholic church, you know, persecute the heretics in

1145
00:53:52.469 --> 00:53:54.429
the middle ages and why is there a cancel culture

1146
00:53:54.438 --> 00:53:55.820
in the US right now? Both on the left

1147
00:53:55.829 --> 00:53:59.719
and on the right. So that's a bit explanatory

1148
00:53:59.728 --> 00:54:01.239
target if I, if I may say and so

1149
00:54:01.250 --> 00:54:05.110
what we say is essentially pretty similar to, you

1150
00:54:05.119 --> 00:54:07.579
know, what we've been saying about the conspiracy theories

1151
00:54:07.010 --> 00:54:08.550
and what, you know, people like cosine and

1152
00:54:08.559 --> 00:54:10.648
tube and, and, and other authors have been

1153
00:54:10.659 --> 00:54:13.878
saying about, you know, um more beliefs and

1154
00:54:13.889 --> 00:54:15.898
political beliefs, essentially, we are proposing that people

1155
00:54:16.050 --> 00:54:20.668
re speech for two main reasons. Uh Mobilization goals

1156
00:54:20.679 --> 00:54:22.750
and signaling goals. So the mobilization goal would be

1157
00:54:22.760 --> 00:54:25.478
fulfilled in the following way. If you care a

1158
00:54:25.489 --> 00:54:29.550
lot about the issue, uh you will want to

1159
00:54:29.889 --> 00:54:31.050
, you know, uh rally your life and to

1160
00:54:31.059 --> 00:54:32.869
motivate your life to, you know, create a

1161
00:54:32.878 --> 00:54:36.219
coalition, create, create a sort of like group

1162
00:54:36.228 --> 00:54:38.429
uh behind yourself with yourself to tackle the threat,

1163
00:54:38.438 --> 00:54:40.800
right? You might want to vanquish uh racism in

1164
00:54:40.809 --> 00:54:43.469
the US. For instance, you might want to

1165
00:54:43.478 --> 00:54:45.989
, um, you know, um thwart the threat

1166
00:54:45.000 --> 00:54:47.769
of communist infiltration in the US in the 19 fifties

1167
00:54:47.780 --> 00:54:50.628
if you're like a, like a ma artist or

1168
00:54:50.639 --> 00:54:52.550
what not. And so there's a sort of like

1169
00:54:52.789 --> 00:54:57.050
intrinsic desire or need to organize collective action if you

1170
00:54:57.059 --> 00:54:58.789
want to advance a cult or if you want to

1171
00:54:58.800 --> 00:55:01.079
defeat an enemy, be it also like an intellectual

1172
00:55:01.090 --> 00:55:02.438
enemy, it doesn't have to be a human out

1173
00:55:02.449 --> 00:55:04.978
group. It can be like dangerous ideas that you

1174
00:55:04.989 --> 00:55:07.719
think are exerting, you know, a corruptive influence

1175
00:55:07.728 --> 00:55:09.030
on your society or whatnot. And so in those

1176
00:55:09.039 --> 00:55:13.699
conditions, that goal of mobilization becomes to try to

1177
00:55:13.929 --> 00:55:16.860
influence people's beliefs so that they believe that uh the

1178
00:55:16.869 --> 00:55:20.878
mobilization is, is justified, is necessary. And

1179
00:55:20.889 --> 00:55:22.789
typically, one way of doing that is to tell

1180
00:55:22.800 --> 00:55:23.739
people that there's a threat because if there's a threat

1181
00:55:23.750 --> 00:55:25.550
, you know, it's gonna activate our evolved sense

1182
00:55:25.559 --> 00:55:29.329
of like threat, avoidance. It's very powerful from

1183
00:55:29.340 --> 00:55:31.148
a motivational perspective. And so often you can motivate

1184
00:55:31.159 --> 00:55:36.030
people to gang together behind you to organize for your

1185
00:55:36.039 --> 00:55:37.369
side if you can convince them that they are facing

1186
00:55:37.378 --> 00:55:39.659
a threat. And in that context, what we're

1187
00:55:39.668 --> 00:55:43.898
saying is that people activists, political leaders, ideological

1188
00:55:43.909 --> 00:55:45.260
entrepreneurs, et cetera, et cetera, they will

1189
00:55:45.269 --> 00:55:49.579
try to, they will tend to try to regulate

1190
00:55:49.590 --> 00:55:52.579
free speech and to maybe repress free speech when they

1191
00:55:52.590 --> 00:55:54.340
think that the speech in question is potentially demobilizing when

1192
00:55:54.349 --> 00:55:58.909
, when they think that potentially the information is contrarian

1193
00:55:58.918 --> 00:56:00.739
or is kind of like calling into question the legitimacy

1194
00:56:00.750 --> 00:56:04.739
of the fight or it contributes to nuance or procession

1195
00:56:04.750 --> 00:56:07.099
of the threat or contributes to uh say that the

1196
00:56:07.110 --> 00:56:08.659
threat is not that bad after all or what not

1197
00:56:08.760 --> 00:56:10.789
, you know what I mean? So like uh

1198
00:56:10.800 --> 00:56:13.898
if I take the example of, you know,

1199
00:56:13.909 --> 00:56:16.708
the the canceling of controversial speakers on us campuses uh

1200
00:56:16.719 --> 00:56:20.860
recently by the left, but also by the right

1201
00:56:21.239 --> 00:56:22.539
, if you're a staunch anti racist, you may

1202
00:56:22.550 --> 00:56:27.099
want to deplatform or to cancel the invitation or to

1203
00:56:27.110 --> 00:56:30.449
stop um the, the, the talk from taking

1204
00:56:30.458 --> 00:56:34.188
place of a biologist who's doing research on, let's

1205
00:56:34.199 --> 00:56:36.128
say the link between genetics and, and, and

1206
00:56:36.139 --> 00:56:40.128
personality or someone who's investigating potential, you know,

1207
00:56:40.478 --> 00:56:45.260
psychological differences or IQ differences between populations and whatnot because

1208
00:56:45.030 --> 00:56:47.110
rightfully or wrongfully, I don't know, it's,

1209
00:56:47.119 --> 00:56:50.860
it's an open question. You might think that those

1210
00:56:50.869 --> 00:56:52.530
pieces of information in, if it's sif research,

1211
00:56:52.829 --> 00:56:57.398
those pieces of information may contribute to normalize, to

1212
00:56:57.409 --> 00:57:00.860
justify uh racism, not necessarily directly but even indirectly

1213
00:57:00.869 --> 00:57:04.449
as you know, making like uh maybe a biological

1214
00:57:04.458 --> 00:57:06.719
conception of human nature a bit more popular or a

1215
00:57:06.728 --> 00:57:07.728
bit more plausible and whatnot. So you might,

1216
00:57:07.739 --> 00:57:10.208
you might be motivated to be press speech that you

1217
00:57:10.219 --> 00:57:15.599
think is contrarian or susceptible to undermine uh the third

1218
00:57:15.610 --> 00:57:17.610
need for the necessity of the mobilization against racism.

1219
00:57:19.128 --> 00:57:21.530
And in particular, you will try to repress speech

1220
00:57:21.539 --> 00:57:23.340
that you think is undermining people's perception of threat,

1221
00:57:23.349 --> 00:57:27.070
right? For instance, you know, research also

1222
00:57:27.079 --> 00:57:29.929
that may downplay the notion that there is a link

1223
00:57:29.938 --> 00:57:31.289
between, you know, um the race of the

1224
00:57:31.300 --> 00:57:34.989
defendant and police violence, for instance, will typically

1225
00:57:35.000 --> 00:57:37.519
be seen as being contrarian as being dangerous by anti

1226
00:57:37.530 --> 00:57:42.699
racist activists because they fear that its dissemination in society

1227
00:57:42.708 --> 00:57:45.260
may influence people's prior beliefs towards them. Believing that

1228
00:57:45.269 --> 00:57:47.378
the, the the the problem of racism is not

1229
00:57:47.389 --> 00:57:49.860
that bad or that, you know, it's it's

1230
00:57:49.869 --> 00:57:52.260
exaggerated or whatnot and that may have demobilizing effects and

1231
00:57:52.269 --> 00:57:54.668
the same like on the right people may want to

1232
00:57:54.679 --> 00:57:59.164
repress free speech of um the free expression of,

1233
00:57:59.175 --> 00:58:00.764
for instance, you know, um critical race theory

1234
00:58:00.804 --> 00:58:02.344
or, you know, uh the notion that,

1235
00:58:02.353 --> 00:58:05.375
you know, gender roles are at least partly socially

1236
00:58:05.385 --> 00:58:10.034
constructed because they feel like dissemination of those contracting messages

1237
00:58:10.215 --> 00:58:15.898
may kind of like diminish or undermine the the enthusiasm

1238
00:58:15.909 --> 00:58:17.309
that people have for traditional Christian values, et cetera

1239
00:58:17.320 --> 00:58:19.829
. And they see that as a threat. Ok

1240
00:58:20.090 --> 00:58:22.059
. So one core function we think of speech repression

1241
00:58:22.070 --> 00:58:25.228
and speech regulation is you try to control people's beliefs

1242
00:58:25.369 --> 00:58:30.269
by intervening on information flows so that people stay motivated

1243
00:58:30.280 --> 00:58:34.418
for causes that you think will bring about shared benefits

1244
00:58:34.429 --> 00:58:36.510
in particular against, you know, social threats.

1245
00:58:37.340 --> 00:58:39.728
And of course, once there are those efforts to

1246
00:58:39.739 --> 00:58:45.039
mobilize allies for causes and against enemies that creates individual

1247
00:58:45.050 --> 00:58:46.780
incentives for people to signal devotion to certain causes to

1248
00:58:46.789 --> 00:58:50.719
those causes and to the group. So once people

1249
00:58:50.728 --> 00:58:52.719
care vastly about anti-racism, once people have a moral

1250
00:58:52.728 --> 00:58:54.760
panic about communist influence in the US, in the

1251
00:58:54.769 --> 00:58:58.648
, in the fifties, once conservative America is going

1252
00:58:58.659 --> 00:59:00.769
through a more panic about critical race theory being taught

1253
00:59:00.780 --> 00:59:06.139
at university or at high school that creates individual opportunities

1254
00:59:06.148 --> 00:59:08.469
for repetition and enhancement. Uh It creates incentives for

1255
00:59:08.478 --> 00:59:14.179
individuals to seem like they're gonna be um very tough

1256
00:59:14.188 --> 00:59:15.829
with uh you know, uh control on ideas with

1257
00:59:15.840 --> 00:59:19.239
like ideas that they are in group, dislike as

1258
00:59:19.250 --> 00:59:21.878
a way of showing their degree of commitment for those

1259
00:59:21.889 --> 00:59:23.860
ideas. Um sorry for the group. Uh and

1260
00:59:23.869 --> 00:59:24.809
, and, and for the causes that they are

1261
00:59:24.820 --> 00:59:28.280
trying to advance, like, if I'm, let's

1262
00:59:28.289 --> 00:59:30.510
say a po position in 19 fifties like Maar and

1263
00:59:30.519 --> 00:59:32.909
there it is and is a sort of like more

1264
00:59:32.918 --> 00:59:37.128
panic against uh about the influence of communism in American

1265
00:59:37.139 --> 00:59:39.188
culture and institutions. Well, I may see it

1266
00:59:39.199 --> 00:59:43.668
as an opportunity to try to erect, to emerge

1267
00:59:43.849 --> 00:59:46.090
myself as a tough politician, as a reliable politician

1268
00:59:46.099 --> 00:59:49.938
who will defend, you know, the traditional values

1269
00:59:49.949 --> 00:59:52.378
of corporate liberal America against the threat of communism.

1270
00:59:52.398 --> 00:59:58.128
And so he quits um by repressing speech um of

1271
00:59:58.139 --> 00:60:01.030
people who are suspected of Soviet sympathies. Someone like

1272
00:60:01.168 --> 00:60:05.639
Mac mccalley can emerge as someone who's committed to protect

1273
00:60:05.648 --> 00:60:07.199
regional America against the threat. And so, yeah

1274
00:60:07.519 --> 00:60:08.719
, you, you, you know, you appear

1275
00:60:08.728 --> 00:60:12.050
tough against the contractions, you appear tough against the

1276
00:60:12.059 --> 00:60:14.659
enemies against people who disagree with you. And that

1277
00:60:14.668 --> 00:60:19.148
can potentially allow you to score reputational points. Ok

1278
00:60:19.159 --> 00:60:21.090
. So, uh, uh, yeah, II

1279
00:60:21.099 --> 00:60:23.000
, I have one more question or topic to explore

1280
00:60:23.010 --> 00:60:25.769
about political activists but, uh, still on the

1281
00:60:25.780 --> 00:60:29.958
topic of repressing free speech. I mean, I've

1282
00:60:29.969 --> 00:60:32.250
asked you a similar kind of question when earlier we

1283
00:60:32.260 --> 00:60:37.510
talked about uh spreading uh fake news and all of

1284
00:60:37.519 --> 00:60:45.070
that and uh how basically uh people care more about

1285
00:60:45.079 --> 00:60:49.949
or a apparently care more about the values and symbols

1286
00:60:49.958 --> 00:60:54.289
driving political decisions than uh the concrete effects they have

1287
00:60:54.300 --> 00:60:58.458
on society. Uh I mean, in this particular

1288
00:60:58.469 --> 00:61:01.739
case, are you also interested in trying to understand

1289
00:61:01.750 --> 00:61:06.369
perhaps if there would be any ways we could try

1290
00:61:06.378 --> 00:61:09.728
to curb the repression of free speech or is that

1291
00:61:09.739 --> 00:61:15.719
not something that you are really studying? Uh I

1292
00:61:15.728 --> 00:61:16.989
mean, I'm obviously interested in that question because,

1293
00:61:17.000 --> 00:61:19.369
you know, the, the, the very fact

1294
00:61:19.378 --> 00:61:21.389
that, that I think that there's a need for

1295
00:61:21.398 --> 00:61:24.239
a g theory of free speech. Um Is it

1296
00:61:24.250 --> 00:61:27.059
still a reflection of the fact that I think that

1297
00:61:27.070 --> 00:61:28.739
, you know, the repression of free speech is

1298
00:61:28.750 --> 00:61:30.829
at least sometimes bad even if sometimes it's also desirable

1299
00:61:31.478 --> 00:61:34.398
. Um I think it's a good thing that for

1300
00:61:34.418 --> 00:61:35.780
instance, in Europe, we're trying to, you

1301
00:61:35.789 --> 00:61:37.840
know, fight uh neo Nazi propaganda by making it

1302
00:61:37.849 --> 00:61:40.619
illegal to have a swat seekers and uh and to

1303
00:61:40.628 --> 00:61:43.378
, and to sing a neo Nazi chants in the

1304
00:61:43.389 --> 00:61:45.688
streets. Um So there are cases where, I

1305
00:61:45.699 --> 00:61:46.809
mean, I'm not, I'm not, I'm not

1306
00:61:46.820 --> 00:61:50.050
a free speech. Absolutely alone. Even if I'm

1307
00:61:50.059 --> 00:61:53.050
very pro free speech, what can we do to

1308
00:61:53.168 --> 00:61:59.079
reinforce or to strengthen free speech? Um I don't

1309
00:61:59.090 --> 00:62:00.418
know, Ricardo, I don't really know. I

1310
00:62:00.429 --> 00:62:01.739
haven't really studied that, uh that question yet.

1311
00:62:02.300 --> 00:62:08.300
Um I guess, I guess, I guess decreasing

1312
00:62:08.309 --> 00:62:12.079
the level of like polarization of society and trying to

1313
00:62:12.090 --> 00:62:15.599
make our national societies a little less conflictual will help

1314
00:62:15.139 --> 00:62:19.760
because um as we argue in our work with Michael

1315
00:62:20.128 --> 00:62:23.269
, um it seems pretty clear that motivations, we

1316
00:62:23.280 --> 00:62:25.898
press free speech co vary and are sort of like

1317
00:62:25.909 --> 00:62:30.938
upregulated, increased by perceptions of society, be conflict

1318
00:62:30.949 --> 00:62:32.539
ridden, you know. Mhm. When you think

1319
00:62:32.550 --> 00:62:35.309
that there's a dangerous out group that is threatening your

1320
00:62:35.320 --> 00:62:37.250
in group, which is like a, like a

1321
00:62:37.260 --> 00:62:38.260
, like a by definition, your perception of conflict

1322
00:62:38.958 --> 00:62:43.378
, um you will feel an increased need to um

1323
00:62:43.389 --> 00:62:45.378
motivate my group to form like a cohesive coalition or

1324
00:62:45.389 --> 00:62:47.639
to, you know, rally new life to,

1325
00:62:47.648 --> 00:62:52.059
to, to fight that threat. And um and

1326
00:62:52.070 --> 00:62:53.128
it's to that extent that people is to the extent

1327
00:62:53.139 --> 00:62:55.989
that people feel that they have benefits in mobilizing,

1328
00:62:57.000 --> 00:62:59.110
that they will feel that they also have benefits in

1329
00:62:59.119 --> 00:63:00.820
repressing speech that they think could be demobilizing, right

1330
00:63:01.119 --> 00:63:06.320
? So the more society is harmonious uh and the

1331
00:63:06.329 --> 00:63:07.500
more, you know, the different subgroups that compose

1332
00:63:07.510 --> 00:63:10.438
society can live their lives fairly freely. And the

1333
00:63:10.449 --> 00:63:14.199
more uh the, the less inequality maybe there is

1334
00:63:14.208 --> 00:63:17.099
and the less effective polarization there is, the less

1335
00:63:17.110 --> 00:63:21.579
it should be a problem. Um The, the

1336
00:63:21.590 --> 00:63:23.820
, the less you should have motivations from activists on

1337
00:63:23.829 --> 00:63:25.619
, on either side to want to be press and

1338
00:63:25.628 --> 00:63:29.519
to police the speech of others because yeah, again

1339
00:63:29.530 --> 00:63:32.679
, like um motivation to repress speech are impossible to

1340
00:63:32.688 --> 00:63:37.570
dissociate from threat, perceptions and motivations to, to

1341
00:63:37.579 --> 00:63:40.159
motivate your own group to do stuff in the world

1342
00:63:40.168 --> 00:63:43.309
and often what motivates. Um Yeah, that,

1343
00:63:43.320 --> 00:63:46.090
that, that mobilization effort is perception of threat.

1344
00:63:46.099 --> 00:63:47.728
So I would say that, yeah, you have

1345
00:63:47.739 --> 00:63:50.280
to make societies a bit more peaceful and a little

1346
00:63:50.289 --> 00:63:52.789
less polarized as for more. So that's of course

1347
00:63:52.800 --> 00:63:54.228
, very difficult, right? Uh It's a sort

1348
00:63:54.239 --> 00:63:58.309
of like, you know, um basic fundamental uh

1349
00:63:58.320 --> 00:64:01.030
program for policymakers and uh you know, it involves

1350
00:64:01.039 --> 00:64:02.269
, you know, maybe uh changing a bit the

1351
00:64:02.280 --> 00:64:05.360
economy, it involves changing. Uh maybe the way

1352
00:64:05.369 --> 00:64:08.489
the the news media are functioning to make them a

1353
00:64:08.500 --> 00:64:11.148
little less uh to make them uh to make them

1354
00:64:11.159 --> 00:64:13.250
a little less polarizing, a little less partisan.

1355
00:64:13.688 --> 00:64:15.849
It calls for a number of reforms in many,

1356
00:64:15.860 --> 00:64:19.510
many different sectors of society. Uh I'm not here

1357
00:64:19.519 --> 00:64:21.769
to make, you know, specific policy recommendations.

1358
00:64:21.780 --> 00:64:24.429
That's something that I hope to maybe think a bit

1359
00:64:24.438 --> 00:64:27.250
more about in the future Michael, uh, might

1360
00:64:27.260 --> 00:64:29.239
have recommendations to that effect. I think he's giving

1361
00:64:29.250 --> 00:64:30.949
a talk about the future of free speech in,

1362
00:64:30.958 --> 00:64:33.668
um, in Sweden, uh, early December.

1363
00:64:33.679 --> 00:64:35.869
That may be of interest to some of your viewers

1364
00:64:36.168 --> 00:64:38.478
. Uh, well, it will probably be too

1365
00:64:38.489 --> 00:64:40.159
late by the time your podcast is out. But

1366
00:64:40.478 --> 00:64:42.780
, yeah, a lot of people who are more

1367
00:64:42.789 --> 00:64:44.829
or less influential, are interested in how to foor

1368
00:64:45.019 --> 00:64:47.389
free speech. Uh, I think, um Jacob

1369
00:64:47.820 --> 00:64:50.780
, um what is his name? Uh Meena who

1370
00:64:50.789 --> 00:64:55.309
wrote like a History of Free Speech, um probably

1371
00:64:55.320 --> 00:64:58.679
also has pretty interesting positive recommendations about that. He's

1372
00:64:58.688 --> 00:65:00.360
got a very interesting book called um Free Speech or

1373
00:65:00.369 --> 00:65:03.619
History from Socrates to uh to I forget what that

1374
00:65:03.628 --> 00:65:05.438
is really good and that I can recommend your viewers

1375
00:65:05.449 --> 00:65:09.128
to, to check out. Uh But yeah,

1376
00:65:09.139 --> 00:65:11.539
I'm not, I'm not gonna venture into like specific

1377
00:65:11.550 --> 00:65:13.619
, you know, policy recommendations in terms of like

1378
00:65:13.628 --> 00:65:15.869
what to do to enforce free speech. Um I

1379
00:65:15.878 --> 00:65:18.148
don't really know but, but for sure that value

1380
00:65:18.159 --> 00:65:21.349
needs to be protected and uh and it's a cardinal

1381
00:65:21.360 --> 00:65:25.449
value of, of us um scientists and I assume

1382
00:65:25.458 --> 00:65:27.208
, you know, most regional liberals of, of

1383
00:65:27.219 --> 00:65:28.938
, of, of, of the West right now

1384
00:65:29.280 --> 00:65:30.320
? Great. So, uh would you like to

1385
00:65:30.329 --> 00:65:34.909
tell people just before we go the kinds of work

1386
00:65:34.918 --> 00:65:38.590
you're doing on right now, what kinds of topics

1387
00:65:38.599 --> 00:65:40.148
you're working on? And, and by the way

1388
00:65:40.159 --> 00:65:42.510
, I didn't mention this at the beginning in our

1389
00:65:42.570 --> 00:65:45.539
in, in the introduction. But in the near

1390
00:65:45.550 --> 00:65:47.429
future, you will also be moving to one stick

1391
00:65:47.438 --> 00:65:50.208
to Jean Nico. Yeah, that's it. Yeah

1392
00:65:50.219 --> 00:65:53.398
. Yeah. Yeah. So my, my contract

1393
00:65:53.550 --> 00:65:56.699
at university is reaching its end in um end of

1394
00:65:56.708 --> 00:66:00.458
March 24 and I have a new job at the

1395
00:66:00.708 --> 00:66:02.360
Envision Nico in Paris. Uh I'll be part of

1396
00:66:02.369 --> 00:66:06.760
the um evolution and the social team. So evolution

1397
00:66:06.780 --> 00:66:09.989
and social cognition and my P I is going to

1398
00:66:10.000 --> 00:66:12.679
be Olivia Mohan, who's a cultural evolutionist, who's

1399
00:66:12.688 --> 00:66:15.918
mostly been uh been doing work on like uh the

1400
00:66:15.929 --> 00:66:18.878
cult evolution of uh of, of uh scriptural systems

1401
00:66:18.889 --> 00:66:24.320
of languages. But he's interested in cultural conservatism.

1402
00:66:24.329 --> 00:66:27.378
Why do people want to prefer the status quo in

1403
00:66:27.389 --> 00:66:29.679
many cases with respect to like, you know,

1404
00:66:29.688 --> 00:66:33.280
preserving uh cultural traditions, preferring um the status quo

1405
00:66:33.289 --> 00:66:35.599
in, in between group relationships in politics, etcetera

1406
00:66:35.978 --> 00:66:38.010
. And so, yeah, we're going to try

1407
00:66:38.019 --> 00:66:40.148
to explore like what, what motivations people may have

1408
00:66:40.159 --> 00:66:41.849
in, in, in, in preferring the status

1409
00:66:41.860 --> 00:66:43.688
quo. And so I'll be back in various and

1410
00:66:43.699 --> 00:66:45.789
working from France, studying from uh yeah, the

1411
00:66:45.800 --> 00:66:47.800
spring 24 and as, as well as my current

1412
00:66:47.809 --> 00:66:49.679
work. Well, I would say that many of

1413
00:66:49.688 --> 00:66:51.208
the papers that have been talk uh that I've been

1414
00:66:51.219 --> 00:66:54.909
talking about today are not necessarily published yet. Uh

1415
00:66:54.918 --> 00:66:57.128
So I'm still like working on them a little bit

1416
00:66:57.360 --> 00:66:59.878
. Uh And in particular, what occupies most of

1417
00:66:59.889 --> 00:67:01.800
my time right now is our work with Michael Mikel

1418
00:67:01.878 --> 00:67:05.530
Bang um about orthodoxy and speech repression. I'm trying

1419
00:67:05.539 --> 00:67:09.250
to write a relatively long theory piece about, about

1420
00:67:09.260 --> 00:67:12.019
that and the social functions of mobilizations and, and

1421
00:67:12.030 --> 00:67:14.860
, and signaling that we think it fulfills. Um

1422
00:67:14.869 --> 00:67:15.659
And so that's my main piece of work right now

1423
00:67:15.668 --> 00:67:18.050
. Yes. Uh And by the way, where

1424
00:67:18.059 --> 00:67:21.369
can people find you on the internet? Uh They

1425
00:67:21.378 --> 00:67:24.938
can find me on Twitter. Uh Of course,

1426
00:67:24.949 --> 00:67:30.340
my handle is um a capital A uh underscore Marie

1427
00:67:30.349 --> 00:67:35.039
um Ma Rie underscore S CIA Marie. I like

1428
00:67:35.050 --> 00:67:39.918
science and also have a website that I need to

1429
00:67:39.929 --> 00:67:42.750
um that maybe you, you would be able to

1430
00:67:42.760 --> 00:67:45.079
uh to cite another video, I guess in some

1431
00:67:45.090 --> 00:67:47.059
websites. Um What I and you can shoot me

1432
00:67:47.070 --> 00:67:49.139
an email at Antoine dot Marie dot sc I at

1433
00:67:49.148 --> 00:67:53.050
gmail.com if they also want to. Of course,

1434
00:67:53.059 --> 00:67:55.398
and I'm on Facebook and you know, find the

1435
00:67:55.409 --> 00:67:58.639
ball through many channels. Great. So look Antoine

1436
00:67:58.648 --> 00:68:00.208
. Thank you so much for coming on the show

1437
00:68:00.219 --> 00:68:02.010
again, as I said at the beginning of the

1438
00:68:02.119 --> 00:68:03.610
show, it was great. Fun to talk to

1439
00:68:03.619 --> 00:68:05.728
you. Thanks a lot. Have a good day

1440
00:68:05.739 --> 00:68:09.559
. See you. Hi guys. Thank you for

1441
00:68:09.570 --> 00:68:11.559
watching this interview. Until the end. If you

1442
00:68:11.570 --> 00:68:13.789
liked it, please share it. Leave a like

1443
00:68:13.800 --> 00:68:15.399
and hit the subscription button. The show is brought

1444
00:68:15.409 --> 00:68:17.708
to you by the N Lights learning and development.

1445
00:68:17.720 --> 00:68:21.520
Then differently check the website at N lights.com and also

1446
00:68:21.529 --> 00:68:26.689
please consider supporting the show on Patreon or paypal.

1447
00:68:26.759 --> 00:68:28.859
I would also like to give a huge thank you

1448
00:68:28.869 --> 00:68:31.890
to my main patrons and Paypal supporters, Perera Larson

1449
00:68:31.899 --> 00:68:34.619
, Jerry Muller and Frederick Suno, Bernard Seche O

1450
00:68:34.770 --> 00:68:38.180
of Alex Adam, Castle Matthew Whitten Bear. No

1451
00:68:38.250 --> 00:68:41.279
wolf, Tim Ho Erica LJ Connors, Philip Forrest

1452
00:68:41.289 --> 00:68:44.248
Connolly. Then the Met Robert Wine in NAI Z

1453
00:68:44.688 --> 00:68:47.889
Mar Nevs calling in Hobel Governor Mikel Stormer Samuel Andre

1454
00:68:47.969 --> 00:68:53.337
Francis for Agns Ferger and H her me and Lain

1455
00:68:53.538 --> 00:68:56.627
Jung Y and the Samuel K Hes Mark Smith J

1456
00:68:57.519 --> 00:69:00.198
Tom Hummel S Friends, David Sloan Wilson, ya

1457
00:69:00.608 --> 00:69:04.609
de ro ro Diego, Jan Punter Romani Charlotte,

1458
00:69:04.619 --> 00:69:09.640
Bli Nico Barba, Adam Hunt Pavlo Stassi Nale medicine

1459
00:69:09.649 --> 00:69:12.899
, Gary G Alman, Sam Ofri and YPJ Barboa

1460
00:69:13.319 --> 00:69:16.359
, Julian Price Edward Hall, Eden Broner Douglas Fry

1461
00:69:16.369 --> 00:69:25.199
Franca Beto Lati Cortez Solis Scott Zachary ftdw Daniel Friedman

1462
00:69:25.208 --> 00:69:27.939
, William Buckner, Paul Giorgio, Luke Loki,

1463
00:69:28.418 --> 00:69:31.720
Georgio Theophano. Chris Williams and Peter Wo David Williams

1464
00:69:31.729 --> 00:69:35.189
Di Costa Anton Erickson Charles Murray, Alex Shaw,

1465
00:69:35.289 --> 00:69:41.189
Marie Martinez, Coralie Chevalier, Bangalore Larry Dey junior

1466
00:69:41.220 --> 00:69:44.998
, Old Ebon, Starry Michael Bailey. Then Spur

1467
00:69:45.007 --> 00:69:47.608
by Robert Grassy Zorn. Jeff mcmahon, Jake Zul

1468
00:69:47.618 --> 00:69:51.748
Barnabas Radis Mark Kemple Thomas Dvor Luke Neeson, Chris

1469
00:69:51.757 --> 00:69:56.167
to Kimberley Johnson, Benjamin Gilbert, Jessica, Nicky

1470
00:69:56.479 --> 00:70:00.770
Linda Brendan Nicholas Carlson Ismael Bensley Man George Katis,

1471
00:70:00.779 --> 00:70:04.529
Valentine Steinman, Perros, Kate Von Goler, Alexander

1472
00:70:05.509 --> 00:70:12.279
Albert Liam Dan Biar Masoud Ali Mohammadi Perpendicular J Ner

1473
00:70:12.838 --> 00:70:15.689
Urla. Good enough Gregory Hastings David Pins of Sean

1474
00:70:15.798 --> 00:70:19.673
Nelson, Mike Levin and Jos Net. A special

1475
00:70:19.685 --> 00:70:23.185
thanks to my producers is our web, Jim Frank

1476
00:70:23.194 --> 00:70:26.784
Luca Toni, Tom Veg and Bernard N Cortes Dixon

1477
00:70:26.793 --> 00:70:30.114
Bendik Muller Thomas Trumble, Catherine and Patrick Tobin,

1478
00:70:30.125 --> 00:70:32.154
John Carl, Negro, Nick Ortiz and Nick Golden

1479
00:70:32.293 --> 00:70:34.704
. And to my executive producers, Matthew lavender,

1480
00:70:34.833 --> 00:70:39.015
Si Adrian Bogdan Knits and Rosie. Thank you for

1481
00:70:39.024 --> 
all.

