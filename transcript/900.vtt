WEBVTT

1
00:00:00.009 --> 00:00:02.980
Hello everybody. Welcome to a new episode of the

2
00:00:03.049 --> 00:00:05.969
Center. I'm your host as always Ricard Lobs. And

3
00:00:05.980 --> 00:00:08.939
today I'm joined by Doctor Kaylyn o'connor. She is

4
00:00:08.949 --> 00:00:11.689
professor in the Department of Logic and Philosophy of

5
00:00:11.699 --> 00:00:15.170
Science at the University of California Herrin. She is

6
00:00:15.180 --> 00:00:18.774
a philosopher of biology and be sciences, philosopher of

7
00:00:18.784 --> 00:00:23.034
science and evolutionary game theorist. She is the author

8
00:00:23.045 --> 00:00:26.944
of books like the Origins of Unfairness Games in

9
00:00:26.954 --> 00:00:31.254
the Philosophy of Biology and the misinformation Age. And

10
00:00:31.264 --> 00:00:34.674
today we're going to talk about some of her

11
00:00:35.025 --> 00:00:38.924
uh topics. So some of the subjects she focus

12
00:00:38.935 --> 00:00:42.994
on. So Doctor o'conner, welcome to the show. It's

13
00:00:43.005 --> 00:00:44.354
a big pleasure to everyone.

14
00:00:45.244 --> 00:00:46.814
Oh, thanks for having me, Ricardo.

15
00:00:47.950 --> 00:00:51.470
So let me start by asking you since you

16
00:00:51.479 --> 00:00:54.840
apply a game theory in your work or at

17
00:00:54.849 --> 00:00:58.259
least to some of your work. So what is

18
00:00:58.270 --> 00:01:02.770
it basically? And to what kinds of topics do

19
00:01:02.779 --> 00:01:05.010
you apply it? Uh Mostly.

20
00:01:06.260 --> 00:01:09.470
Yeah. So game theory, it's um it's a branch

21
00:01:09.480 --> 00:01:13.900
of math that is used to study strategic human

22
00:01:13.910 --> 00:01:17.099
interactions. And so a strategic interaction. Is it anything

23
00:01:17.110 --> 00:01:20.230
where you have two individuals who are interacting and

24
00:01:20.239 --> 00:01:22.639
where they both care about what the other one

25
00:01:22.650 --> 00:01:25.330
does. So I have some stake in the game

26
00:01:25.339 --> 00:01:28.059
and what you're doing and the reverse is true.

27
00:01:28.370 --> 00:01:31.750
Um Game theory when it was first developed was

28
00:01:31.760 --> 00:01:37.410
applied just to humans. And usually the idea was

29
00:01:37.419 --> 00:01:40.209
we'll analyze people as if they're fully rational and

30
00:01:40.220 --> 00:01:42.300
then use that to predict what they might do

31
00:01:42.309 --> 00:01:45.389
or explain what we see people doing. So assume

32
00:01:45.400 --> 00:01:47.400
that I think really hard about what you're gonna

33
00:01:47.410 --> 00:01:49.360
do and then make my best choice for an

34
00:01:49.370 --> 00:01:51.529
action based on what you're gonna do and what

35
00:01:51.540 --> 00:01:54.989
I want to happen. Um Later on, it was

36
00:01:55.000 --> 00:01:58.379
introduced to biology as well to apply to different

37
00:01:58.389 --> 00:02:02.360
kinds of critters, you know, to animals, even things

38
00:02:02.370 --> 00:02:05.989
like micro organisms sometimes, and some of the assumptions

39
00:02:06.000 --> 00:02:09.440
were changed to think less about like rationality and

40
00:02:09.449 --> 00:02:13.839
more about how um how animals might learn to

41
00:02:13.850 --> 00:02:17.929
behave strategically or they might evolve to behave strategically.

42
00:02:18.179 --> 00:02:20.809
So a lot of the work I do uses

43
00:02:20.820 --> 00:02:24.910
those latter kinds of models or tools and I've

44
00:02:24.919 --> 00:02:27.919
applied game theoretic models to all sorts of systems.

45
00:02:27.929 --> 00:02:31.979
So I've used them to think about signaling in

46
00:02:31.990 --> 00:02:35.889
biology and in humans. So things like human language

47
00:02:35.899 --> 00:02:39.020
and how animals communicate with each other. I've used

48
00:02:39.029 --> 00:02:42.539
them to think about like perception in the brain.

49
00:02:42.690 --> 00:02:46.059
I've used them to think about things related to

50
00:02:46.070 --> 00:02:49.229
unfairness, which is related to that book. You mentioned

51
00:02:49.410 --> 00:02:53.039
like how to unfair norms emerge in human societies.

52
00:02:53.050 --> 00:02:55.820
Um I've used them to think about the evolution

53
00:02:55.830 --> 00:03:01.149
of moral emotions, like guilt and shame. Uh, AND

54
00:03:01.160 --> 00:03:04.509
also to some degree to think about stuff like

55
00:03:04.520 --> 00:03:07.399
misinformation and the spread of knowledge and belief in

56
00:03:07.410 --> 00:03:08.059
humans.

57
00:03:08.740 --> 00:03:12.800
Mhm. Yeah. And we'll get into some of those

58
00:03:12.809 --> 00:03:16.160
topics, uh, later in our conversation. But I would

59
00:03:16.169 --> 00:03:20.679
like to ask you now about science, the institution

60
00:03:20.690 --> 00:03:25.110
of science and scientists themselves because there's, that's also

61
00:03:25.119 --> 00:03:30.199
a topic that you study. So are scientists themselves

62
00:03:30.210 --> 00:03:33.000
subject to social pressures?

63
00:03:34.360 --> 00:03:39.770
Uh Yeah. OK. We're like jumping to different stuff.

64
00:03:39.779 --> 00:03:45.240
So, yes, they are. This isn't um an observation

65
00:03:45.250 --> 00:03:48.929
that's like new to me by any means. It's

66
00:03:48.940 --> 00:03:52.410
something that people have been noticing and talking about

67
00:03:52.419 --> 00:03:55.570
for decades and decades in thinking about how science

68
00:03:55.580 --> 00:04:00.630
works. Uh So scientists of course, are humans, they

69
00:04:00.639 --> 00:04:04.789
like other humans care about what other people think

70
00:04:04.800 --> 00:04:09.020
they grew up in human societies. They have human

71
00:04:09.029 --> 00:04:12.919
biases, human social tendencies. And there's a lot of

72
00:04:12.929 --> 00:04:17.070
really compelling evidence showing that all of that influences

73
00:04:17.079 --> 00:04:20.089
how science as an enterprise gets done.

74
00:04:22.079 --> 00:04:25.170
Uh And I mean, one of the things that

75
00:04:25.179 --> 00:04:28.059
uh we as humans are, at least to some

76
00:04:28.070 --> 00:04:33.679
extent is we are conformist. So is conformity, something

77
00:04:33.690 --> 00:04:37.269
that also happens in science and if so, is

78
00:04:37.279 --> 00:04:39.100
it good or bad?

79
00:04:39.929 --> 00:04:42.230
Yeah. So this is something that I've worked on

80
00:04:42.239 --> 00:04:46.790
with um a collaborator of mine, Jim Weall. So

81
00:04:46.799 --> 00:04:50.230
the idea of conformity of some sort mattering to

82
00:04:50.239 --> 00:04:53.390
science goes back a pretty long way. So for

83
00:04:53.399 --> 00:04:56.480
example, Thomas Coon, in this very famous book, The

84
00:04:56.489 --> 00:05:00.540
Structure of Scientific Revolutions has this idea that, you

85
00:05:00.549 --> 00:05:03.869
know, scientists tend to work within a paradigm and

86
00:05:03.950 --> 00:05:06.230
they'll all be kind of attached to this para

87
00:05:06.565 --> 00:05:08.834
this way of thinking about the world and there

88
00:05:08.845 --> 00:05:11.894
will be social pressure sort of keeping people within

89
00:05:11.904 --> 00:05:14.515
the paradigm and supporting it. And I think there's

90
00:05:14.524 --> 00:05:17.815
some idea there about people conforming with each other.

91
00:05:17.825 --> 00:05:20.484
And then he has this idea that like you

92
00:05:20.494 --> 00:05:23.554
get younger people who are less part of this

93
00:05:23.565 --> 00:05:27.859
community coming up and challenging the old notions in

94
00:05:27.869 --> 00:05:31.100
our work. What um Jim and I did was

95
00:05:31.109 --> 00:05:34.779
thought about groups of learners who are getting evidence

96
00:05:34.790 --> 00:05:37.420
from the world. So we built these models of

97
00:05:37.429 --> 00:05:41.279
individuals who can gather evidence and share evidence. And

98
00:05:41.290 --> 00:05:43.989
we asked, would they get better or worse at

99
00:05:44.000 --> 00:05:47.279
learning if we made them want to conform? And

100
00:05:47.290 --> 00:05:49.859
the way we included conformity in the model is

101
00:05:49.869 --> 00:05:51.720
we assumed all the agents are in a network.

102
00:05:51.730 --> 00:05:54.920
So this is something that represents their social connections.

103
00:05:54.929 --> 00:05:57.320
Each node in the network is an individual and

104
00:05:57.329 --> 00:05:59.839
then each link between them is like a social

105
00:05:59.850 --> 00:06:05.269
tie. And so our agents um in deciding what

106
00:06:05.279 --> 00:06:08.299
evidence to gather and what actions to take, they

107
00:06:08.309 --> 00:06:10.920
would both take evidence that they had gathered or

108
00:06:10.929 --> 00:06:13.559
seen in the past, but they would also think

109
00:06:13.570 --> 00:06:16.059
about what their neighbors were doing. So, for example,

110
00:06:16.070 --> 00:06:18.269
if I had a lot of evidence that say

111
00:06:18.910 --> 00:06:22.589
the COVID vaccine is relatively low risk, but I

112
00:06:22.600 --> 00:06:24.989
had a lot of neighbors who were not getting

113
00:06:25.000 --> 00:06:28.179
vaccinated. In this model, I might choose not to

114
00:06:28.190 --> 00:06:31.309
get vaccinated because I prefer to conform with neighbors.

115
00:06:31.420 --> 00:06:34.140
And then we ask, how does that impact learning

116
00:06:34.149 --> 00:06:35.880
and decision making in the group? And then these

117
00:06:35.890 --> 00:06:39.709
models, we found that on average conformity would tend

118
00:06:39.720 --> 00:06:43.149
to make the group worse at learning and individuals

119
00:06:43.160 --> 00:06:46.250
more likely to take bad actions. And there were

120
00:06:46.260 --> 00:06:49.899
a couple of reasons for that. So one thing

121
00:06:49.910 --> 00:06:53.299
is that conformity would stop individuals from sharing good

122
00:06:53.309 --> 00:06:57.250
evidence. So say, I think personally that vaccines are

123
00:06:57.260 --> 00:06:58.950
safe and I have good evidence that that's the

124
00:06:58.959 --> 00:07:01.470
case. But I'm with a bunch of people who

125
00:07:01.480 --> 00:07:04.700
aren't getting vaccinated if I conform to them, I

126
00:07:04.709 --> 00:07:07.040
never tell them about my good evidence. I never

127
00:07:07.049 --> 00:07:09.480
get vaccinated and let them see what happens. So

128
00:07:09.489 --> 00:07:12.440
you create these kind of information bottlenecks where good

129
00:07:12.450 --> 00:07:14.820
information isn't flowing in the network,

130
00:07:16.000 --> 00:07:19.940
but the science or uh the science tend to

131
00:07:19.950 --> 00:07:24.630
be a conservative enterprise. And if so, what does

132
00:07:24.640 --> 00:07:29.179
that mean exactly within the domain of science to

133
00:07:29.190 --> 00:07:30.339
be conservative?

134
00:07:32.459 --> 00:07:36.209
Yeah, that's a really um a big question and

135
00:07:36.220 --> 00:07:39.399
I think a really hard one to answer. So

136
00:07:40.380 --> 00:07:45.970
some people in philosophy of science worry about conservatism

137
00:07:45.980 --> 00:07:48.899
in science and when they talk about that. So

138
00:07:48.910 --> 00:07:51.100
this isn't, this is something I've written a little

139
00:07:51.109 --> 00:07:52.980
bit on, but it's not like a central topic

140
00:07:52.989 --> 00:07:55.429
for me. So just keep that in mind when

141
00:07:55.440 --> 00:07:59.420
they talk about that. Um Usually what they mean

142
00:07:59.429 --> 00:08:02.399
is something like there are forces or structures in

143
00:08:02.410 --> 00:08:06.480
science that keep scientists working on the same kinds

144
00:08:06.489 --> 00:08:11.420
of problems or keep them from, for example, looking

145
00:08:11.429 --> 00:08:14.450
at questions that are too weird or too outside

146
00:08:14.459 --> 00:08:17.100
the norm or that are, for example, high risk,

147
00:08:17.109 --> 00:08:19.929
high reward or that are sometimes people call it

148
00:08:19.940 --> 00:08:26.350
mavericky or um just more unusual. And so a

149
00:08:26.359 --> 00:08:28.690
lot of people argue that there are forces that

150
00:08:28.700 --> 00:08:32.429
kind of keep scientists from doing stuff that's too

151
00:08:32.438 --> 00:08:36.078
different or too unusual. And sometimes people argue that

152
00:08:36.087 --> 00:08:39.188
that's a bad thing that um what you want

153
00:08:39.198 --> 00:08:41.318
across a community of science is to have at

154
00:08:41.328 --> 00:08:44.528
least a good handful of people working on topics

155
00:08:44.539 --> 00:08:47.948
that maybe don't seem right or hypotheses that people

156
00:08:48.059 --> 00:08:51.799
don't currently believe or things that seem risky or

157
00:08:51.809 --> 00:08:54.340
strange. And if you have that handful of people,

158
00:08:54.349 --> 00:08:57.119
maybe most of them fail, but some of them

159
00:08:57.130 --> 00:09:00.320
do succeed. And in doing so might discover things

160
00:09:00.330 --> 00:09:03.020
that are really unusual or sort of revolutionize a

161
00:09:03.030 --> 00:09:05.880
science. So that's why a lot of people are

162
00:09:05.890 --> 00:09:09.969
like, conservatism could be bad in science. Um The

163
00:09:09.979 --> 00:09:12.950
different arguments I've heard about like why you see

164
00:09:12.960 --> 00:09:17.349
conservatism in science are things like uh well, increasingly

165
00:09:17.359 --> 00:09:21.559
the age of scientific investigators is going up, increasingly

166
00:09:21.570 --> 00:09:23.369
it takes longer and longer to get to the

167
00:09:23.380 --> 00:09:25.960
position where you're the head of a lab and

168
00:09:25.969 --> 00:09:28.409
you might just be indoctrinated for a very long

169
00:09:28.419 --> 00:09:30.570
time within a field before you're the one picking

170
00:09:30.580 --> 00:09:36.039
the research choices. Um People argue that uh grant

171
00:09:36.049 --> 00:09:39.200
giving agencies are inherently conservative because they're trying to

172
00:09:39.210 --> 00:09:42.059
get these good outcomes. And so they tend to

173
00:09:42.070 --> 00:09:44.619
give money to, to projects that look more safe,

174
00:09:44.630 --> 00:09:49.609
more dependable, more reliable. Um Some agencies have set

175
00:09:49.619 --> 00:09:51.880
up these sort of high risk, high reward special

176
00:09:51.890 --> 00:09:55.250
grants to try to push against that. I wrote

177
00:09:55.260 --> 00:09:59.570
this one paper that modeled scientific communities and asked,

178
00:09:59.729 --> 00:10:02.080
OK, what would be the conditions under which we'd

179
00:10:02.090 --> 00:10:07.239
expect conservative science to spread as people train their

180
00:10:07.250 --> 00:10:10.409
students and then those students get jobs. And the

181
00:10:10.419 --> 00:10:14.489
argument I made in that paper is that while

182
00:10:14.500 --> 00:10:19.330
conservative science tends to be more like less risky,

183
00:10:19.450 --> 00:10:21.609
you know, that you're gonna be able to get

184
00:10:21.619 --> 00:10:25.390
some discovery and publish it compared to something that's

185
00:10:25.400 --> 00:10:28.030
more high risk where maybe you're gonna get a

186
00:10:28.039 --> 00:10:30.229
huge payoff, but maybe it's just not gonna turn

187
00:10:30.239 --> 00:10:33.440
out to be anything interesting at all. Um And

188
00:10:33.450 --> 00:10:36.380
so in my models, I made that assumption and

189
00:10:36.390 --> 00:10:39.330
I found that often, in fact, high risk science

190
00:10:39.750 --> 00:10:42.340
would be likely to spread because the people doing

191
00:10:42.349 --> 00:10:46.250
it are getting these really high payoffs. They become,

192
00:10:46.260 --> 00:10:48.739
who succeed, are getting these really high payoffs, they

193
00:10:48.750 --> 00:10:52.099
become famous, their students can get jobs. But the

194
00:10:52.109 --> 00:10:54.590
problem with that is that it's often hard to

195
00:10:54.599 --> 00:10:57.469
repeat. So on the assumption that if I'm a

196
00:10:57.479 --> 00:10:59.320
sort of risk taking scientist and I happen to

197
00:10:59.330 --> 00:11:01.890
be successful, my students, if they try to take

198
00:11:01.900 --> 00:11:04.140
risks, may or may not be, I find that

199
00:11:04.150 --> 00:11:07.489
in those cases, conservatism can kind of dominate in

200
00:11:07.500 --> 00:11:08.280
science.

201
00:11:09.409 --> 00:11:12.669
And I imagine that in this case, it's high

202
00:11:12.679 --> 00:11:17.510
risk because it might imply several different sorts of

203
00:11:17.520 --> 00:11:21.739
potential damage, like reputation damage and in the extreme,

204
00:11:21.750 --> 00:11:27.739
perhaps losing one's own uh career and academic credibility.

205
00:11:27.750 --> 00:11:28.260
I guess

206
00:11:28.880 --> 00:11:30.880
there can be those kinds of risks and there

207
00:11:30.890 --> 00:11:33.020
can also just be the risk that you spend

208
00:11:33.030 --> 00:11:35.419
a lot of time and effort on some project

209
00:11:35.429 --> 00:11:37.280
and then you just don't get anything out at

210
00:11:37.289 --> 00:11:39.500
the end. So for example, if you think about

211
00:11:39.510 --> 00:11:41.780
someone who's trying to get tenure in an academic

212
00:11:41.789 --> 00:11:46.140
system, they have to publish before the tenure clock

213
00:11:46.150 --> 00:11:48.500
runs out. So if you take on a project

214
00:11:48.510 --> 00:11:51.200
that like maybe it's gonna turn out great, but

215
00:11:51.210 --> 00:11:53.169
there's a pretty good chance the whole project is

216
00:11:53.179 --> 00:11:56.179
gonna fail, you can see why that might be

217
00:11:56.390 --> 00:11:58.580
a bad choice for that individual.

218
00:11:59.640 --> 00:12:03.059
But these are not easy problem. Uh These, these

219
00:12:03.070 --> 00:12:06.099
problems are not easy to navigate, right? Because on

220
00:12:06.109 --> 00:12:10.549
the one hand, it's understandable that uh looking at

221
00:12:10.559 --> 00:12:15.190
the whole system, uh many times it's not worth

222
00:12:15.200 --> 00:12:20.229
it to waste resources on research or ideas that

223
00:12:20.239 --> 00:12:23.489
won't produce anything. Of course, I, I imagine that

224
00:12:23.650 --> 00:12:26.869
it's hard before and to really know for sure

225
00:12:26.880 --> 00:12:30.710
what would be wasteful or not. Uh, BUT on

226
00:12:30.719 --> 00:12:34.854
the other hand, uh, we also very much need,

227
00:12:34.864 --> 00:12:39.065
at least sometimes, uh, people who think a little

228
00:12:39.075 --> 00:12:41.815
bit more outside of the box to push the

229
00:12:41.825 --> 00:12:43.265
fields forward.

230
00:12:43.794 --> 00:12:45.534
Right. Yeah, that's right. And I think, you know,

231
00:12:45.544 --> 00:12:48.255
say you're the National Science Foundation in the US

232
00:12:48.265 --> 00:12:52.585
or say you're, um, an eu grant giving body,

233
00:12:53.770 --> 00:12:58.349
you're giving these grants, you are paid by tax

234
00:12:58.359 --> 00:13:01.190
dollars and run through a government. And so you

235
00:13:01.200 --> 00:13:04.369
need to justify to the people of, you know,

236
00:13:04.380 --> 00:13:07.989
your country or your institution, why you're giving the

237
00:13:08.000 --> 00:13:11.349
grants you're giving and what they're for. And sometimes

238
00:13:11.359 --> 00:13:12.900
I think it can be really hard to say

239
00:13:12.909 --> 00:13:15.809
like there was a good reason why we gave

240
00:13:15.820 --> 00:13:17.380
a grant to this person who's doing this thing

241
00:13:17.390 --> 00:13:20.520
that sounds kind of wacky or out there. Um

242
00:13:21.039 --> 00:13:25.429
And so that I think there are like these

243
00:13:25.440 --> 00:13:29.150
very practical reasons why you wouldn't necessarily want to

244
00:13:29.159 --> 00:13:32.049
support higher risk science, even if you can make

245
00:13:32.059 --> 00:13:34.580
the argument like, ok, but across a whole body

246
00:13:34.590 --> 00:13:36.359
of scientists, we want to have at least some

247
00:13:36.369 --> 00:13:39.830
people doing this higher risk stuff. Uh One of

248
00:13:39.840 --> 00:13:44.219
my colleagues, Kyle Stanford has argued that um you

249
00:13:44.229 --> 00:13:48.440
know, changes in funding, like have promoted conservatism in

250
00:13:48.450 --> 00:13:51.190
part because when you had a good number of

251
00:13:51.200 --> 00:13:54.770
like independently wealthy scientists, they could just do whatever

252
00:13:54.780 --> 00:13:57.969
they wanted, right? Like they, you know, they didn't

253
00:13:57.979 --> 00:14:00.299
have to answer to anybody if they think, like,

254
00:14:00.390 --> 00:14:03.099
well, tomorrow I'm gonna go look at all the

255
00:14:03.159 --> 00:14:07.599
earthworms all over England. Like Charles Darwin did. Who's

256
00:14:07.609 --> 00:14:10.309
gonna say no? Yeah. Yeah.

257
00:14:10.789 --> 00:14:13.280
Yeah. That, that was exactly one of the things

258
00:14:13.289 --> 00:14:15.219
that was coming to my mind while you were

259
00:14:15.229 --> 00:14:18.940
speaking because, uh, I mean, of course, historically, we

260
00:14:18.950 --> 00:14:22.979
know that most of the people who made a

261
00:14:22.989 --> 00:14:28.570
scientific progress were usually very, very wealthy people, or

262
00:14:28.580 --> 00:14:32.919
at least people with, with good enough, uh, monetary

263
00:14:32.929 --> 00:14:36.679
resources and other kinds of resources to devote their

264
00:14:36.690 --> 00:14:40.960
time and they didn't have to answer to anybody

265
00:14:41.049 --> 00:14:44.599
but mo for the most part, uh, and they

266
00:14:44.609 --> 00:14:47.419
could dedicate as much time as they wanted and

267
00:14:47.429 --> 00:14:51.059
as long as they wanted to study any kind

268
00:14:51.070 --> 00:14:53.919
of subject like Charles Darwin in the, in the

269
00:14:53.929 --> 00:14:57.229
19th century. And I guess that also to some

270
00:14:57.239 --> 00:15:01.320
extent, Einstein in the 20th century. But, uh, I

271
00:15:01.330 --> 00:15:06.960
mean, um, uh, uh, those were on the one

272
00:15:06.969 --> 00:15:09.760
hand, lucky people and on the other hand, uh,

273
00:15:09.770 --> 00:15:13.000
privileged people. And on the other hand, I mean,

274
00:15:13.010 --> 00:15:18.979
if we are to really push science forward more

275
00:15:18.989 --> 00:15:22.770
rapidly and involving more people, which I imagine it's

276
00:15:22.780 --> 00:15:27.119
better than just rely on perhaps a handful of

277
00:15:27.130 --> 00:15:31.409
lucky geniuses or something like that, uh, I, I

278
00:15:31.419 --> 00:15:37.510
mean, it, it's not really feasible to wait for

279
00:15:37.520 --> 00:15:39.369
that kind of thing to happen.

280
00:15:40.010 --> 00:15:43.210
Right. No, I think, you know, another point which

281
00:15:43.219 --> 00:15:45.169
a lot of people have made and seems right?

282
00:15:45.179 --> 00:15:48.440
Also is that when we're thinking about this desire

283
00:15:48.450 --> 00:15:51.150
to have science as a group working on many,

284
00:15:51.159 --> 00:15:55.510
many different types of topics, um like another way

285
00:15:55.520 --> 00:15:57.469
that you get that is by drawing on different

286
00:15:57.479 --> 00:16:01.229
kinds of people with different backgrounds and concerns. So

287
00:16:02.010 --> 00:16:04.280
in some way, like being, you know, whatever a

288
00:16:04.289 --> 00:16:06.320
wealthy gentleman means, you have all the freedom to

289
00:16:06.330 --> 00:16:08.500
work on whatever wacky thing you want. But it

290
00:16:08.510 --> 00:16:11.479
was also the case that historically in science, especially

291
00:16:11.489 --> 00:16:13.349
if we're looking at like Western science since the

292
00:16:13.359 --> 00:16:17.119
scientific revolution. Well, it's like a lot of wealthy,

293
00:16:17.130 --> 00:16:21.684
independent white European and men, right? And so there's

294
00:16:21.695 --> 00:16:24.534
not as much diversity as a perspective as you

295
00:16:24.544 --> 00:16:27.085
would see in a lot of scientific communities now.

296
00:16:27.275 --> 00:16:29.155
And so people also think, well, that kind of

297
00:16:29.164 --> 00:16:33.565
diversity of perspective is another source for new ideas,

298
00:16:33.575 --> 00:16:36.405
different ideas, ideas that will push science forward.

299
00:16:37.359 --> 00:16:40.510
Yeah. Also because uh I don't know if this

300
00:16:40.520 --> 00:16:44.400
is something that you look into specifically. But over

301
00:16:44.409 --> 00:16:46.340
the years I've been talking on the show with

302
00:16:46.520 --> 00:16:51.260
um for example, cultural psychologists and cognitive scientists and

303
00:16:51.690 --> 00:16:55.869
uh even people that come from different cultural backgrounds

304
00:16:55.880 --> 00:17:00.219
tend to think about things in different ways. And

305
00:17:00.409 --> 00:17:04.420
it's very much valuable to science to have people

306
00:17:04.469 --> 00:17:09.380
with different cultural backgrounds and coming to the table

307
00:17:09.390 --> 00:17:12.709
with different uh cognitions, I guess.

308
00:17:13.290 --> 00:17:15.550
Yeah. And I think there's a lot of examples

309
00:17:15.560 --> 00:17:17.989
from the history of science that demonstrate how that

310
00:17:18.000 --> 00:17:19.630
can can be beneficial.

311
00:17:20.660 --> 00:17:23.819
Uh So since we're talking about science, from the

312
00:17:23.829 --> 00:17:27.660
perspective of uh game theory or trying to understand

313
00:17:27.670 --> 00:17:32.209
science through game theory or more specifically evolutionary game

314
00:17:32.219 --> 00:17:37.130
theory, uh is the scientific community also a population

315
00:17:37.140 --> 00:17:39.599
that undergoes selection,

316
00:17:41.449 --> 00:17:44.030
I think it can be thought of that way.

317
00:17:44.040 --> 00:17:48.479
So there's been this kind of handful of people

318
00:17:48.489 --> 00:17:56.119
using evolutionary models to think about scientific communities. So

319
00:17:56.130 --> 00:17:59.520
the idea of thinking about science as an evolutionary

320
00:17:59.530 --> 00:18:03.119
system goes back like much further. So for example,

321
00:18:03.130 --> 00:18:05.770
David Hall wrote this very influential book where he

322
00:18:05.780 --> 00:18:07.849
was thinking about the evolution of science, but he

323
00:18:07.859 --> 00:18:10.050
was thinking of the units more or as ideas

324
00:18:10.060 --> 00:18:13.209
where the ideas themselves are being selected and changed.

325
00:18:13.380 --> 00:18:16.369
Whereas recently, a bunch of people have been writing

326
00:18:16.380 --> 00:18:18.949
papers where the units are scientists and you can

327
00:18:18.959 --> 00:18:22.959
ask questions like who remains within a scientific community

328
00:18:22.969 --> 00:18:26.709
and why um given that scientists are using different

329
00:18:26.719 --> 00:18:29.329
sorts of approaches, what approaches will tend to stay

330
00:18:29.339 --> 00:18:32.150
in the community, what approaches will tend to spread,

331
00:18:32.449 --> 00:18:34.829
uh which ones will be more prominent and so

332
00:18:34.839 --> 00:18:39.050
will be copied more. And you know, this isn't,

333
00:18:39.060 --> 00:18:42.260
it's not like um this is entirely novel, you

334
00:18:42.270 --> 00:18:45.380
know, there's this whole field of cultural evolutionary theory

335
00:18:45.390 --> 00:18:47.949
which thinks about how do we take evolutionary theory

336
00:18:48.020 --> 00:18:51.280
and apply it to human practices and norms and

337
00:18:51.290 --> 00:18:54.640
actions and beliefs and systems. And so you can

338
00:18:54.650 --> 00:18:57.989
do the same thing within science. It, it, time

339
00:18:58.030 --> 00:18:59.209
I think pretty successfully.

340
00:18:59.859 --> 00:19:05.160
Mhm. Bicultural evolutionary theory, you mean, uh, the work,

341
00:19:05.170 --> 00:19:09.800
uh, been being done by people like Robert Boyd,

342
00:19:09.810 --> 00:19:13.880
Peter Richardson, Joe Eric and perhaps, uh, more on

343
00:19:13.890 --> 00:19:17.010
the side of the Parisian school than Spur, I

344
00:19:17.290 --> 00:19:18.670
mean, people like that.

345
00:19:19.199 --> 00:19:21.550
Yeah, those are a bunch of the, like, most

346
00:19:21.560 --> 00:19:24.939
prominent people doing cultural evolutionary theory though. There's actually,

347
00:19:24.949 --> 00:19:26.910
there's a lot of threads of it, you know,

348
00:19:26.920 --> 00:19:30.329
there's um people who do more game theoretic approaches,

349
00:19:30.339 --> 00:19:32.640
people who do other types of different sorts of

350
00:19:32.650 --> 00:19:36.020
approaches. There's a lot of uh work in anthropology

351
00:19:36.030 --> 00:19:40.150
on cultural evolutionary theory that's less modeling, more empirical.

352
00:19:40.180 --> 00:19:44.079
Um It's a thoroughly interdisciplinary area. And so you

353
00:19:44.089 --> 00:19:47.719
have all these kind of different paradigms or frameworks

354
00:19:47.729 --> 00:19:49.670
for thinking about cultural evolution.

355
00:19:50.099 --> 00:19:52.359
Yeah. I it in fact, integrates a lot of

356
00:19:52.369 --> 00:19:56.180
different things. There are people that work uh with

357
00:19:56.189 --> 00:20:02.680
everything from evolutionary psychology to anthropology, cultural evolution, human

358
00:20:03.170 --> 00:20:07.459
ecology. Basically they apply all of those tools in

359
00:20:07.469 --> 00:20:13.030
cultural evolutionary theory, right? Yeah, that's right. So uh

360
00:20:13.040 --> 00:20:16.560
let me ask you about another topic regarding science

361
00:20:16.569 --> 00:20:20.239
just before we move on to other subjects. So,

362
00:20:20.479 --> 00:20:26.000
um there are scientific retraction sometimes uh when it

363
00:20:26.010 --> 00:20:29.599
happens, does it usually work or not?

364
00:20:31.189 --> 00:20:33.589
Uh So this is I I take it you're

365
00:20:33.599 --> 00:20:35.310
asking this question because I have this paper on

366
00:20:35.319 --> 00:20:39.569
scientific retraction. Um This is a topic that has

367
00:20:39.579 --> 00:20:43.099
been really interesting to me because, you know, we

368
00:20:43.109 --> 00:20:46.439
think of science as this kind of, you know,

369
00:20:46.449 --> 00:20:49.219
we would want science to be an ideal community

370
00:20:49.229 --> 00:20:51.339
of learners, right? And that's what science is trying

371
00:20:51.349 --> 00:20:53.959
to be a group of people who are using

372
00:20:53.969 --> 00:20:57.439
the best methods, the best practices available to learn

373
00:20:57.449 --> 00:20:59.489
about the world and develop good beliefs about the

374
00:20:59.500 --> 00:21:03.609
world. And like science is pretty successful in doing

375
00:21:03.619 --> 00:21:06.560
that. But of course, there are, it's, it's all

376
00:21:06.569 --> 00:21:08.550
it's people in the end. So it's not gonna

377
00:21:08.560 --> 00:21:11.939
be perfect. So one thing that's been really widely

378
00:21:11.949 --> 00:21:15.900
observed in people who do empirical studies of scientific

379
00:21:15.910 --> 00:21:20.119
communities is that retractions often fail or aren't fully

380
00:21:20.130 --> 00:21:25.140
successful. So maybe someone um uh is caught having

381
00:21:25.150 --> 00:21:28.180
committed fraud and some of their papers are retracted

382
00:21:28.420 --> 00:21:30.770
often if you look later at those papers, they're

383
00:21:30.780 --> 00:21:34.310
being cited in the literature and they're not being

384
00:21:34.319 --> 00:21:37.380
cited as retracted or fraudulent papers, they're just being

385
00:21:37.390 --> 00:21:39.780
cited sort of straightforward by people who don't know

386
00:21:39.790 --> 00:21:42.579
that they've actually been retracted. And so this happens

387
00:21:42.589 --> 00:21:45.500
kind of again and again and again. Um AND

388
00:21:45.510 --> 00:21:47.849
then related to that, I think you see similar

389
00:21:47.859 --> 00:21:51.280
things happening outside of science just in broader societies.

390
00:21:51.290 --> 00:21:55.219
So for example, during the COVID-19 pandemic, it happened

391
00:21:55.310 --> 00:21:58.579
dozens of times that there would be some uh

392
00:21:58.599 --> 00:22:03.579
scientific claim shared by journalists, you know, for example,

393
00:22:03.589 --> 00:22:07.310
uh there was this very influential early study done

394
00:22:07.319 --> 00:22:11.239
in Northern California that um claimed that the fatality

395
00:22:11.250 --> 00:22:13.400
rate of COVID was much lower than it actually

396
00:22:13.410 --> 00:22:18.750
is that study was really widely cited, uh turned

397
00:22:18.760 --> 00:22:22.020
out they had, had an error in their initial

398
00:22:22.030 --> 00:22:26.579
reprint that they quickly fixed, but their initial numbers

399
00:22:26.589 --> 00:22:29.260
propagated pretty far. And I don't think the like

400
00:22:29.270 --> 00:22:32.699
report of the error and the proper numbers propagated

401
00:22:32.719 --> 00:22:35.380
nearly as far. Right. So people didn't get the

402
00:22:35.390 --> 00:22:38.359
message that their initial numbers were wrong and in

403
00:22:38.369 --> 00:22:43.130
fact, underestimated the fatality. Right. So that's another kind

404
00:22:43.140 --> 00:22:45.319
of instance, they're both instances where you have these

405
00:22:45.329 --> 00:22:51.689
scientific claims that have like viability legitimacy, they spread

406
00:22:51.699 --> 00:22:55.030
in social networks when they are reversed or known

407
00:22:55.040 --> 00:22:58.250
to be false. Not everyone finds out that they

408
00:22:58.260 --> 00:23:01.050
would were false who initially found out about this

409
00:23:01.060 --> 00:23:05.619
claim. So some co-author um Anders Guy and Travis

410
00:23:05.630 --> 00:23:08.689
Laquan and I built models where we would have

411
00:23:08.699 --> 00:23:10.569
a network and we would have something like a

412
00:23:10.579 --> 00:23:13.130
false claim spreading in the network and then we'd

413
00:23:13.140 --> 00:23:16.319
introduce a retraction and let that spread as well.

414
00:23:16.329 --> 00:23:18.229
And we would ask, well, what are the situations

415
00:23:18.239 --> 00:23:20.280
where people find out about the retraction? When do

416
00:23:20.290 --> 00:23:22.729
they continue to hold the false belief, et cetera?

417
00:23:23.140 --> 00:23:24.569
Um And we found that in a lot of

418
00:23:24.579 --> 00:23:26.859
cases, people would just hold the false belief as

419
00:23:26.869 --> 00:23:29.680
an accident of history. You know, you have these

420
00:23:29.689 --> 00:23:32.050
things spreading from person to person. And so some

421
00:23:32.060 --> 00:23:34.439
people just get the false thing and they never

422
00:23:34.500 --> 00:23:37.640
hear about the true thing. It's just what happens

423
00:23:37.650 --> 00:23:41.599
randomly, right? Because of chance. But we found some

424
00:23:41.609 --> 00:23:44.609
things like would influence how often that happened. So

425
00:23:44.619 --> 00:23:48.349
for example, we found that retractions or reversals that

426
00:23:48.359 --> 00:23:52.069
are instigated by the original source are much more

427
00:23:52.079 --> 00:23:54.750
successful. So if I'm hearing the network and I

428
00:23:54.760 --> 00:23:57.170
say something false, it starts to spread. If I

429
00:23:57.180 --> 00:23:59.300
say oops, that was false, that spreads to the

430
00:23:59.310 --> 00:24:01.750
same people and they care about that. But if

431
00:24:01.760 --> 00:24:04.250
some other person is like, no, they're wrong and

432
00:24:04.260 --> 00:24:07.050
they're not sort of connected to the people finding

433
00:24:07.060 --> 00:24:09.189
out about this false thing in the first place.

434
00:24:09.420 --> 00:24:11.880
People don't care as much about this retraction, right?

435
00:24:11.890 --> 00:24:14.199
So they're the right people who hold false beliefs

436
00:24:14.209 --> 00:24:16.449
aren't finding out they're wrong. So that was one

437
00:24:16.459 --> 00:24:20.209
thing. Another thing is that weirdly like very prominent

438
00:24:20.219 --> 00:24:25.160
false beliefs were often more reversible because, you know,

439
00:24:25.170 --> 00:24:28.040
imagine, um some study that turns out to be

440
00:24:28.050 --> 00:24:30.599
wrong but nobody knows about it. Well, someone comes

441
00:24:30.609 --> 00:24:33.260
and says this was wrong. Well, nobody cares. Right.

442
00:24:33.270 --> 00:24:35.599
So there aren't that many people to spread the

443
00:24:35.609 --> 00:24:39.680
retraction. Whereas a very prominent false belief, if someone

444
00:24:40.469 --> 00:24:42.810
shows that it's wrong, there's a lot of people

445
00:24:42.819 --> 00:24:45.250
to spread the retraction to spread that it was

446
00:24:45.260 --> 00:24:47.969
wrong. So we found in our models that sometimes

447
00:24:48.310 --> 00:24:52.020
uh some false belief being really widely held meant

448
00:24:52.030 --> 00:24:53.650
that it was easier to reverse.

449
00:24:54.589 --> 00:24:59.869
Mhm. And I mean, if that happens, um would

450
00:24:59.880 --> 00:25:04.689
you have any suggestions as to how people should

451
00:25:04.699 --> 00:25:06.810
deal with it? I mean, when the paper gets

452
00:25:06.819 --> 00:25:12.219
retracted. If for some reason, there are some people

453
00:25:12.229 --> 00:25:15.089
out there that never get to learn that it

454
00:25:15.099 --> 00:25:19.420
was retracted and keep citing it in their own

455
00:25:19.430 --> 00:25:24.800
papers. What would be perhaps good solutions to try

456
00:25:24.810 --> 00:25:26.380
to deal with that?

457
00:25:27.880 --> 00:25:30.310
If we're talking about within science, there are some

458
00:25:30.319 --> 00:25:33.660
things that we recommend that um other people have

459
00:25:33.670 --> 00:25:36.680
talked about too. So one thing is that authors

460
00:25:36.689 --> 00:25:40.579
and journals aren't usually incentivized to talk about something

461
00:25:40.589 --> 00:25:43.430
being retracted because who wants to say like I

462
00:25:43.439 --> 00:25:46.239
was wrong or we published a paper where the

463
00:25:46.250 --> 00:25:51.209
person committed fraud. Uh So especially with journals, a

464
00:25:51.219 --> 00:25:53.589
lot of them are not very active about sharing

465
00:25:53.599 --> 00:25:55.810
that something's been retracted. You know, they're not like

466
00:25:55.819 --> 00:26:01.449
splashing it on their front page. Uh They're often

467
00:26:01.459 --> 00:26:04.739
not even like making a very clear sort of

468
00:26:04.750 --> 00:26:07.319
retraction statement on the page where it might be

469
00:26:07.329 --> 00:26:09.369
linked. Sometimes you'll just see ones where it just

470
00:26:09.380 --> 00:26:11.819
says retract in like a tiny little word. So

471
00:26:11.829 --> 00:26:14.560
I think journals should be much more active about

472
00:26:14.569 --> 00:26:19.280
communicating about retraction. Um Search engines like Google scholar

473
00:26:19.290 --> 00:26:22.189
should be much better about making clear when things

474
00:26:22.199 --> 00:26:24.000
are retracted. Like have a big note on the

475
00:26:24.010 --> 00:26:25.939
top of a paper when you search it saying

476
00:26:25.949 --> 00:26:31.489
that it's been retracted. Um As much as possible

477
00:26:31.500 --> 00:26:34.770
individuals when they're being responsible should share when they

478
00:26:34.780 --> 00:26:37.239
were wrong or when they cited something wrong. But

479
00:26:37.250 --> 00:26:39.410
of course, it's hard to get people to do

480
00:26:39.420 --> 00:26:41.670
that uh, the other thing I think would be

481
00:26:41.680 --> 00:26:44.920
good is, you know, when academic journals publish a

482
00:26:44.930 --> 00:26:48.829
paper, um, you know, there's a whole process of

483
00:26:48.839 --> 00:26:51.510
like checking over that paper, if we have a

484
00:26:51.520 --> 00:26:55.079
database of retracted articles, that would be a natural

485
00:26:55.089 --> 00:26:57.670
time step to just compare all the citations in

486
00:26:57.680 --> 00:27:00.069
the paper with things that have been retracted and

487
00:27:00.079 --> 00:27:02.900
flag ones that might be false as well.

488
00:27:03.459 --> 00:27:08.750
Mhm. So, changing topics now, I mean, I've already

489
00:27:08.760 --> 00:27:13.140
had many discussions on the show with moral philosophers,

490
00:27:13.150 --> 00:27:16.619
particularly moral philosophers about this, but also with uh

491
00:27:16.630 --> 00:27:22.979
with uh some scientists is naturalism congruent with any

492
00:27:22.989 --> 00:27:27.550
me ethics. I mean, if we, if we tackle

493
00:27:27.560 --> 00:27:31.829
things from a naturalistic perspective, I mean, are we

494
00:27:31.839 --> 00:27:37.400
committing ourselves necessarily to, for example, moral realism or

495
00:27:37.410 --> 00:27:39.040
moral anti realism?

496
00:27:40.550 --> 00:27:45.479
Um So I, like, you know, as a grad

497
00:27:45.489 --> 00:27:48.489
student, I co wrote a paper on this with

498
00:27:48.500 --> 00:27:51.170
like my advisor. It's not, it's not really my

499
00:27:51.180 --> 00:27:54.579
area of study. Um I mean, I can tell

500
00:27:54.589 --> 00:27:56.430
you what we said in that, which was something

501
00:27:56.439 --> 00:28:00.189
like you can both have a meta ethics that's

502
00:28:00.199 --> 00:28:04.119
thoroughly naturalistic, you know, things that like human brains

503
00:28:04.130 --> 00:28:06.189
evolved the way they did for all of these

504
00:28:06.199 --> 00:28:10.229
sort of practical and pragmatic reasons that evolution explains

505
00:28:10.239 --> 00:28:12.869
why we have the moral beliefs and instincts we

506
00:28:12.880 --> 00:28:15.199
have. So we wanna say you can hold that

507
00:28:15.209 --> 00:28:17.569
true and at the same time be like, but

508
00:28:17.579 --> 00:28:19.969
I have first order moral beliefs and I still

509
00:28:19.979 --> 00:28:22.229
gonna argue for them with all the sort of

510
00:28:22.239 --> 00:28:24.520
force that I did before. Like, I can believe

511
00:28:24.530 --> 00:28:27.400
all that and still think, like, hurting people is

512
00:28:27.410 --> 00:28:29.560
wrong. And so I can still argue about what

513
00:28:29.569 --> 00:28:32.099
we ought to do and not do whether or

514
00:28:32.109 --> 00:28:35.359
not I have a fully naturalistic picture of human

515
00:28:35.369 --> 00:28:36.180
morality.

516
00:28:36.839 --> 00:28:40.900
Mhm. Uh, OK. So another topic that you've, uh,

517
00:28:40.910 --> 00:28:47.069
wrote a bit about is perceptual categories. So, do

518
00:28:47.079 --> 00:28:50.619
we know if they have evolved to track properties

519
00:28:50.630 --> 00:28:52.819
of the world or not?

520
00:28:53.800 --> 00:28:57.290
Yeah. So this is something that, um, gets into

521
00:28:57.300 --> 00:29:01.750
a lot of like really deep discussions and debate

522
00:29:01.760 --> 00:29:05.640
in philosophy, philosophy of perception, but also in cognitive

523
00:29:05.650 --> 00:29:09.969
science, I mean, obviously, so we have our perceptual

524
00:29:09.979 --> 00:29:13.489
senses to mediate our behaviors in the world, right?

525
00:29:13.500 --> 00:29:16.760
Where organisms, our goals, I mean, when I put

526
00:29:16.770 --> 00:29:20.219
goals, our goals as shaped by evolution are to

527
00:29:20.229 --> 00:29:25.239
like eat, drink sleep, keep our bodies alive, eventually

528
00:29:25.250 --> 00:29:29.099
reproduce, right? Um, AND maybe keep our Children alive

529
00:29:29.109 --> 00:29:31.540
so that they can reproduce. So we have bodies

530
00:29:31.550 --> 00:29:33.680
that are trying to do those things. Perception is

531
00:29:33.689 --> 00:29:35.599
useful in as much as it allows us to

532
00:29:35.609 --> 00:29:38.969
do those things. Um, SO some people like Don

533
00:29:38.979 --> 00:29:42.680
Hoffman have made arguments like, well, there's no reason

534
00:29:42.689 --> 00:29:46.060
to think that our perception needs to accurately track

535
00:29:46.069 --> 00:29:48.520
the world, then all it needs to do is

536
00:29:48.530 --> 00:29:52.219
promote our fitness. And so it could be the

537
00:29:52.229 --> 00:29:55.780
case that it promotes our fitness in ways that,

538
00:29:56.050 --> 00:30:00.329
um, but by, you know, creating perceptual categories or

539
00:30:00.339 --> 00:30:04.099
perceptual experiences that don't sort of glom onto the

540
00:30:04.109 --> 00:30:07.050
stuff that's really there, the structures that really exist.

541
00:30:07.390 --> 00:30:11.739
Uh Now I think that, and then, and he

542
00:30:11.750 --> 00:30:13.520
says for that reason, we have no reason to

543
00:30:13.530 --> 00:30:16.800
think that our sort of perception and our perceptual

544
00:30:16.810 --> 00:30:20.040
experiences is anything like what the world is like

545
00:30:20.050 --> 00:30:21.959
or tells us about what the world is like.

546
00:30:22.290 --> 00:30:26.479
Um SO he uses models to do that. I

547
00:30:26.489 --> 00:30:29.839
use similar models to make a different argument, which

548
00:30:29.849 --> 00:30:33.109
is something like, uh, in fact, there's plenty of

549
00:30:33.119 --> 00:30:35.119
reason to often think that the structure of the

550
00:30:35.130 --> 00:30:37.959
world is really important in determining how our fitness

551
00:30:37.969 --> 00:30:41.430
is going to look like. Um, THAT there's at

552
00:30:41.439 --> 00:30:43.589
least going to be some reasons why our perception

553
00:30:43.599 --> 00:30:48.439
needs to tell us veridical or world tracking things

554
00:30:48.449 --> 00:30:52.000
in order for us to survive. And so making

555
00:30:52.010 --> 00:30:54.219
the jump to, like, our perception has nothing to

556
00:30:54.229 --> 00:30:56.050
do with the world seems like much too strong

557
00:30:56.060 --> 00:30:57.939
of a jump to me for that reason.

558
00:30:59.060 --> 00:31:01.949
Yeah, I, I've in fact already had, uh, Doctor

559
00:31:01.959 --> 00:31:04.839
Hoffman twice on the show and the, yes, he,

560
00:31:04.849 --> 00:31:09.140
he has at least some compelling arguments to support

561
00:31:09.150 --> 00:31:12.359
his position. But, uh, I mean, since you come

562
00:31:12.369 --> 00:31:16.010
from, uh, I'm not sure if you would label

563
00:31:16.020 --> 00:31:18.660
it the opposite place or at, but at least

564
00:31:18.670 --> 00:31:21.750
a different place. Uh, HOW do you look at

565
00:31:21.760 --> 00:31:23.790
it exactly? Do you look at it through a

566
00:31:23.800 --> 00:31:27.819
pragmatic lens, for example, since you're also coming at

567
00:31:27.829 --> 00:31:29.900
it from an evolutionary perspective.

568
00:31:31.359 --> 00:31:35.439
Yeah. So um I do think of perception through

569
00:31:35.449 --> 00:31:38.400
this kind of pragmatic lens and like an evolutionary

570
00:31:38.410 --> 00:31:41.099
lens where I assume, you know, there's some things,

571
00:31:41.109 --> 00:31:45.250
some structures in the world, uh there are actions

572
00:31:45.260 --> 00:31:47.630
that would be better or worse for organisms to

573
00:31:47.640 --> 00:31:50.180
take in the presence of these different structures. So

574
00:31:50.189 --> 00:31:53.329
for example, if humans are in the presence of

575
00:31:53.339 --> 00:31:56.510
blackberries eating is a good action. If they're in

576
00:31:56.520 --> 00:31:59.900
the presence of uh juniper berries eating is not

577
00:31:59.910 --> 00:32:03.459
such a good action. Um So there are ways

578
00:32:03.469 --> 00:32:06.250
in which the structure of the world impacts like

579
00:32:06.260 --> 00:32:08.469
our fitness based on how we act. So I

580
00:32:08.479 --> 00:32:12.390
use models that assume that and then, um I

581
00:32:12.400 --> 00:32:14.650
ask in these models, well, what, what will our

582
00:32:14.660 --> 00:32:18.150
perceptual categories look like? And there are certain kinds

583
00:32:18.160 --> 00:32:20.849
of things that will often be the case. So

584
00:32:20.859 --> 00:32:25.670
part of um Dawn's argument is that, for example,

585
00:32:26.219 --> 00:32:30.300
uh you know, there might be things in the

586
00:32:30.310 --> 00:32:32.060
world that are different from each other and we

587
00:32:32.069 --> 00:32:33.849
should respond to them in the same way and

588
00:32:33.859 --> 00:32:37.750
then we can categorize them together. So maybe something

589
00:32:37.979 --> 00:32:41.459
like a lower level animal, say a dragon fly

590
00:32:41.829 --> 00:32:44.699
can have the same perceptual experience for many different

591
00:32:44.709 --> 00:32:47.329
types of food because all they need to do

592
00:32:47.339 --> 00:32:50.079
in response to that food is eat. And so

593
00:32:50.089 --> 00:32:54.349
their reception doesn't have to disambiguate those things um,

594
00:32:54.359 --> 00:32:56.579
and he would say, well then their perception isn't

595
00:32:56.589 --> 00:33:00.239
tracking the world. My response would be something like

596
00:33:00.250 --> 00:33:02.170
there's ways in which it's not tracking the world

597
00:33:02.180 --> 00:33:04.219
in ways in which it is, you know, it

598
00:33:04.229 --> 00:33:06.810
is putting whatever all the, I don't know what

599
00:33:06.819 --> 00:33:09.359
Dragonflies eat, all of that kind of little bug

600
00:33:09.369 --> 00:33:12.760
into the same edible category. Right. And all of

601
00:33:12.770 --> 00:33:14.709
this kind of bug are in the same category.

602
00:33:14.770 --> 00:33:19.040
So it's tracking sameness among those different food sources.

603
00:33:19.050 --> 00:33:21.719
It's failing to track difference between them. But that

604
00:33:21.729 --> 00:33:24.500
doesn't mean there's no sort of important correspondence between

605
00:33:24.510 --> 00:33:27.030
perception and the world. Mhm

606
00:33:28.020 --> 00:33:30.880
So uh I would like to ask you now

607
00:33:30.890 --> 00:33:38.750
about learning generalization and evolutionarily stable strategy. So, and

608
00:33:38.760 --> 00:33:43.859
to compare them as models for learning basically. So

609
00:33:43.869 --> 00:33:46.800
uh first of all, tell us what is learning

610
00:33:47.079 --> 00:33:48.310
generalization.

611
00:33:49.619 --> 00:33:54.439
Yeah. So this is a completely like ubiquitous, widespread

612
00:33:54.449 --> 00:33:59.270
learning behavior where um so an organism encounters something

613
00:33:59.280 --> 00:34:01.949
in the world, it learns something about it. So

614
00:34:01.959 --> 00:34:06.469
maybe it learns like touching good eating good, avoiding

615
00:34:06.479 --> 00:34:12.909
good, whatever it learns. Um But that learned response

616
00:34:12.918 --> 00:34:15.179
is not just applied to that single state that

617
00:34:15.188 --> 00:34:19.070
the organism encountered, but to many similar states. So

618
00:34:19.080 --> 00:34:21.149
if I try eating a blackberry and it tastes

619
00:34:21.159 --> 00:34:23.649
good, I don't learn to eat only that exact

620
00:34:23.659 --> 00:34:26.750
blackberry with that exact weight, that exact color, that

621
00:34:26.760 --> 00:34:29.389
exact smell, that exact location I learned to eat

622
00:34:29.399 --> 00:34:33.728
blackberries in general. Um And that sounds like, well,

623
00:34:33.739 --> 00:34:36.358
Yeah, obviously, duh. Right. Of course, you should do

624
00:34:36.368 --> 00:34:39.398
that, but it's actually a nontrivial task, you know,

625
00:34:39.697 --> 00:34:42.138
uh when you expand out from that first initial

626
00:34:42.148 --> 00:34:44.947
stimuli, how many things do you include, like, how

627
00:34:44.958 --> 00:34:47.768
dissimilar, how similar, what counts as something that you

628
00:34:47.779 --> 00:34:51.059
ought to eat in response to this initial learning

629
00:34:51.069 --> 00:34:55.708
experience? Um So that's what learning generalization is. Uh

630
00:34:55.717 --> 00:34:57.938
Do you want me to talk about like why

631
00:34:57.948 --> 00:35:01.780
it's interesting from the perspective of like, OK, so

632
00:35:02.459 --> 00:35:06.790
there's this classic argument in evolutionary game theory. So

633
00:35:06.800 --> 00:35:11.219
the study of the evolution of strategic behaviors uh

634
00:35:11.229 --> 00:35:13.879
having to do with learning. So when you have

635
00:35:13.889 --> 00:35:18.060
these strategic situations, there are certain kinds of behaviors

636
00:35:18.300 --> 00:35:21.709
that are um are going to be the ones

637
00:35:21.719 --> 00:35:24.810
you expect to evolve. Basically, the ones that can

638
00:35:24.820 --> 00:35:32.120
be stable in an evolutionary scenario. And the argument

639
00:35:32.439 --> 00:35:35.250
is something like this, you should only expect the

640
00:35:35.260 --> 00:35:38.719
selection of learning that will learn those ideal strategies.

641
00:35:39.570 --> 00:35:42.350
Because if you have a learner who doesn't learn

642
00:35:42.360 --> 00:35:45.010
those ideal strategies, then they're gonna get a lower

643
00:35:45.020 --> 00:35:47.510
payoff than the ones who do, then they should

644
00:35:47.520 --> 00:35:51.389
be selected against. So expect organisms to learn these

645
00:35:51.399 --> 00:35:57.790
stable strategies in these strategic scenarios. But, and you

646
00:35:57.800 --> 00:36:00.330
know, I've done work arguing this other people have

647
00:36:00.340 --> 00:36:02.949
as well. But the issue with that argument is

648
00:36:02.959 --> 00:36:06.530
that there are tradeoffs and benefits in learning. So

649
00:36:06.540 --> 00:36:09.010
learning doesn't happen all in one second. It takes

650
00:36:09.020 --> 00:36:11.600
time and you're getting payoffs over the entire time

651
00:36:11.610 --> 00:36:15.530
that that's happening. So learning generalization is a behavior

652
00:36:15.540 --> 00:36:19.929
that doesn't lead to these ideal evolutionarily stable strategies.

653
00:36:19.939 --> 00:36:23.379
These perfect strategies basically because it leads you to

654
00:36:23.389 --> 00:36:28.340
extend learning to situations where you didn't uh experience

655
00:36:28.350 --> 00:36:32.629
the um the first learning experience, right to new

656
00:36:32.639 --> 00:36:35.449
situations. And so you extend your learning to situations

657
00:36:35.459 --> 00:36:37.429
where it might not be quite as good as

658
00:36:37.439 --> 00:36:39.620
the ones where you learned it. So I use

659
00:36:39.629 --> 00:36:41.850
models to show that this is the case learning

660
00:36:41.860 --> 00:36:44.570
generalization isn't going to fit with this kind of

661
00:36:44.580 --> 00:36:49.129
classic argument about learning, but very clearly, it has

662
00:36:49.139 --> 00:36:53.290
evolved. And the reason is that it helps you

663
00:36:53.300 --> 00:36:55.800
learn faster. So imagine if you had to learn

664
00:36:55.810 --> 00:36:59.600
the right behavior in response to every single blackberry

665
00:36:59.610 --> 00:37:04.520
you ever saw individually. Well, you're not learning, you're

666
00:37:04.530 --> 00:37:07.090
not really learning at all, but in any case,

667
00:37:07.100 --> 00:37:10.360
you're learning very, very, very slowly. And so you're

668
00:37:10.370 --> 00:37:13.010
gonna have all these times in your lifetime where

669
00:37:13.020 --> 00:37:15.489
you could have extended a lesson to a useful

670
00:37:15.500 --> 00:37:18.239
scenario, but you didn't. And so the idea is

671
00:37:18.250 --> 00:37:20.189
that selection is going to account for that need

672
00:37:20.199 --> 00:37:23.110
for speed, even if this kind of speedy learning

673
00:37:23.120 --> 00:37:26.020
behavior necessarily stops you from learning kind of most

674
00:37:26.030 --> 00:37:27.850
perfect precise thing you could.

675
00:37:28.770 --> 00:37:35.110
Mhm But does that mean then that learning generalization

676
00:37:35.139 --> 00:37:41.110
uh contradicts evolutionarily stable strategies as a model for

677
00:37:41.120 --> 00:37:44.989
learning or does it implement in any way?

678
00:37:45.760 --> 00:37:49.870
Well, so OK, so evolutionarily stable strategies, they are

679
00:37:50.139 --> 00:37:53.320
um outcomes in a game that you expect to

680
00:37:53.330 --> 00:37:57.129
evolve. So that's a little bit separate from learning.

681
00:37:57.639 --> 00:37:59.669
It's not the case that in fact, they always

682
00:37:59.679 --> 00:38:02.459
evolve even in evolutionary models, but that's a whole

683
00:38:02.469 --> 00:38:06.719
separate thing. Um So they're the sort of behaviors

684
00:38:06.729 --> 00:38:09.020
you expect to evolve. But you can also ask

685
00:38:09.030 --> 00:38:13.620
what learning behaviors get selected for. And so the

686
00:38:13.629 --> 00:38:16.500
argument is about, will you see learning behaviors that

687
00:38:16.510 --> 00:38:19.780
always lead to those evolutionarily stable strategies or will

688
00:38:19.790 --> 00:38:22.899
you see other learning behaviors? And you know, the

689
00:38:22.909 --> 00:38:25.699
argument is you'll see other learning behaviors because they're

690
00:38:25.709 --> 00:38:29.739
faster or have other kinds of benefits besides getting

691
00:38:29.750 --> 00:38:31.060
to the perfect strategies?

692
00:38:31.709 --> 00:38:35.040
Mhm So uh the last topic I would like

693
00:38:35.050 --> 00:38:40.780
to get into today is misinformation during and about

694
00:38:40.790 --> 00:38:46.000
the COVID-19 pandemic. So, uh first of all, what

695
00:38:46.010 --> 00:38:48.459
do you think uh what were the questions that

696
00:38:48.469 --> 00:38:52.350
you found more interesting uh about this topic from

697
00:38:52.360 --> 00:38:55.739
coming from the perspective of the perspective of game

698
00:38:55.750 --> 00:38:56.219
theory?

699
00:38:58.679 --> 00:39:02.820
Um Well, so I thought so not just coming

700
00:39:02.830 --> 00:39:04.979
from the perspective of game theory. So a lot

701
00:39:04.989 --> 00:39:08.659
of my work on um misinformation that isn't in

702
00:39:08.669 --> 00:39:12.100
game theory but in other kinds of modeling. Uh

703
00:39:12.620 --> 00:39:15.159
But so there were a lot of things that

704
00:39:15.169 --> 00:39:18.399
happened during the pandemic that were like really interesting

705
00:39:18.409 --> 00:39:21.800
from the point of view of thinking about misinformation,

706
00:39:21.810 --> 00:39:24.780
spread of beliefs, social spread of beliefs, all this

707
00:39:24.790 --> 00:39:29.219
stuff. So here was one thing. Um So Jim

708
00:39:29.459 --> 00:39:31.860
Weall and I published this book in 2019, the

709
00:39:31.870 --> 00:39:34.699
misinformation Age where one of the things we argued

710
00:39:34.929 --> 00:39:40.199
is that in cases where false beliefs really hurt

711
00:39:40.209 --> 00:39:42.429
people, they should be less likely to hold those

712
00:39:42.439 --> 00:39:46.840
false beliefs. So for example, if everyone is like,

713
00:39:46.850 --> 00:39:50.679
oh Red Kool Aid is the most delicious. It's

714
00:39:50.689 --> 00:39:53.929
even if that were somehow not true, uh It's

715
00:39:53.939 --> 00:39:56.290
totally fine for me to conform with everyone and

716
00:39:56.300 --> 00:39:59.370
drink Red Kool Aid or to trust everyone in

717
00:39:59.379 --> 00:40:01.780
saying, say they said it was nutritious to trust

718
00:40:01.790 --> 00:40:04.209
everyone and saying it was nutritious. It doesn't matter.

719
00:40:04.219 --> 00:40:06.000
It's not gonna hurt me if I drink Red

720
00:40:06.010 --> 00:40:07.870
Kool Aid, right? It's just not that bad for

721
00:40:07.879 --> 00:40:10.489
me, even if this belief was a little wrong.

722
00:40:10.760 --> 00:40:14.010
On the other hand, if everyone was like, oh

723
00:40:14.310 --> 00:40:18.250
it's safest to drink cyanide, we shouldn't see that

724
00:40:18.260 --> 00:40:22.739
false belief spreading, right? We shouldn't see everyone starting

725
00:40:22.750 --> 00:40:24.870
to drink that because it really matters to people.

726
00:40:24.879 --> 00:40:28.780
If they get it wrong. Now with COVID, I

727
00:40:28.790 --> 00:40:31.449
would have thought before the pandemic that this would

728
00:40:31.459 --> 00:40:33.929
be a scenario. That's more like the latter case

729
00:40:33.939 --> 00:40:38.090
because you could make choices during the pandemic where

730
00:40:38.100 --> 00:40:41.010
if you make that choice today, it directly leads

731
00:40:41.020 --> 00:40:44.830
to your death in 24 days, right? You just

732
00:40:44.840 --> 00:40:49.199
die in a couple weeks and in particular exposure,

733
00:40:49.209 --> 00:40:52.409
choices could put people at risk of death just

734
00:40:52.419 --> 00:40:55.689
very directly and there's a pretty high fatality rate

735
00:40:55.699 --> 00:41:00.260
from COVID, but you still saw all of these

736
00:41:00.270 --> 00:41:03.820
kinds of social effects on belief and misinformation related

737
00:41:03.830 --> 00:41:08.689
to COVID. In spite of its very prominent, like

738
00:41:08.699 --> 00:41:12.270
visible real world danger to people, you still saw

739
00:41:12.280 --> 00:41:16.550
people choosing to be exposed based on partisan identity,

740
00:41:16.560 --> 00:41:20.090
for example, um or choosing whether or not to

741
00:41:20.100 --> 00:41:22.750
wear masks based on their social identities or whether

742
00:41:22.760 --> 00:41:25.409
they were conforming with other people in their social

743
00:41:25.419 --> 00:41:28.149
networks. Uh So I thought that that was very

744
00:41:28.159 --> 00:41:31.129
interesting and surprised me quite a lot actually.

745
00:41:32.250 --> 00:41:35.580
And, uh I mean, how do you make sense

746
00:41:35.590 --> 00:41:36.750
of it then?

747
00:41:37.739 --> 00:41:39.850
I mean, I guess the way I make sense

748
00:41:39.860 --> 00:41:44.469
of it is thinking that like, you know, social

749
00:41:44.479 --> 00:41:47.780
pressures are just stronger and more important to people

750
00:41:47.790 --> 00:41:50.879
than maybe I initially would have thought. And they're

751
00:41:50.889 --> 00:41:53.280
just, you know, a whole thesis of this book

752
00:41:53.290 --> 00:41:56.389
was that like, social stuff matters to belief a

753
00:41:56.399 --> 00:41:58.899
lot. And I feel like, oh, I, I guess

754
00:41:58.909 --> 00:42:01.719
that was right, even more right than I expected,

755
00:42:01.729 --> 00:42:09.899
you know, um, that people would be influenced by

756
00:42:09.909 --> 00:42:12.340
these sort of social factors so much that they

757
00:42:12.350 --> 00:42:15.080
would be willing to take fairly significant risks to

758
00:42:15.090 --> 00:42:15.530
their home.

759
00:42:16.850 --> 00:42:20.139
But, I mean, from the perspective, you've, you've used

760
00:42:20.149 --> 00:42:25.620
to tackle this topic, uh do these beliefs that

761
00:42:25.629 --> 00:42:32.580
are epistemic wrong, I guess, uh still serve uh

762
00:42:32.689 --> 00:42:38.909
particular functions, namely social function functions. And is that

763
00:42:38.919 --> 00:42:42.929
the main reason why they spread, even though they

764
00:42:42.939 --> 00:42:47.510
might be harmful to people who believe them and

765
00:42:47.520 --> 00:42:48.350
spread them

766
00:42:49.439 --> 00:42:53.260
Yeah, that's right. They often can. So um this

767
00:42:53.270 --> 00:42:55.560
is something that I haven't exactly model but, you

768
00:42:55.570 --> 00:42:58.169
know, I've looked at other people's work on one

769
00:42:58.179 --> 00:43:02.620
thing that beliefs do is play roles as far

770
00:43:02.629 --> 00:43:06.770
as in group out group signaling, communicating your social

771
00:43:06.780 --> 00:43:11.800
identity, bonding with other people, impressing other people. So

772
00:43:11.810 --> 00:43:14.860
holding beliefs and sharing beliefs and taking actions based

773
00:43:14.870 --> 00:43:18.739
on belief, it all has functions, you know, with

774
00:43:18.750 --> 00:43:21.560
respect to like our bodies and their relation to

775
00:43:21.570 --> 00:43:24.620
the world, but it also has functions with respect

776
00:43:24.629 --> 00:43:27.929
to us and our social relationships. So for example,

777
00:43:27.939 --> 00:43:30.219
I might put on a mega hat to signal

778
00:43:30.229 --> 00:43:32.179
to the people. I want to signal to I'm

779
00:43:32.189 --> 00:43:35.419
part of your in group and they will assume

780
00:43:35.429 --> 00:43:37.820
in my wearing that hat that I also hold

781
00:43:37.850 --> 00:43:40.379
various sorts of beliefs about the world and how

782
00:43:40.389 --> 00:43:41.020
it works.

783
00:43:42.679 --> 00:43:47.209
So do you think that if we uh get

784
00:43:47.219 --> 00:43:51.379
a good understanding of how all of these works?

785
00:43:51.389 --> 00:43:54.620
I mean, why people hold certain beliefs and why

786
00:43:54.629 --> 00:43:58.399
they communicate them to other people that perhaps we

787
00:43:58.409 --> 00:44:02.620
could, could use this knowledge to tackle misinformation?

788
00:44:04.040 --> 00:44:08.699
I certainly think it can help. Um So for

789
00:44:08.709 --> 00:44:12.780
example, understanding online platforms and how the spread of

790
00:44:12.790 --> 00:44:15.780
belief works on online platforms, I think can help

791
00:44:15.790 --> 00:44:20.719
those platforms develop better information environments or better algorithms

792
00:44:20.729 --> 00:44:23.139
to protect people. So just to give a little

793
00:44:23.149 --> 00:44:26.500
example, again, not my research, other people's research, but

794
00:44:26.790 --> 00:44:30.610
um it seems like emotional language has a big

795
00:44:30.620 --> 00:44:34.590
impact on how wide widely spread things like tweets

796
00:44:34.600 --> 00:44:39.429
on Twitter are. Uh And so knowing that can

797
00:44:39.439 --> 00:44:42.850
help a platform realize, OK, if you have emotional

798
00:44:42.860 --> 00:44:47.389
language attached to say misinformation, um maybe that's the

799
00:44:47.399 --> 00:44:51.590
sort of thing the algorithm shouldn't actively promote. And

800
00:44:51.600 --> 00:44:53.909
so that's an example of knowing something about how

801
00:44:53.919 --> 00:44:58.379
humans work, possibly helping to improve information spread in

802
00:44:58.389 --> 00:44:59.239
a social group.

803
00:45:00.010 --> 00:45:03.669
Mhm. Yeah. And perhaps, I guess that holding certain

804
00:45:03.679 --> 00:45:08.620
very influential people accountable for what they say, like

805
00:45:08.629 --> 00:45:12.590
for example, Donald Trump would also help a little

806
00:45:12.600 --> 00:45:12.949
bit.

807
00:45:13.659 --> 00:45:16.110
Yeah. Ok.

808
00:45:17.060 --> 00:45:20.959
Yeah. Uh OK. So Doctor o'connor, uh just before

809
00:45:20.969 --> 00:45:23.540
we go, would you like to tell people where

810
00:45:23.550 --> 00:45:25.540
they can find you and your work on the

811
00:45:25.550 --> 00:45:26.060
internet

812
00:45:27.419 --> 00:45:30.239
there? I have a website. It's just my name

813
00:45:30.250 --> 00:45:33.040
Kelly o'connor dot com. I put up all my

814
00:45:33.050 --> 00:45:36.239
preprinted papers there. So any research that I've done

815
00:45:36.250 --> 00:45:40.239
can be found there except my books. Um AND

816
00:45:40.250 --> 00:45:43.229
I'm also on Twitter. It's Ken Meister is my

817
00:45:43.239 --> 00:45:48.360
handle like my name and then Meist er my

818
00:45:48.370 --> 00:45:50.360
dad used to call me the Ken Meister and

819
00:45:50.370 --> 00:45:52.219
then when I became, you know, when I actually

820
00:45:52.229 --> 00:45:55.530
got a master's degree, like I thought it would

821
00:45:55.540 --> 00:45:56.070
be funny.

822
00:45:56.360 --> 00:46:00.750
Yeah, it totally makes sense. Ok, so uh I'm

823
00:46:00.760 --> 00:46:03.330
leaving links to that in the description box of

824
00:46:03.340 --> 00:46:06.459
the interview and Doctor o'connor, thank you so much

825
00:46:06.469 --> 00:46:08.500
again for taking the time to come on the

826
00:46:08.510 --> 00:46:10.780
show. It's been really fun to talk to you.

827
00:46:11.409 --> 00:46:14.620
Yeah, nice talking to you, Ricardo and hopefully we'll

828
00:46:14.629 --> 00:46:16.149
catch each other around sometime.

829
00:46:17.129 --> 00:46:20.129
Hi guys. Thank you for watching this interview. Until

830
00:46:20.139 --> 00:46:22.629
the end. If you liked it, please do not

831
00:46:22.639 --> 00:46:27.889
forget to like it, share, comment and subscribe. And

832
00:46:27.899 --> 00:46:31.100
if you like more generally, what I'm doing, please

833
00:46:31.110 --> 00:46:36.209
consider support the show on Patreon or paypal. You

834
00:46:36.219 --> 00:46:38.479
have all of the links in the description of

835
00:46:38.489 --> 00:46:41.500
this interview. This show is brought to you by

836
00:46:41.510 --> 00:46:45.429
En Lights learning and development. Done differently. Check their

837
00:46:45.439 --> 00:46:49.189
website at alights.com. I would also like to give

838
00:46:49.199 --> 00:46:51.479
a huge thank you to my main patrons and

839
00:46:51.489 --> 00:46:56.110
paypal supporters per Larson, Jerry Mueller and Frederick Sunda

840
00:46:56.280 --> 00:47:00.719
Bernards O of Election and Visor Adam Castle Matthew

841
00:47:00.729 --> 00:47:04.050
Whit Whitting Bear, no wolf, Tim Hollis, Eric Alania,

842
00:47:04.260 --> 00:47:08.199
John Connors Philip Forrest Connelly, Robert Winde Nai Z

843
00:47:08.689 --> 00:47:12.689
Mark Nevs Colin Holbrook, Simon Columbus, Phil Kor Michael

844
00:47:12.699 --> 00:47:17.584
Stormer, Samuel Andreev for the S Alexander Dan Bauer

845
00:47:17.604 --> 00:47:21.764
Fergal Ken Hall, her og Michel Jonathan lebron Jars

846
00:47:22.415 --> 00:47:26.495
and the Samuel K, Eric Heins Mark Smith Jan

847
00:47:26.695 --> 00:47:30.405
We Amal S Franz David Sloan Wilson, Yasa, Des

848
00:47:31.405 --> 00:47:36.415
Roma Roach Diego, Jannik Punter, Da Man Charlotte Bliss,

849
00:47:36.445 --> 00:47:40.405
Nicole Barbar Wam and Pao Assy Naw Guy Madison,

850
00:47:40.415 --> 00:47:44.110
Gary GH, some of the Adrian Yin Nick Golden

851
00:47:44.120 --> 00:47:46.870
Paul talent in Ju Bar was Julian Price Edward

852
00:47:46.879 --> 00:47:50.800
Hall, Eden Bronner, Douglas Fry Franca Bertolotti, Gabriel Pan

853
00:47:51.169 --> 00:47:56.199
Cortez, Lelis Scott Zachary Fish, Tim Duffy, Sonny Smith,

854
00:47:56.209 --> 00:48:01.250
John Wiesman, Martin Aland, Daniel Friedman. William Buckner, Paul

855
00:48:01.260 --> 00:48:05.010
George Arnold Luke Lo A Georges the often Chris

856
00:48:05.020 --> 00:48:10.669
Williamson, Peter Oren, David Williams the Costa Anton Erickson

857
00:48:10.679 --> 00:48:16.879
Charles Murray, Alex Shaw and Murray Martinez Chevalier, Bangalore

858
00:48:17.199 --> 00:48:22.310
atheists, Larry Daley Junior Holt Eric B. Starry Michael

859
00:48:22.320 --> 00:48:26.860
Bailey, then Sperber, Robert Grassy Rough the RP MD,

860
00:48:27.340 --> 00:48:33.149
Ior Jeff mcmahon, Jake Zul Barnabas Radix, Mark Campbell,

861
00:48:33.159 --> 00:48:37.379
Richard Bowen Thomas, the Dubner, Luke Ni and Greece

862
00:48:37.389 --> 00:48:42.479
story, Manuel Oliveira, Kimberly Johnson and Benjamin Gilbert. A

863
00:48:42.629 --> 00:48:46.120
special thanks to my producers is our web gem

864
00:48:46.129 --> 00:48:50.500
Frank Luca Stefi, Tom Weam Bernard ni Ortiz Dixon,

865
00:48:50.580 --> 00:48:54.709
Benedict Mueller Vege, Gli Thomas Trumble, Catherine and Patrick

866
00:48:54.919 --> 00:48:59.439
Tobin, John Carlo Montenegro, Robert Lewis and Al Nick

867
00:48:59.449 --> 00:49:03.350
Ortiz. And to my executive producers, Matthew Lavender, Si

868
00:49:03.939 --> 00:49:06.479
Adrian and Bogdan Kut. Thank you for all.

