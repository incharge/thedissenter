WEBVTT

1
00:00:00.430 --> 00:00:03.129
Hello everybody. Welcome to a new episode of the

2
00:00:03.140 --> 00:00:05.658
Decent. I'm your host, Ricardo Lops. And

3
00:00:05.669 --> 00:00:09.130
today I'm joined by Katerina Kovacevic. She is a

4
00:00:09.140 --> 00:00:13.050
phd candidate in the Department of Cognitive Science at Central

5
00:00:13.060 --> 00:00:17.458
European University. Her main research interest is responsibility.

6
00:00:17.469 --> 00:00:21.929
She investigates how people ascribe responsibility for good and bad

7
00:00:21.940 --> 00:00:27.500
outcomes across various situations. And today we're focusing on

8
00:00:27.579 --> 00:00:32.978
uh responsibility and strategic ignorance. So Karina, welcome

9
00:00:32.990 --> 00:00:34.990
to the show. It's a big pleasure to everyone

10
00:00:35.649 --> 00:00:38.969
. Thank you. It's also a pleasure from OK

11
00:00:38.978 --> 00:00:42.348
. So uh actually, this is very interesting because

12
00:00:42.359 --> 00:00:47.520
uh I've already talked about responsibility with philosophers on the

13
00:00:47.529 --> 00:00:50.520
show, but not with someone who approaches it from

14
00:00:50.529 --> 00:00:55.950
a sort of uh psychological slash cognitive science perspective.

15
00:00:55.959 --> 00:01:00.329
So, uh how do you approach responsibility from that

16
00:01:00.340 --> 00:01:03.548
perspective? And what kinds of questions are you trying

17
00:01:03.560 --> 00:01:07.569
to answer in your research actually? Yeah, that's

18
00:01:07.579 --> 00:01:10.698
a good question to make some kind of a distinction

19
00:01:10.799 --> 00:01:14.049
in the approach of philosophers and psychologists uh to this

20
00:01:14.058 --> 00:01:18.760
type of topics. Um The main difference is that

21
00:01:18.769 --> 00:01:23.379
we in psychology approach from the descriptive perspective, which

22
00:01:23.388 --> 00:01:25.778
means that we don't want to say what is right

23
00:01:25.790 --> 00:01:27.799
, what is wrong how people should ascribe responsibility in

24
00:01:27.808 --> 00:01:32.388
which cases, but rather how people do that,

25
00:01:32.400 --> 00:01:37.969
what are people's moral intuitions? Um And um when

26
00:01:37.980 --> 00:01:41.558
do they feel like it's um um good to,

27
00:01:41.569 --> 00:01:45.469
to um ascribe responsibility, what are some relevant factors

28
00:01:45.480 --> 00:01:48.558
for them that influences their intuitions and their, their

29
00:01:48.569 --> 00:01:53.500
decisions? And um when we talk about the methodology

30
00:01:53.510 --> 00:01:57.000
, so how I approach this um is uh mostly

31
00:01:57.010 --> 00:02:00.010
through V studies uh which is when you write a

32
00:02:00.019 --> 00:02:06.500
certain story um about some hypothetical agents. And then

33
00:02:06.510 --> 00:02:09.550
you ask the person um some questions about their moral

34
00:02:09.558 --> 00:02:14.288
judgments such as um is the person responsible for the

35
00:02:14.300 --> 00:02:17.288
certain outcome um in the story. And since I'm

36
00:02:17.300 --> 00:02:23.159
interested in um different factors that could influence the description

37
00:02:23.169 --> 00:02:27.139
of responsibility, then I just vary uh the context

38
00:02:27.149 --> 00:02:29.229
of the story, what the agent knew what they

39
00:02:29.240 --> 00:02:31.770
didn't know uh what were their intentions and similar to

40
00:02:31.778 --> 00:02:37.569
see whether that would trigger different intuitions, uh moral

41
00:02:37.580 --> 00:02:42.058
intuitions in, in participants. Um Yeah, and

42
00:02:42.069 --> 00:02:47.118
also what psychologists do um when investigating responsibility and some

43
00:02:47.129 --> 00:02:51.419
other moral judgments is uh especially in my department because

44
00:02:51.429 --> 00:02:55.258
I'm a cognitive scientist, we um discover the processes

45
00:02:55.520 --> 00:03:00.169
behind uh this moral judgment. So what kind of

46
00:03:00.179 --> 00:03:04.909
cognitive process lies behind certain intuitions? In my case

47
00:03:04.919 --> 00:03:07.679
, I talk a lot about counterfactual thinking, for

48
00:03:07.689 --> 00:03:09.979
example, that is a thinking of possible alternatives.

49
00:03:10.000 --> 00:03:13.909
If this didn't happen, then something else wouldn't have

50
00:03:13.919 --> 00:03:15.788
happened. Um So yeah, that's like a,

51
00:03:15.800 --> 00:03:22.258
a crash course on what um psychologist does in uh

52
00:03:22.270 --> 00:03:24.319
in this type of a field. Yeah. OK

53
00:03:24.330 --> 00:03:28.699
. So let's get into some of your results and

54
00:03:28.710 --> 00:03:31.800
what we know about how people ascribe responsibility to others

55
00:03:31.808 --> 00:03:35.599
in different kinds of situations and so on. So

56
00:03:35.729 --> 00:03:40.460
when exactly do people as ascribe responsibility to others?

57
00:03:42.719 --> 00:03:46.028
Um So I will start first from a more bro

58
00:03:46.038 --> 00:03:49.750
broad answer. So, um when I talk about

59
00:03:49.758 --> 00:03:52.558
responsibility, I, I talk mostly in my current

60
00:03:52.569 --> 00:03:57.149
research about responsibility for the negative outcome. Um Then

61
00:03:57.159 --> 00:04:02.189
I see um which factors influence um people to ascribe

62
00:04:02.199 --> 00:04:05.219
responsibility more or less likely. So what are some

63
00:04:05.229 --> 00:04:10.669
kind of a mitigating factors in ascribing responsibility? And

64
00:04:10.868 --> 00:04:14.949
what I be shown by now is that um something

65
00:04:14.960 --> 00:04:17.088
that has been already known um to an extent that

66
00:04:17.100 --> 00:04:20.980
epistemic states a relevant factor, so whether the agent

67
00:04:20.988 --> 00:04:25.040
uh knew what would be the possible consequences of their

68
00:04:25.048 --> 00:04:28.709
actions or they didn't know. So some kind of

69
00:04:28.720 --> 00:04:31.699
beliefs about the situation and what could happen, the

70
00:04:31.709 --> 00:04:38.420
level of knowledge and information. Um And um moreover

71
00:04:38.428 --> 00:04:40.410
, which is not a factor that I, that

72
00:04:40.420 --> 00:04:42.720
I provide in my studies, but it is important

73
00:04:42.730 --> 00:04:46.278
to mention is um intentions that they don't have.

74
00:04:46.759 --> 00:04:50.278
Um So if you have desire to harm the others

75
00:04:50.290 --> 00:04:53.678
, uh then it's more likely that you will be

76
00:04:53.689 --> 00:04:56.949
ascribed as responsible uh for uh some kind of a

77
00:04:56.959 --> 00:04:59.769
bad outcome. But in my research, I mostly

78
00:04:59.778 --> 00:05:03.829
focus on um this peculiar case of agents, uh

79
00:05:04.230 --> 00:05:08.670
, who could have had the knowledge, um,

80
00:05:08.678 --> 00:05:13.170
to see how people ascribe responsibility to them, um

81
00:05:13.319 --> 00:05:15.629
, in, um, so to see how people

82
00:05:15.639 --> 00:05:17.209
react to ignorant agents and how they, uh,

83
00:05:17.220 --> 00:05:19.910
distinguish them from the people who have the knowledge.

84
00:05:20.290 --> 00:05:24.069
Um, yeah, because it's, it's been shown

85
00:05:24.509 --> 00:05:27.569
like, um, that when you have the knowledge

86
00:05:27.670 --> 00:05:30.819
you're more likely to be responsible but the fact that

87
00:05:30.829 --> 00:05:35.528
you're ignorant is not always escalating. So that's interesting

88
00:05:35.540 --> 00:05:39.850
to see when it is and when it isn't exculpating

89
00:05:40.069 --> 00:05:44.928
. Yeah. So you're mostly interested here in epistemic

90
00:05:44.939 --> 00:05:48.309
responsibility, correct? Yeah, but just a bit

91
00:05:48.319 --> 00:05:51.750
of a disclaimer. Um I, I'm pretty sure

92
00:05:51.759 --> 00:05:55.988
that philosophers will have a different definition of what epistemic

93
00:05:56.000 --> 00:05:59.588
responsibility is. Uh whether there's a duty to know

94
00:05:59.600 --> 00:06:03.350
enough about before forming certain attitudes, how I operationalize

95
00:06:03.480 --> 00:06:08.790
epistemic responsibility in my case is more something like epistemic

96
00:06:08.798 --> 00:06:13.389
duty, which is a duty to get informed before

97
00:06:13.399 --> 00:06:17.048
acting. Um And that means that if you have

98
00:06:17.059 --> 00:06:23.178
a certain sufficient care for other people, you,

99
00:06:23.189 --> 00:06:26.420
there is some kind of a normative expectation that you

100
00:06:26.428 --> 00:06:30.298
would take certain precautionary measures or certain actions to get

101
00:06:30.309 --> 00:06:35.000
informed um before uh doing something that could hire mothers

102
00:06:35.009 --> 00:06:39.750
uh or similar. So that is what I I

103
00:06:39.759 --> 00:06:43.629
talk about when I talk about epistemic responsibility. So

104
00:06:43.639 --> 00:06:46.528
basically that there is um when you have sufficient care

105
00:06:46.540 --> 00:06:50.088
for others, you have epistemic intention, which is

106
00:06:50.100 --> 00:06:55.769
intention to learn the relevant information that leads to checking

107
00:06:56.028 --> 00:06:59.939
and that leads to preventing the negative outcome from happening

108
00:06:59.949 --> 00:07:02.449
because now you have the relevant adequate knowledge for that

109
00:07:03.000 --> 00:07:08.369
. So let's get a little bit into more detail

110
00:07:08.379 --> 00:07:13.738
here. So what if people act without relevant or

111
00:07:13.750 --> 00:07:16.428
sufficient knowledge in their possession? I mean, are

112
00:07:16.439 --> 00:07:20.079
they, are they held responsible or not or are

113
00:07:20.088 --> 00:07:26.028
they held responsible in particular situations and not others?

114
00:07:26.040 --> 00:07:29.608
How does it work? Exactly? Well, as

115
00:07:29.619 --> 00:07:31.048
uh my b A professor, like to say,

116
00:07:31.059 --> 00:07:34.100
uh the best answer is it depends. Uh but

117
00:07:34.108 --> 00:07:36.699
now I will, I will explain why it depends

118
00:07:38.129 --> 00:07:41.139
. Um So what we know by now is that

119
00:07:41.149 --> 00:07:43.129
in a lot of cases, as I already mentioned

120
00:07:43.139 --> 00:07:47.910
to repeat is that uh being knowledgeable um is um

121
00:07:48.009 --> 00:07:51.769
more um being, being more judged than, than

122
00:07:53.019 --> 00:07:57.470
having a lack of knowledge about certain situations. But

123
00:07:57.480 --> 00:08:01.420
the fact that you didn't know about something uh is

124
00:08:01.428 --> 00:08:05.730
not enough. Um in some cases, for example

125
00:08:05.738 --> 00:08:07.209
, you cannot say if you travel to another country

126
00:08:07.220 --> 00:08:11.338
and you didn't buy the transport ticket, you cannot

127
00:08:11.350 --> 00:08:11.769
say, oh, I didn't know how to use

128
00:08:11.778 --> 00:08:15.790
the machine, um which could be the truth that

129
00:08:15.798 --> 00:08:18.269
you didn't know how to exactly buy the ticket,

130
00:08:18.278 --> 00:08:20.509
but it was your, your obligation to get informed

131
00:08:20.519 --> 00:08:24.540
how to do that. Um So um or when

132
00:08:24.548 --> 00:08:28.278
you produce some kind of um uh very bad uh

133
00:08:28.290 --> 00:08:31.389
consequences, for example, you're a doctor and you

134
00:08:31.399 --> 00:08:33.830
didn't check some important allergy when you are prescribed the

135
00:08:33.840 --> 00:08:37.349
, the, the the medication. Um you cannot

136
00:08:37.359 --> 00:08:39.389
say, oh, I didn't know about that because

137
00:08:39.399 --> 00:08:41.869
it was kind of an obligation to get informed about

138
00:08:41.879 --> 00:08:45.629
that. Um But what we show in, in

139
00:08:45.639 --> 00:08:50.279
our research as a very important factor in this case

140
00:08:50.288 --> 00:08:54.440
is whether there was an opportunity to learn the information

141
00:08:54.899 --> 00:08:58.269
that is very important. Um If there was no

142
00:08:58.279 --> 00:09:01.038
opportunity to learn the information, the relevant quant factual

143
00:09:01.048 --> 00:09:05.639
. So this relevant alternative is about some external factors

144
00:09:05.649 --> 00:09:07.808
. So if it would have been possible to learn

145
00:09:07.840 --> 00:09:11.710
the relevant information, then the person would have done

146
00:09:11.719 --> 00:09:15.779
that. But if there was the opportunity to learn

147
00:09:15.788 --> 00:09:18.849
something relevant, but you deliberately chose not to do

148
00:09:18.859 --> 00:09:24.009
that, then you're more likely to be as scrapped

149
00:09:24.019 --> 00:09:30.349
as responsible. Um And concretely in our um experiments

150
00:09:30.840 --> 00:09:33.908
, those agents who uh didn't take the opportunity to

151
00:09:33.918 --> 00:09:39.590
learn that they had were um they ascribe responsibility almost

152
00:09:39.599 --> 00:09:43.259
the same as the agents who knew about the possible

153
00:09:43.269 --> 00:09:48.058
negative consequences. For example, um If uh you

154
00:09:48.070 --> 00:09:52.109
see there is a sign that parking spots are maybe

155
00:09:52.119 --> 00:09:56.759
uh reserved, but you don't really approach to check

156
00:09:56.769 --> 00:10:00.989
it because you're rushing for your dentist appointment um that's

157
00:10:01.000 --> 00:10:05.029
seen as negative and you are described as responsible.

158
00:10:05.090 --> 00:10:09.798
Um statistically even the same as uh as a person

159
00:10:09.808 --> 00:10:13.229
who sees that the the spot is re uh reserved

160
00:10:13.239 --> 00:10:16.779
, but just doesn't do anything about that. So

161
00:10:16.788 --> 00:10:20.349
there is something in this opportunity to learn that people

162
00:10:20.969 --> 00:10:26.058
um take is important. Um We interpret this as

163
00:10:26.070 --> 00:10:33.048
that um people think about um counter factuals that are

164
00:10:33.058 --> 00:10:35.889
related to epistemic intentions. So explain this a bit

165
00:10:35.899 --> 00:10:39.219
more what I mean by this is that people think

166
00:10:39.229 --> 00:10:43.190
whether there would have been uh an agent who had

167
00:10:43.200 --> 00:10:46.750
a better um more sufficient concern for others who would

168
00:10:46.759 --> 00:10:52.619
have done things differently if there is an agent who

169
00:10:52.639 --> 00:10:56.408
uh cared more about the welfare of others. Um

170
00:10:56.418 --> 00:11:01.009
Would he um check if uh if the spot is

171
00:11:01.019 --> 00:11:03.849
reserved and if the answer is yes, then the

172
00:11:03.859 --> 00:11:07.450
person who didn't check if the spot was reserved is

173
00:11:07.460 --> 00:11:11.048
responsible. But then to complicate the story a bit

174
00:11:11.058 --> 00:11:13.109
more, we added some other factors in, in

175
00:11:13.119 --> 00:11:16.950
the question. So now we say, OK,

176
00:11:16.960 --> 00:11:20.928
the opportunity to learn the information is, is relevant

177
00:11:22.259 --> 00:11:24.479
. Um But then we can ask, is a

178
00:11:24.489 --> 00:11:30.960
person always um obliged to use that opportunity to learn

179
00:11:30.969 --> 00:11:35.719
something and maybe intuitively, um we are not expected

180
00:11:35.729 --> 00:11:39.798
to know everything about everything, right? So when

181
00:11:39.808 --> 00:11:43.239
it is very, it is more important uh to

182
00:11:43.250 --> 00:11:46.969
get, to get informed. And one of the

183
00:11:46.979 --> 00:11:48.979
interesting factors that, that, that we write is

184
00:11:50.210 --> 00:11:56.200
how effortful this action of getting the the adequate information

185
00:11:56.210 --> 00:12:01.440
is. So with the hypothesis that if um if

186
00:12:01.450 --> 00:12:05.710
the action is very effortful and too costly for the

187
00:12:05.719 --> 00:12:09.729
agent, uh that will be some mitigating factor,

188
00:12:09.739 --> 00:12:13.168
and uh there will be less expectation from this agent

189
00:12:13.178 --> 00:12:15.619
to engage in the action that is very costly for

190
00:12:15.629 --> 00:12:20.469
them. Um So, um we showed that that

191
00:12:20.479 --> 00:12:24.979
this was the relevant factor and when there were two

192
00:12:24.989 --> 00:12:30.609
agents who both didn't engage um in the um action

193
00:12:30.619 --> 00:12:33.580
of checking, uh the one who needed less effort

194
00:12:33.879 --> 00:12:39.340
uh to do this action um is where as was

195
00:12:39.349 --> 00:12:41.509
described as, as more responsible to give you an

196
00:12:41.519 --> 00:12:45.820
example. Um If you just need to read the

197
00:12:45.830 --> 00:12:48.558
sign uh to check whether the spot is reserved or

198
00:12:48.570 --> 00:12:52.038
not. Um And you didn't engage in such a

199
00:12:52.048 --> 00:12:54.908
simple action that would not be costly for you,

200
00:12:56.590 --> 00:13:00.119
then you are s ascribe responsible. But if this

201
00:13:00.129 --> 00:13:03.428
action involved waiting for a parking attendant to come and

202
00:13:03.440 --> 00:13:05.460
you don't know when he's coming and you're in a

203
00:13:05.469 --> 00:13:09.710
rush because you will um miss your appointment, then

204
00:13:09.719 --> 00:13:15.168
a person, people are a bit um more um

205
00:13:15.178 --> 00:13:20.168
empathizing with the situation and they ascribe uh less responsibility

206
00:13:20.590 --> 00:13:22.808
. So in this case, when they come back

207
00:13:22.820 --> 00:13:26.639
to this counterfactual theory, we can say that even

208
00:13:26.649 --> 00:13:31.359
the agent who had sufficient care for others uh maybe

209
00:13:31.369 --> 00:13:35.529
wouldn't um check in this situation when the costs are

210
00:13:35.538 --> 00:13:37.080
too high. So there is kind of a benefit

211
00:13:37.090 --> 00:13:39.139
of a doubt given to this to this person.

212
00:13:39.149 --> 00:13:41.750
And we can say that the relevant counterfactual becomes,

213
00:13:41.830 --> 00:13:46.369
if the action was not so effortful, then the

214
00:13:46.379 --> 00:13:50.570
agent would have done it. Um Yeah, so

215
00:13:50.950 --> 00:13:54.928
there is some relevant factor. There are some um

216
00:13:54.940 --> 00:13:58.219
other ones, but let's say these, these are

217
00:13:58.229 --> 00:14:00.340
some, some of the most relevant. We also

218
00:14:00.349 --> 00:14:05.250
checked um the weather. Um the probability of the

219
00:14:05.259 --> 00:14:11.349
navigate outcome happening is what is relevant. Um With

220
00:14:11.359 --> 00:14:16.750
, uh with the idea that if it is highly

221
00:14:16.759 --> 00:14:20.678
unlikely that something uh negative would happen, are you

222
00:14:20.690 --> 00:14:24.739
still asked to, to check? Um and to

223
00:14:24.750 --> 00:14:28.340
, to put some uh put some effort in order

224
00:14:28.349 --> 00:14:31.899
to, to check what is uh what is the

225
00:14:31.908 --> 00:14:35.769
relevant information about that event or to prevent? Um

226
00:14:35.428 --> 00:14:41.340
But this factor didn't seem to be uh relevant.

227
00:14:41.849 --> 00:14:43.869
I mean, there may be reasons that uh the

228
00:14:43.879 --> 00:14:48.798
probabilities that we had were still above some threshold,

229
00:14:48.808 --> 00:14:52.129
above which it is expected from people to act.

230
00:14:52.139 --> 00:14:54.710
So if, even if it's like 30 40% probability

231
00:14:54.719 --> 00:14:58.548
of something negative happening, if the action is so

232
00:14:58.558 --> 00:15:01.960
uh effortless, such as just reading the sign,

233
00:15:01.969 --> 00:15:03.279
you're expected to do it if you can prevent some

234
00:15:03.288 --> 00:15:05.798
harm to others. But this factor is still a

235
00:15:05.808 --> 00:15:09.869
bit uh something that, that is, that should

236
00:15:09.879 --> 00:15:13.690
be researched more. Yeah. So, I mean

237
00:15:13.700 --> 00:15:16.408
, uh it's not the case that people always hold

238
00:15:16.418 --> 00:15:20.729
people responsible. If they haven't gathered all the relevant

239
00:15:20.739 --> 00:15:24.070
information, it depends. Uh I mean, a

240
00:15:24.080 --> 00:15:28.190
little bit on how much effort you have to put

241
00:15:28.200 --> 00:15:31.440
into it to gather all the relevant information. But

242
00:15:31.450 --> 00:15:33.879
I would, I mean, in terms of the

243
00:15:33.889 --> 00:15:37.440
effort, I would imagine that there are particular instances

244
00:15:37.450 --> 00:15:41.349
where people, even if it would take a lot

245
00:15:41.359 --> 00:15:46.234
of effort would still hold you responsible. I mean

246
00:15:46.244 --> 00:15:48.514
, just one clear, I guess example that comes

247
00:15:48.524 --> 00:15:52.974
to mind is in health related situations. I mean

248
00:15:52.984 --> 00:15:58.224
, if a doctor for some reason gets across a

249
00:15:58.234 --> 00:16:03.379
patient with a very rare condition and just because it's

250
00:16:03.389 --> 00:16:06.580
very rare and it would take him or her a

251
00:16:06.590 --> 00:16:10.590
lot of uh uh g uh a lot of time

252
00:16:10.599 --> 00:16:14.969
to gather enough information to really understand how that rare

253
00:16:14.979 --> 00:16:18.820
condition works and how it might interfere with treatments and

254
00:16:18.830 --> 00:16:26.090
other uh comorbidities. For example, people probably people

255
00:16:26.099 --> 00:16:27.668
in that specific case because they are dealing with a

256
00:16:27.788 --> 00:16:34.739
supposed experts would be less lenient in terms of,

257
00:16:34.788 --> 00:16:40.009
I mean, not holding them responsible, particularly if

258
00:16:40.019 --> 00:16:44.989
something bad happens because of that. Yes. Um

259
00:16:45.019 --> 00:16:47.190
Yes, that's, that's a very important what you're

260
00:16:47.200 --> 00:16:49.788
saying. Um I see two relevant things there in

261
00:16:49.798 --> 00:16:55.178
your example, one is um the responsibility that comes

262
00:16:55.190 --> 00:16:56.960
with the obligations with the roles. So we can

263
00:16:56.969 --> 00:17:02.219
talk about the normative expectations of certain roles. So

264
00:17:02.229 --> 00:17:07.318
um like doctors, lawyers or some certain um roles

265
00:17:07.328 --> 00:17:10.795
in the society that are connected to some certain behaviors

266
00:17:11.164 --> 00:17:12.963
. And we expect more, more uh from a

267
00:17:12.973 --> 00:17:17.404
doctor, especially in this situation than of some person

268
00:17:17.414 --> 00:17:19.555
who is not an expert in the field and didn't

269
00:17:19.564 --> 00:17:23.424
oblige to help others and similar. The other relevant

270
00:17:23.434 --> 00:17:26.943
factor that I see there is mostly when we talk

271
00:17:26.953 --> 00:17:30.943
about health and especially in this situation where there is

272
00:17:30.953 --> 00:17:33.703
a serious rare disease we talk about a very severe

273
00:17:33.713 --> 00:17:38.469
outcomes. Um And this is something, this effect

274
00:17:38.479 --> 00:17:41.068
of severity of the outcome is something that has been

275
00:17:41.078 --> 00:17:45.410
studied. Um um in some other context, um

276
00:17:45.420 --> 00:17:48.699
uh when, for example, negligence. Um and

277
00:17:48.709 --> 00:17:52.660
it, it does make a difference, people ascribe

278
00:17:52.670 --> 00:17:56.420
more moral blame uh for agents who uh who produce

279
00:17:56.430 --> 00:18:00.900
more severe um outcomes. Um But in this context

280
00:18:00.910 --> 00:18:03.098
, when we talk about ignorant agents who didn't get

281
00:18:03.108 --> 00:18:07.049
informed and some, some bad happened. Um Then

282
00:18:07.059 --> 00:18:07.598
that, that's a factor that I would, I

283
00:18:07.608 --> 00:18:11.209
would uh like to uh to investigate in some of

284
00:18:11.219 --> 00:18:18.039
my future experiments. What my intuition about it uh

285
00:18:18.160 --> 00:18:23.848
is now is that people are um computing the the

286
00:18:23.858 --> 00:18:27.380
utilities and maybe this now sounds uh sounds weird but

287
00:18:27.390 --> 00:18:30.239
maybe this is the behavioral economist side of me.

288
00:18:30.539 --> 00:18:36.199
Um But I believe that people do um um consider

289
00:18:36.209 --> 00:18:38.709
uh the benefits, cost costs and benefits. Um

290
00:18:38.769 --> 00:18:44.449
And this case, when um the potential harm is

291
00:18:44.539 --> 00:18:48.799
so severe, then um the ratio between uh the

292
00:18:48.809 --> 00:18:52.818
cost for the agent who needs to put some effort

293
00:18:52.449 --> 00:18:56.910
is still lower than the cost. Um The uh

294
00:18:56.920 --> 00:19:00.150
of uh of the agent who would suffer uh from

295
00:19:00.160 --> 00:19:02.588
uh the lack of this prevention. So, in

296
00:19:02.598 --> 00:19:07.068
this case, um the more severe the outcome is

297
00:19:07.078 --> 00:19:11.229
I believe that people hold more expectations from agents to

298
00:19:11.239 --> 00:19:17.189
check the relevant information. Um I cannot claim this

299
00:19:17.199 --> 00:19:18.189
as for my, from my data, but this

300
00:19:18.199 --> 00:19:21.809
is my my, my strong intuition about this.

301
00:19:22.299 --> 00:19:23.880
Um So let's see, maybe some uh next time

302
00:19:23.890 --> 00:19:27.529
I can, I can give you a empirical answer

303
00:19:27.539 --> 00:19:30.500
to that question. Um But yeah, this,

304
00:19:30.509 --> 00:19:33.078
this is what I uh what I think uh for

305
00:19:33.088 --> 00:19:37.348
now. So uh we'll get back to outcomes in

306
00:19:37.358 --> 00:19:40.939
a second. But let me just ask you one

307
00:19:40.949 --> 00:19:45.430
relate relevant information because I was just thinking about what

308
00:19:45.439 --> 00:19:52.229
people usually consider relevant information to be. I mean

309
00:19:52.239 --> 00:19:53.459
, of course, we would have to look perhaps

310
00:19:53.469 --> 00:19:57.979
at specific cases, specific examples. But for example

311
00:19:57.989 --> 00:20:00.750
, going back to the case of, I mean

312
00:20:00.759 --> 00:20:04.750
, relevant information, what if for some reason,

313
00:20:04.930 --> 00:20:11.449
the person cannot have access to particular kinds of relevant

314
00:20:11.459 --> 00:20:18.519
information or if what people think is relevant information is

315
00:20:18.529 --> 00:20:21.920
not even there because for example, we don't know

316
00:20:21.930 --> 00:20:25.739
enough about a particular disease. It has not been

317
00:20:25.750 --> 00:20:30.309
studied enough, there are particular aspects of it that

318
00:20:30.318 --> 00:20:33.578
we don't know about yet. So, uh I

319
00:20:33.588 --> 00:20:36.068
mean, uh basically, what I'm trying to understand

320
00:20:36.078 --> 00:20:42.828
is uh what people tend to consider relevant information basically

321
00:20:44.539 --> 00:20:47.519
. Oh yeah, that's an interesting question. Well

322
00:20:47.529 --> 00:20:49.979
, 11 thing that I would like to mention here

323
00:20:49.989 --> 00:20:53.739
is that um the factor that I was uh I

324
00:20:53.750 --> 00:20:57.259
was uh talking about previously is this opportunity to learn

325
00:20:57.529 --> 00:21:00.709
. And of course, there is no opportunity to

326
00:21:00.719 --> 00:21:04.539
get some information. Um Even the doctor wouldn't be

327
00:21:04.559 --> 00:21:07.939
uh found responsible if there is just no, the

328
00:21:07.949 --> 00:21:11.229
medicine did not develop to the extent that could help

329
00:21:11.239 --> 00:21:15.689
a person or there is no medication available in a

330
00:21:15.699 --> 00:21:18.939
certain context. And uh of course, in this

331
00:21:18.949 --> 00:21:22.229
case, situations when there is no opportunity to act

332
00:21:22.239 --> 00:21:26.338
differently and then the the relevant quant factuals are about

333
00:21:26.348 --> 00:21:29.318
some external factors, right? But this question,

334
00:21:29.328 --> 00:21:33.108
what is the relevant information? Uh I mean,

335
00:21:33.118 --> 00:21:36.670
I'm gonna get a bit uh theoretical but when I

336
00:21:36.680 --> 00:21:38.979
talk about what the relevant information is, I talk

337
00:21:38.989 --> 00:21:42.229
about information that has the power to change the outcome

338
00:21:44.229 --> 00:21:45.848
. OK. Um And that's true what you say

339
00:21:45.858 --> 00:21:51.529
that it's not always obvious what type of information uh

340
00:21:51.539 --> 00:21:55.098
could change the outcome, right? Especially if you're

341
00:21:55.108 --> 00:21:57.250
a doctor and you need to uh check for multiple

342
00:21:57.259 --> 00:22:00.920
options, then you don't know exactly which information would

343
00:22:00.930 --> 00:22:04.910
answer your questions. But if we talk about um

344
00:22:06.459 --> 00:22:11.640
some other interpersonal relations, uh for example, um

345
00:22:11.650 --> 00:22:14.489
you want to take a car of your sibling um

346
00:22:14.500 --> 00:22:18.439
that you share um the relevant information would be,

347
00:22:18.769 --> 00:22:22.650
does my sibling need the car for today? Um

348
00:22:22.660 --> 00:22:26.140
And um you would call, call them to check

349
00:22:26.150 --> 00:22:29.949
um about this. So, uh sometimes what is

350
00:22:29.959 --> 00:22:32.930
the relevant information is very obvious uh such as in

351
00:22:32.939 --> 00:22:37.838
the case of um checking what reading the parking sign

352
00:22:37.848 --> 00:22:40.670
and reading if the spot is, is reserved or

353
00:22:40.680 --> 00:22:42.289
not, can you park there or not? And

354
00:22:42.299 --> 00:22:45.410
the relevant information is yes or no, right?

355
00:22:45.650 --> 00:22:48.880
Uh Because that could change the course of the outcome

356
00:22:48.890 --> 00:22:51.838
if a person, uh, who needs this space

357
00:22:51.848 --> 00:22:53.828
cannot park there. Right. Uh, but,

358
00:22:53.838 --> 00:22:56.318
uh, I mean, I, I agree in

359
00:22:56.328 --> 00:22:59.400
some situations, um, what is relevant to be

360
00:22:59.410 --> 00:23:02.180
known, uh, is not so obvious. Yeah

361
00:23:02.959 --> 00:23:04.719
, that's true. Yeah. And going back to

362
00:23:04.729 --> 00:23:10.259
the outcomes because we've already talked here about how,

363
00:23:10.269 --> 00:23:14.779
uh, if it's a severe outcome, people usually

364
00:23:15.000 --> 00:23:18.338
tend to ascribe more responsibility to the person. But

365
00:23:18.519 --> 00:23:23.094
what if, uh, people get a good outcome

366
00:23:23.104 --> 00:23:26.354
even if they didn't hold, uh, uh,

367
00:23:26.364 --> 00:23:32.654
the relevant information that it was just a good outcome

368
00:23:32.664 --> 00:23:36.584
basically by chance. Do, does that matter to

369
00:23:36.594 --> 00:23:40.868
people or not? Mhm. Um, but do

370
00:23:40.880 --> 00:23:45.108
you mean there, whether a person would be responsible

371
00:23:45.318 --> 00:23:48.019
? Um, or for, I mean, if

372
00:23:48.029 --> 00:23:49.910
, in that case, so let's say that uh

373
00:23:51.140 --> 00:23:55.588
, they get, they got a good outcome but

374
00:23:55.598 --> 00:24:00.459
later people learn that it was mostly by chance because

375
00:24:00.469 --> 00:24:03.250
the person didn't really, uh have, uh,

376
00:24:03.259 --> 00:24:07.894
the relevant information. I mean, just basically did

377
00:24:07.904 --> 00:24:11.914
things they went into it blind, let's say,

378
00:24:11.924 --> 00:24:14.104
and they got a good outcome. I mean,

379
00:24:14.114 --> 00:24:18.243
does that matter? Do people care about that at

380
00:24:18.255 --> 00:24:21.795
all or not? I mean, does that over

381
00:24:21.805 --> 00:24:25.594
, is that more important to people than the fact

382
00:24:25.604 --> 00:24:29.795
that they just got a good outcome or not?

383
00:24:29.963 --> 00:24:30.904
Um, that, that's a good question when we

384
00:24:30.914 --> 00:24:36.068
talk about responsibility, the way I define responsibility is

385
00:24:36.078 --> 00:24:38.969
that, uh, some certain outcome is needed because

386
00:24:38.979 --> 00:24:44.719
responsibility can be seen as a, um, response

387
00:24:44.729 --> 00:24:47.799
to something So when something happens, we can talk

388
00:24:47.809 --> 00:24:52.328
about who is responsible for that certain outcome. Um

389
00:24:52.338 --> 00:24:59.670
But there are other moral judgments that would take these

390
00:24:59.680 --> 00:25:03.890
uh mental states more into account. And that uh

391
00:25:03.900 --> 00:25:08.588
for example, um um Cushman theory, Cushman is

392
00:25:08.598 --> 00:25:15.680
talking about um this how for um judgments of blame

393
00:25:15.689 --> 00:25:18.890
and punishment outcome is, is uh what is very

394
00:25:18.900 --> 00:25:22.789
important and more important than when we talk about the

395
00:25:22.799 --> 00:25:26.949
judgment of moral wrongness, for example. So in

396
00:25:26.959 --> 00:25:30.618
this case, when somebody, um even when there

397
00:25:30.630 --> 00:25:33.078
is no negative outcome, we can still maybe not

398
00:25:33.088 --> 00:25:36.299
talk about whether the person is responsible because nothing bad

399
00:25:36.309 --> 00:25:37.838
happened, but we can talk about whether they acted

400
00:25:37.848 --> 00:25:41.920
morally wrongly. Um for not gathering the right information

401
00:25:42.078 --> 00:25:45.549
such as a doctor who didn't really check for some

402
00:25:45.559 --> 00:25:49.078
relevant conditions before giving the medication that could have produced

403
00:25:49.088 --> 00:25:53.029
some negative consequences. So in that case, I

404
00:25:53.039 --> 00:25:57.180
wouldn't talk about responsibility, but I could talk about

405
00:25:57.189 --> 00:26:00.219
whether it was right or wrong. Um Not to

406
00:26:00.229 --> 00:26:04.219
have a certain information. Um That is the case

407
00:26:04.229 --> 00:26:07.959
of, for example, uh drunk driving, you

408
00:26:07.969 --> 00:26:11.479
can be morally judged for driving drunk, even if

409
00:26:11.489 --> 00:26:15.449
you came home safely by pure luck, right?

410
00:26:15.900 --> 00:26:18.848
So we can, we can judge this action and

411
00:26:18.858 --> 00:26:21.838
your behavior as not being the good one, but

412
00:26:21.848 --> 00:26:25.979
we cannot make you responsible for an outcome that didn't

413
00:26:25.989 --> 00:26:27.969
happen, right? Um This is my take on

414
00:26:27.979 --> 00:26:32.019
this, that uh when I talk about responsibility in

415
00:26:32.029 --> 00:26:33.578
this case, I talk about the, the responsibility

416
00:26:33.588 --> 00:26:37.430
for something that happened, um, some people would

417
00:26:37.439 --> 00:26:41.479
maybe talk about responsibility, like, um, and

418
00:26:41.489 --> 00:26:45.630
that assigning responsibility to people to do certain actions,

419
00:26:45.640 --> 00:26:48.189
like some kind of obligations. But, um,

420
00:26:48.199 --> 00:26:52.108
I talk mostly about the, the responsibility for the

421
00:26:52.118 --> 00:26:55.029
harm that, that happened in this case but we're

422
00:26:55.039 --> 00:26:56.049
sure we can talk about whether the, the,

423
00:26:56.059 --> 00:26:59.578
the, the lack of knowledge, uh, was

424
00:26:59.588 --> 00:27:03.539
bad. Right. Mhm. Right. So let's

425
00:27:03.549 --> 00:27:07.130
get now into strategic ignorance since we're talking about also

426
00:27:07.140 --> 00:27:11.019
epistemic responsibility here. I guess that this ties very

427
00:27:11.029 --> 00:27:15.779
well to that. So what is strategic ignorance?

428
00:27:15.789 --> 00:27:22.358
Exactly. Yes. So we talked about different situations

429
00:27:22.368 --> 00:27:25.559
in which agents are ignorant, right? And there

430
00:27:25.568 --> 00:27:30.130
can be multiple causes why somebody um it was left

431
00:27:30.140 --> 00:27:34.818
ignorant. Strategic ignorance is this specific situation uh when

432
00:27:34.828 --> 00:27:40.779
uh people are using as Dan and colleagues say this

433
00:27:40.789 --> 00:27:45.229
moral wiggle room, uh what it means it means

434
00:27:45.239 --> 00:27:48.239
that since we talked about how in um a lot

435
00:27:48.250 --> 00:27:52.189
of cases, the fact that didn't, didn't know

436
00:27:52.549 --> 00:27:56.400
that something bad can happen, can mitigate the responsibility

437
00:27:56.568 --> 00:28:00.699
. People can use this space to say, oh

438
00:28:00.709 --> 00:28:04.118
I didn't know such as this case with not buying

439
00:28:04.130 --> 00:28:07.269
a transport ticket. Oh I didn't know how to

440
00:28:07.279 --> 00:28:10.930
buy a transport ticket. Um So I'm I'm not

441
00:28:10.939 --> 00:28:12.739
responsible for um I shouldn't be punished for this,

442
00:28:12.750 --> 00:28:18.479
right? So strategic ignorance is ignoring some relevant information

443
00:28:18.259 --> 00:28:23.279
in order to run away from uh the responsibility of

444
00:28:23.289 --> 00:28:27.068
of your actions. Um In my case, we

445
00:28:27.078 --> 00:28:32.160
call it fearful ignorance, but still in these cases

446
00:28:32.170 --> 00:28:33.939
, um, that I mentioned to you such as

447
00:28:33.949 --> 00:28:38.420
a person not checking the parking sign, um,

448
00:28:40.670 --> 00:28:42.309
we can say that the person did that on purpose

449
00:28:42.318 --> 00:28:45.750
so that they are not late for their appointment because

450
00:28:45.759 --> 00:28:48.588
in this case, they would need to change uh

451
00:28:48.598 --> 00:28:52.269
the spot. Uh, they would need to park

452
00:28:52.279 --> 00:28:56.078
and lose some time. So they strategically ignore this

453
00:28:56.088 --> 00:29:00.170
in order not to be uh not to feel morally

454
00:29:00.180 --> 00:29:03.809
obliged to change their actions. That's the idea.

455
00:29:03.299 --> 00:29:07.930
Um Either to themselves um or to other people.

456
00:29:07.939 --> 00:29:11.799
So basically, to save some kind of a positive

457
00:29:11.809 --> 00:29:15.009
picture of themselves or they want other people to think

458
00:29:15.019 --> 00:29:17.009
more positively of them. So if you say,

459
00:29:17.019 --> 00:29:18.459
oh, I didn't know I took somebody else's spot

460
00:29:18.699 --> 00:29:22.348
, um It's not the same as saying, oh

461
00:29:22.358 --> 00:29:23.108
yeah, I, I knew that was reserved,

462
00:29:23.118 --> 00:29:29.289
but I didn't care. Right. Um And there

463
00:29:29.299 --> 00:29:32.900
are interesting experiments of, of Dan and colleagues.

464
00:29:33.269 --> 00:29:37.719
Um you know, dictator games. Um So this

465
00:29:37.729 --> 00:29:45.338
economic games where um you give certain um um certain

466
00:29:45.348 --> 00:29:48.029
um outcomes for people. If you get six points

467
00:29:48.039 --> 00:29:51.259
, a person will, or$6 a person will

468
00:29:51.269 --> 00:29:53.670
get$1. But uh if you choose to have

469
00:29:53.680 --> 00:29:56.739
um$5 a person will also have$5 and then

470
00:29:56.750 --> 00:30:00.189
you are asked to choose which outcome uh would,

471
00:30:00.199 --> 00:30:06.630
would you prefer? Um And basically what these type

472
00:30:06.640 --> 00:30:08.098
of games are testing is whether there is some kind

473
00:30:08.108 --> 00:30:11.500
of a uh prosocial, so whether people would uh

474
00:30:11.559 --> 00:30:15.318
also think about other people's welfare and not only their

475
00:30:15.328 --> 00:30:18.229
own, right? Um And when you show this

476
00:30:18.239 --> 00:30:22.630
information transparent like that, so what is the outcome

477
00:30:22.640 --> 00:30:26.650
for you and for the other person? Um people

478
00:30:26.660 --> 00:30:30.598
in a lot of cases um are doing, choosing

479
00:30:30.608 --> 00:30:34.078
a prosocial answer. So choosing such as that they

480
00:30:34.088 --> 00:30:37.660
maximize or optimize the developer for the other person.

481
00:30:37.670 --> 00:30:41.729
So what they would rather choose, rather choose five

482
00:30:41.739 --> 00:30:44.269
for both than six for them and one for the

483
00:30:44.279 --> 00:30:48.699
other person. But what's interesting is that if you

484
00:30:48.709 --> 00:30:52.630
cover um what is the outcome for the other person

485
00:30:53.769 --> 00:30:59.209
? And you say it's 50% chance that it's either

486
00:30:59.219 --> 00:31:03.858
one or five. Um So a person who chooses

487
00:31:03.868 --> 00:31:06.309
, doesn't know what would be the possible outcome for

488
00:31:06.318 --> 00:31:11.059
the other person. And then the question is um

489
00:31:11.068 --> 00:31:14.699
they, the people have the opportunity to cover,

490
00:31:14.709 --> 00:31:18.318
to uncover this and to learn what would be the

491
00:31:18.328 --> 00:31:19.529
outcome for the other person. So they have this

492
00:31:19.539 --> 00:31:25.180
like uh reveal, reveal part, the a lot

493
00:31:25.189 --> 00:31:26.250
of people they don't choose to reveal, that's the

494
00:31:26.259 --> 00:31:30.880
thing. Uh less than 60% percent of um of

495
00:31:30.890 --> 00:31:34.699
people choose to reveal what would be the outcome for

496
00:31:34.709 --> 00:31:38.489
the other person and why they do this. So

497
00:31:38.500 --> 00:31:41.390
they, they con they can continue choosing more for

498
00:31:41.400 --> 00:31:45.469
them without feeling bad because they have some kind of

499
00:31:45.479 --> 00:31:48.519
a benefit of a doubt or this moral wiggle room

500
00:31:48.529 --> 00:31:51.289
. So you say, well, I choose more

501
00:31:51.299 --> 00:31:52.118
for me, but I didn't know what the other

502
00:31:52.130 --> 00:31:56.199
person would get and they can be, they can

503
00:31:56.209 --> 00:31:59.660
save their self uh self picture. And also how

504
00:31:59.670 --> 00:32:02.130
other people see them, they can be, they

505
00:32:02.140 --> 00:32:06.420
can remain perceived as, as a fair person.

506
00:32:06.689 --> 00:32:08.578
That that's the idea of that. Of course,

507
00:32:08.588 --> 00:32:13.559
if they choose to reveal, then people would choose

508
00:32:13.568 --> 00:32:15.689
an option that uh mostly choose the option that maximizes

509
00:32:15.699 --> 00:32:20.459
also the benefit for the other person. Um So

510
00:32:20.469 --> 00:32:22.979
this type of strategic ignorance is, is, is

511
00:32:22.989 --> 00:32:27.539
very interesting. Um But now in real life,

512
00:32:27.549 --> 00:32:30.209
some examples apart from this, not reading the parking

513
00:32:30.219 --> 00:32:37.618
sign and parking wherever you want are um even some

514
00:32:37.630 --> 00:32:40.910
like health related things like you are, you don't

515
00:32:40.920 --> 00:32:45.400
want to know how bad smoking actually is because uh

516
00:32:47.049 --> 00:32:52.449
with smoking or you don't want to learn particular information

517
00:32:52.459 --> 00:32:54.848
about your group that you belong to because you don't

518
00:32:54.858 --> 00:33:00.078
want to um think more about your group identity and

519
00:33:00.088 --> 00:33:04.328
reconsider it. Um And uh or for example,

520
00:33:04.338 --> 00:33:07.430
you don't want to see how mu how many um

521
00:33:07.439 --> 00:33:12.420
sugar, how much sugar your favorite drink has because

522
00:33:12.430 --> 00:33:15.088
you just want to continue drinking it um and similar

523
00:33:15.219 --> 00:33:17.578
. So people do this in, in a lot

524
00:33:17.588 --> 00:33:21.680
of cases that are related to others or, or

525
00:33:21.689 --> 00:33:25.088
just related to themselves. And um yeah, um

526
00:33:25.098 --> 00:33:29.489
and to be able to uh to continue with doing

527
00:33:29.500 --> 00:33:34.559
what they want to do but still um having some

528
00:33:34.568 --> 00:33:37.848
nice, some good picture about themselves. And there's

529
00:33:37.858 --> 00:33:42.318
also a problem with some certain ecological behaviors that people

530
00:33:42.328 --> 00:33:45.588
are ignoring how they affect the community. The,

531
00:33:45.598 --> 00:33:49.049
the, the, the, the ecology uh because

532
00:33:49.059 --> 00:33:52.739
changing their behavior to start recycling um et cetera can

533
00:33:52.750 --> 00:33:57.289
be effortful and not something that they want to change

534
00:33:58.068 --> 00:34:01.019
. Um Maybe some people would call this rather deliberate

535
00:34:01.029 --> 00:34:07.380
ignorance than strategic ignorance. So this uh terminology is

536
00:34:07.390 --> 00:34:12.188
sometimes a bit confusing because you have uh strategic ignorance

537
00:34:12.199 --> 00:34:15.500
, deliberate ignorance, skillful ignorance, uh et cetera

538
00:34:15.030 --> 00:34:17.449
. Maybe I would say the strategic ignorance is more

539
00:34:17.458 --> 00:34:22.550
concerned with this uh that relates for others to others

540
00:34:22.378 --> 00:34:25.079
, such as kind of this justification. Why you

541
00:34:25.090 --> 00:34:30.409
did something or being perceived as positive in the eyes

542
00:34:30.418 --> 00:34:32.110
of other people. But so maybe I would call

543
00:34:32.119 --> 00:34:38.550
deliberate ignorance um this um behavior that are related to

544
00:34:38.559 --> 00:34:43.867
your health, choices and stuff. Um Hertwig and

545
00:34:43.878 --> 00:34:46.197
people from the Max Planck Institute for Human Development have

546
00:34:46.208 --> 00:34:49.818
a lot of papers on deliberate ignorance. Um And

547
00:34:49.829 --> 00:34:52.677
then they use this terminology. Um So, so

548
00:34:52.688 --> 00:34:55.588
yeah, those are some also some interesting literature to

549
00:34:55.599 --> 00:34:58.548
check on this topic. Who is more interested?

550
00:34:58.559 --> 00:35:00.958
Yeah. Yeah, great. So uh this will

551
00:35:00.969 --> 00:35:04.878
probably be my last uh question. But I would

552
00:35:04.889 --> 00:35:07.809
also like to get into uh perhaps some more concrete

553
00:35:07.820 --> 00:35:13.000
example, uh health care related example. You al

554
00:35:13.059 --> 00:35:15.840
you also mentioned health there. So because the there

555
00:35:15.849 --> 00:35:19.019
are particular cases. And I also read about this

556
00:35:19.030 --> 00:35:22.840
in the literature where people actually use strategic ignorance but

557
00:35:22.849 --> 00:35:28.679
are really a little bit more severe in terms of

558
00:35:28.688 --> 00:35:31.309
the possible outcomes. It can have, I mean

559
00:35:31.320 --> 00:35:34.739
, people, when they read about it, many

560
00:35:34.750 --> 00:35:36.929
people think, oh my God, why would people

561
00:35:36.938 --> 00:35:37.789
do that? So when it comes to, for

562
00:35:37.800 --> 00:35:43.789
example, uh, sexually transmitted infections or ST s

563
00:35:44.360 --> 00:35:46.909
, there are actually cases where people, I mean

564
00:35:46.918 --> 00:35:52.824
, they might think that they might have been exposed

565
00:35:52.833 --> 00:35:55.864
to an ST I, and they might have got

566
00:35:55.875 --> 00:36:00.675
it but they don't want to get tested, uh

567
00:36:00.684 --> 00:36:04.244
, in that particular case and sometimes it might be

568
00:36:04.253 --> 00:36:07.594
something very serious like A I DS or something like

569
00:36:07.603 --> 00:36:09.449
that. So, uh, a and in that

570
00:36:09.458 --> 00:36:12.849
particular case, I guess that people find it a

571
00:36:12.860 --> 00:36:15.340
bit weird, that kind of literature because, uh

572
00:36:15.349 --> 00:36:17.159
, I mean, if it's, for example,

573
00:36:17.250 --> 00:36:21.039
uh, tobacco use or if you're, uh,

574
00:36:21.070 --> 00:36:24.199
consuming too much su sugar, people might think that

575
00:36:24.208 --> 00:36:27.780
, ok, that's not great, but at least

576
00:36:27.789 --> 00:36:31.012
you're not affecting other people or potentially harming other people

577
00:36:31.021 --> 00:36:35.762
. It's just yourself. But in that particular case

578
00:36:36.103 --> 00:36:39.302
, I mean, there's a very big potential there

579
00:36:39.311 --> 00:36:44.731
that you will be harming other people. And in

580
00:36:44.742 --> 00:36:46.001
that specific, I mean, it, it could

581
00:36:46.012 --> 00:36:50.242
be the case that in a particular case you were

582
00:36:50.251 --> 00:36:54.195
exposed accidentally an sti, I mean, it could

583
00:36:54.206 --> 00:36:59.005
have not been transmitted sexually and you didn't know at

584
00:36:59.016 --> 00:37:01.114
all. And so that's a different thing. But

585
00:37:01.166 --> 00:37:05.916
if you think that you might have got it,

586
00:37:05.925 --> 00:37:07.875
then, I mean, that's completely different, right

587
00:37:07.885 --> 00:37:12.496
? So why, why, why do people do

588
00:37:12.505 --> 00:37:15.436
that? Exactly. That's a great, uh,

589
00:37:15.445 --> 00:37:17.659
comment. Now I can get a bit more personal

590
00:37:17.668 --> 00:37:21.648
as to why did they choose to start working on

591
00:37:21.659 --> 00:37:22.860
this topic? That is not exactly the example that

592
00:37:22.869 --> 00:37:28.869
you mentioned, but, um, how this question

593
00:37:28.898 --> 00:37:31.559
of how much should we know? How much should

594
00:37:31.570 --> 00:37:35.360
we check? How much should we care of not

595
00:37:35.369 --> 00:37:39.260
affecting other people? All these questions were always some

596
00:37:39.269 --> 00:37:43.010
kind of my interest. Um and this concern for

597
00:37:43.019 --> 00:37:45.719
others. But when we talk about health, it

598
00:37:45.728 --> 00:37:49.769
especially arose when I moved to Austria. So I

599
00:37:49.789 --> 00:37:52.260
disclaimer, I moved to Austria in the time of

600
00:37:52.269 --> 00:37:57.780
COVID. So 2021 and in Austria, the lockdown

601
00:37:57.789 --> 00:38:00.840
measures, the COVID testing measures were very severe.

602
00:38:00.119 --> 00:38:06.559
So I moved from Serbia where PC R tests were

603
00:38:06.570 --> 00:38:08.648
not possible to be done by your own choice.

604
00:38:08.659 --> 00:38:12.360
Only if you have symptoms, if you wanted to

605
00:38:12.369 --> 00:38:15.809
do it just to check if you maybe were exposed

606
00:38:15.139 --> 00:38:19.398
, uh or you think that uh you were exposed

607
00:38:19.409 --> 00:38:22.769
or similar, you needed to pay€60 let's say

608
00:38:22.179 --> 00:38:27.489
then I moved to Austria where testing is the easiest

609
00:38:27.500 --> 00:38:30.260
thing ever. You just do it at your home

610
00:38:30.269 --> 00:38:30.688
. It's for free. You can do it every

611
00:38:30.699 --> 00:38:34.409
day at the time when I moved. And then

612
00:38:34.418 --> 00:38:37.148
I felt obliged to do the test almost every day

613
00:38:37.159 --> 00:38:42.188
because I felt, what is the justification of not

614
00:38:42.199 --> 00:38:45.070
doing the test it for free? It's easy.

615
00:38:45.340 --> 00:38:46.458
There is no reason not to do it. So

616
00:38:46.469 --> 00:38:50.869
I felt even a bit obsessed with, with this

617
00:38:50.878 --> 00:38:53.208
because I felt so responsible to check, right?

618
00:38:54.250 --> 00:38:57.239
Um And then I was asking this question. So

619
00:38:57.250 --> 00:38:59.478
why do I feel like this, you know how

620
00:38:59.489 --> 00:39:04.590
this moving change these expectations for myself so much?

621
00:39:05.099 --> 00:39:07.780
And um and yeah, I mean, this kind

622
00:39:07.789 --> 00:39:10.550
of a, um, um, thinking that maybe

623
00:39:10.559 --> 00:39:14.030
you are, uh, you have the disease or

624
00:39:14.039 --> 00:39:15.769
similar can be translated to the example that, that

625
00:39:15.780 --> 00:39:19.989
you mentioned. And indeed this uh, question of

626
00:39:20.000 --> 00:39:22.989
transmissible sexually transmissible diseases is very, very important.

627
00:39:23.458 --> 00:39:27.659
Um, and I think I'm not sure, uh

628
00:39:27.668 --> 00:39:30.478
, we could ask legal um, practitioners but,

629
00:39:30.489 --> 00:39:34.429
uh, that even transmitting, uh, transmitting some

630
00:39:34.438 --> 00:39:37.760
serious, uh, sexually transmittable diseases is, um

631
00:39:37.769 --> 00:39:40.570
, is illegal if not to, to, to

632
00:39:40.590 --> 00:39:44.320
dis, to disclose that you have them, right

633
00:39:44.898 --> 00:39:46.860
? Um, so it's a very serious problem.

634
00:39:47.269 --> 00:39:52.840
Um And, um, of course, here we

635
00:39:52.849 --> 00:39:57.800
have the similar, um, motivation for not checking

636
00:39:58.039 --> 00:40:00.860
, which is, um, in case of some

637
00:40:00.869 --> 00:40:02.969
uh transmittable diseases, um, is that you want

638
00:40:02.978 --> 00:40:07.280
to continue, um, doing what you want,

639
00:40:07.000 --> 00:40:09.559
um, which is having some, some, some

640
00:40:09.570 --> 00:40:13.438
pleasure and you don't want to talk about that.

641
00:40:13.449 --> 00:40:15.898
You don't want to maybe stop for some time,

642
00:40:15.159 --> 00:40:19.099
uh, and similar. Um, and then this

643
00:40:19.110 --> 00:40:22.090
, there is this strategic motivation I'm not gonna check

644
00:40:22.099 --> 00:40:23.438
because then I can continue doing what I'm doing.

645
00:40:23.639 --> 00:40:27.949
Um, and I don't feel so guilty about it

646
00:40:27.958 --> 00:40:31.739
. Right. The other part of that, um

647
00:40:31.750 --> 00:40:36.070
, that doesn't include this lack of care for others

648
00:40:36.228 --> 00:40:37.030
, which I would say it's a lack of care

649
00:40:37.039 --> 00:40:40.019
for others if you, um, have a serious

650
00:40:40.030 --> 00:40:44.619
suspicion, especially that you could be, um,

651
00:40:44.628 --> 00:40:46.679
infected, but you don't check the other part of

652
00:40:46.688 --> 00:40:52.159
that, um, is connected with the emotional regulation

653
00:40:52.179 --> 00:40:54.769
, which is also a case with some other diseases

654
00:40:55.329 --> 00:40:59.639
. Um that people don't want to know if they

655
00:40:59.648 --> 00:41:00.878
are sick. Uh So even if they have some

656
00:41:00.889 --> 00:41:07.800
symptoms, they are afraid of some um some negative

657
00:41:07.809 --> 00:41:12.139
information, especially if you, if, if some

658
00:41:12.148 --> 00:41:15.769
very serious transmittable diseases are in question that you may

659
00:41:15.780 --> 00:41:19.679
feel uh afraid to learn uh that you have some

660
00:41:19.688 --> 00:41:22.239
of them. Um And this fear is stopping you

661
00:41:22.250 --> 00:41:25.898
from, from checking. Um But of course,

662
00:41:25.909 --> 00:41:29.739
I mean, what my research would say about that

663
00:41:29.750 --> 00:41:31.610
is that um if there is some reason to,

664
00:41:31.619 --> 00:41:36.000
to be suspicious, especially and uh if you have

665
00:41:36.309 --> 00:41:40.579
um and a sufficient care for others that uh it

666
00:41:40.590 --> 00:41:44.550
is expected like that you would have have checked,

667
00:41:44.559 --> 00:41:47.188
especially if it's uh if it's available to you.

668
00:41:47.199 --> 00:41:51.090
But since if we talk about something very serious,

669
00:41:51.099 --> 00:41:52.750
then I get to go back to my question.

670
00:41:52.760 --> 00:41:57.519
People would consider the utility and if it's uh it's

671
00:41:57.530 --> 00:42:01.059
an important uh topic as such here, transmittable diseases

672
00:42:01.070 --> 00:42:04.929
, you could put a bit more effort uh to

673
00:42:04.938 --> 00:42:07.918
find out whether you have something that could harm others

674
00:42:08.648 --> 00:42:12.110
. Yeah. A and in the particular case of

675
00:42:12.119 --> 00:42:14.449
, uh, STIs, for example. And in

676
00:42:14.458 --> 00:42:15.800
the law, II, I mean, of course

677
00:42:15.809 --> 00:42:19.668
, I'm not a law expert or anything like that

678
00:42:19.679 --> 00:42:22.418
and I don't know the laws of different countries but

679
00:42:22.429 --> 00:42:28.719
I would imagine just intuitively, uh, someone simply

680
00:42:28.728 --> 00:42:34.628
suspecting that they might have been infected and not testing

681
00:42:35.010 --> 00:42:36.739
, uh, in the eyes of the law would

682
00:42:36.750 --> 00:42:40.398
be different than actually having tested and know for sure

683
00:42:40.409 --> 00:42:45.000
that you have the disease and still continue on with

684
00:42:45.010 --> 00:42:46.688
the behavior and infecting other people. Because in the

685
00:42:46.699 --> 00:42:52.159
first case, you might still have some plausible deniability

686
00:42:52.168 --> 00:42:54.599
. Right. Yes. Um Yes, that's true

687
00:42:54.728 --> 00:42:58.039
. And as far as I know, as I

688
00:42:58.050 --> 00:43:01.139
think, I know, um this, uh I

689
00:43:01.148 --> 00:43:06.570
like um punishment, uh like legal punishment for that

690
00:43:06.579 --> 00:43:07.469
behavior is only in the case if you know.

691
00:43:07.978 --> 00:43:13.389
Um But since I was working on this case of

692
00:43:13.398 --> 00:43:15.750
beautiful ignorance, I was reading a lot even about

693
00:43:15.760 --> 00:43:19.869
some uh philosophy of law and some law articles.

694
00:43:19.878 --> 00:43:24.260
And in some cases, some philosophers even claim um

695
00:43:24.269 --> 00:43:28.719
that in some cases, the willful ignorance can be

696
00:43:28.728 --> 00:43:35.280
perceived similarly to knowledge. Uh because in, in

697
00:43:35.289 --> 00:43:37.840
view of the mens rea which is this guilty mind

698
00:43:37.780 --> 00:43:43.090
and if there was a sufficient suspicion. Yeah.

699
00:43:43.289 --> 00:43:47.559
Um, did that you could have checked, that

700
00:43:47.570 --> 00:43:52.860
could be also punishable. I think it depends on

701
00:43:52.869 --> 00:43:54.539
the country on the legal system. It's a question

702
00:43:54.550 --> 00:43:59.010
whether it's just like, um, now some kind

703
00:43:59.019 --> 00:44:01.039
of a open discussion, I'm not sure exactly about

704
00:44:01.050 --> 00:44:04.590
this because again, I'm not a legal practitioner just

705
00:44:04.599 --> 00:44:06.860
interested in this topic. So, uh, I

706
00:44:06.869 --> 00:44:08.405
come, uh, to read some, some certain

707
00:44:08.414 --> 00:44:13.034
stuff like this. Um, but, uh,

708
00:44:13.043 --> 00:44:15.543
yeah, in some cases being, um, having

709
00:44:15.554 --> 00:44:20.135
a reasonable suspicion, such in this case is your

710
00:44:20.304 --> 00:44:22.204
three ex partners of yours told you that they have

711
00:44:22.215 --> 00:44:25.844
HPV, uh, or you have some certain symptoms

712
00:44:25.853 --> 00:44:30.619
of it. I'm not sure how this particular case

713
00:44:30.628 --> 00:44:32.760
would be treated in the legal system. Um uh

714
00:44:32.769 --> 00:44:37.599
or uh when we talk about um some, some

715
00:44:37.610 --> 00:44:42.478
other diseases. Um but uh but that, that

716
00:44:42.489 --> 00:44:47.719
um this reasonable suspicion can sometimes be even enough.

717
00:44:47.958 --> 00:44:52.438
Um Yeah, maybe not in this health context but

718
00:44:52.449 --> 00:44:54.090
in some other context when you're harming others or killing

719
00:44:54.099 --> 00:44:59.000
someone extent. Um and it's similar, maybe in

720
00:44:59.010 --> 00:45:05.449
these cases, this having enough reasons to be suspicious

721
00:45:05.458 --> 00:45:08.550
could, could lead to more severe judgments. Yes

722
00:45:09.000 --> 00:45:13.110
. Yeah. The only reason why I brought that

723
00:45:13.119 --> 00:45:16.590
distinction to the table between actually knowing that you have

724
00:45:16.599 --> 00:45:21.349
the disease and just suspecting that you might have it

725
00:45:21.360 --> 00:45:27.958
, but not really testing is that psychologically speaking and

726
00:45:27.969 --> 00:45:30.418
from the perspective of psychology and cognitive science, perhaps

727
00:45:30.708 --> 00:45:36.369
uh for people who actually do not care about harming

728
00:45:36.378 --> 00:45:39.478
others and just want to go on having fun or

729
00:45:39.489 --> 00:45:44.688
something like that, then in the case where they

730
00:45:44.699 --> 00:45:47.119
don't test, I, I mean, in their

731
00:45:47.128 --> 00:45:52.300
own self interest. Uh, it would be better

732
00:45:52.309 --> 00:45:54.119
for them because that, because then they would,

733
00:45:54.168 --> 00:45:58.519
if they are ready, do not care about harming

734
00:45:58.530 --> 00:46:01.119
others, then adding to that they would have more

735
00:46:01.128 --> 00:46:06.628
plausible deniability because I didn't even actually test it.

736
00:46:06.639 --> 00:46:08.648
So I didn't know that the idea of this moral

737
00:46:08.659 --> 00:46:13.519
wiggle room and that was strategic, strategic ignorance is

738
00:46:13.829 --> 00:46:19.099
using this fact that sometimes not knowing, uh is

739
00:46:19.110 --> 00:46:23.469
mitigating your moral judgment towards you. And you're using

740
00:46:23.478 --> 00:46:28.780
this moral wiggle room of not saying, oh,

741
00:46:28.789 --> 00:46:30.769
I didn't know exactly that's the problem. But what

742
00:46:30.780 --> 00:46:34.289
I want to claim there is that, well,

743
00:46:34.300 --> 00:46:36.320
that's, that's not always the excuse, you know

744
00:46:36.329 --> 00:46:38.300
, maybe we're supposed to know, maybe we're supposed

745
00:46:38.309 --> 00:46:40.570
to care more and then if you cared more,

746
00:46:40.579 --> 00:46:44.570
then you will know. And uh yeah, some

747
00:46:44.579 --> 00:46:46.539
, some philosophers would agree with this. Um um

748
00:46:46.550 --> 00:46:51.148
because what we do sometimes is as moral psychologist,

749
00:46:51.159 --> 00:46:54.579
we um can take some, some notions from,

750
00:46:54.750 --> 00:46:59.239
from philosophy, some uh prescriptive theories and then test

751
00:46:59.250 --> 00:47:02.090
them empirically. And um some people do talk about

752
00:47:02.099 --> 00:47:07.099
the quality of y and uh this uh recognizing whether

753
00:47:07.110 --> 00:47:10.519
the agent uh had uh in consideration the, the

754
00:47:10.530 --> 00:47:15.719
, the welfare of others, even if come to

755
00:47:15.728 --> 00:47:19.699
produce some negative outcomes. Um They, they would

756
00:47:19.708 --> 00:47:23.000
be not judged as blame, very irresponsible. Um

757
00:47:23.010 --> 00:47:27.800
Yeah, so um yeah, II I want to

758
00:47:27.809 --> 00:47:30.679
, to study this more in my future research,

759
00:47:30.208 --> 00:47:36.059
this for sociality and care for others. Um And

760
00:47:36.070 --> 00:47:39.260
how it influences some certain decisions that we make and

761
00:47:39.269 --> 00:47:44.199
certain expectations uh from people. Um Yeah, I

762
00:47:44.208 --> 00:47:45.519
think it's important to take other people into because I

763
00:47:45.530 --> 00:47:47.360
mean, that's my personal value. I would say

764
00:47:47.739 --> 00:47:52.219
uh taking some other people in the consideration when you

765
00:47:52.228 --> 00:47:54.849
make certain decisions and then to see what, to

766
00:47:54.860 --> 00:47:58.699
what extent it is, uh it is reasonable to

767
00:47:58.708 --> 00:48:01.559
what extent people uh people think it is reasonable.

768
00:48:01.570 --> 00:48:05.000
Um Yeah, let's, let's say like that.

769
00:48:05.340 --> 00:48:07.478
Ok, great. So let's leave it. Uh

770
00:48:07.489 --> 00:48:10.840
Let's leave that as perhaps a teaser for a possible

771
00:48:10.849 --> 00:48:16.148
future conversation once you've finished your phd. So just

772
00:48:16.159 --> 00:48:19.519
before we go, would you like to tell people

773
00:48:19.530 --> 00:48:21.820
where they can find you and your work on the

774
00:48:21.829 --> 00:48:25.550
internet? Um Yes. So we recently published a

775
00:48:25.559 --> 00:48:30.938
preprint that um is um called, you should have

776
00:48:30.949 --> 00:48:35.398
checked and uh the importance of epistemic intentions in description

777
00:48:35.409 --> 00:48:38.199
of responsibility um with my co authors, Francesca Bonna

778
00:48:38.398 --> 00:48:42.648
Lomi and Christoph Heinz, my supervisor. Um Also

779
00:48:42.659 --> 00:48:45.800
you can find me on the Aces um like the

780
00:48:45.809 --> 00:48:50.780
Cards Aces uh page uh at CEO, which is

781
00:48:50.789 --> 00:48:53.938
the, the name of my lab. Um um

782
00:48:53.949 --> 00:48:58.300
So, so yeah, um those are some,

783
00:48:58.309 --> 00:49:00.260
some places you can find me and there you can

784
00:49:00.269 --> 00:49:02.369
see my email address on the CEO website. Um

785
00:49:02.378 --> 00:49:05.489
So yes, if you, if you have any

786
00:49:05.500 --> 00:49:08.250
questions, please feel free to, to uh to

787
00:49:08.260 --> 00:49:10.559
uh email me. Yeah, for sure. Or

788
00:49:10.570 --> 00:49:13.739
always on linkedin, you can find me. So

789
00:49:13.750 --> 00:49:17.360
I'm, I'm on multiple locations. Ok, great

790
00:49:17.369 --> 00:49:20.820
. So Luke Karin, thank you so much again

791
00:49:20.829 --> 00:49:22.800
for taking the time to come on the show.

792
00:49:22.809 --> 00:49:24.039
It's been a real pleasure to talk to you.

793
00:49:24.590 --> 00:49:29.320
Thank you so much. Thanks. Ok, great

794
00:49:29.329 --> 00:49:30.769
. So Luke Karin, thank you so much again

795
00:49:30.780 --> 00:49:32.769
for taking the time to come on the show.

796
00:49:32.780 --> 00:49:35.989
It's been a real pleasure to talk to you.

797
00:49:36.539 --> 00:49:38.938
Thank you so much. Hi, everyone. Thank

798
00:49:38.949 --> 00:49:42.059
you for watching this interview. Until the end.

799
00:49:42.070 --> 00:49:45.280
If you like what I'm doing, please consider supporting

800
00:49:45.289 --> 00:49:47.039
the show on Patreon or paypal. You can find

801
00:49:47.050 --> 00:49:51.438
the links in the description box down below and if

802
00:49:51.449 --> 00:49:52.489
you like the interview, please share it, leave

803
00:49:52.500 --> 00:49:57.019
a like hit the subscription button and comment. The

804
00:49:57.030 --> 00:49:59.530
show is brought to you by N Lights learning and

805
00:49:59.539 --> 00:50:02.820
development and differently check their website at N lights.com.

806
00:50:04.148 --> 00:50:06.159
I would also like to give a huge thank you

807
00:50:06.168 --> 00:50:09.159
to my main patrons and paypal supporters, Pero Larson

808
00:50:09.168 --> 00:50:13.188
, Jerry Mueller and Frederick Sunder Bernard. So all

809
00:50:13.349 --> 00:50:15.409
of Alex Adam, Castle Matthew Whittenberg, Arno Wolf

810
00:50:15.418 --> 00:50:19.418
, Tim Hollis, Eric Alan J con Philip Forrest

811
00:50:19.429 --> 00:50:22.128
Connolly. Then the mere Robert Windier ruin. Nai

812
00:50:22.599 --> 00:50:28.362
Z Mark Nebs Colin hope Mikel Storm Andre Francis for

813
00:50:28.532 --> 00:50:32.672
the Agn Fergal Ken Harl hero, Jonathan lebron and

814
00:50:34.913 --> 00:50:39.561
Eric Heinz Mark Smith Hummels are friends, David Wr

815
00:50:40.041 --> 00:50:46.155
Ro Romani Charlotte, Bli Nicolo, Barbaro, Adam

816
00:50:46.166 --> 00:50:50.324
hunt Palo Stassi, Nele Bach Guy, Madison,

817
00:50:50.335 --> 00:50:52.545
Gary G Hellman, Samo Zal, Adrian Yi Paul

818
00:50:52.726 --> 00:50:58.144
Tolentino. Julian Price Edward Hall, Eden Bruner Douglas

819
00:50:58.155 --> 00:51:01.105
Fre Franco Bartolo Gabriel Pan Cortez or Lalit Scott Zachary

820
00:51:01.114 --> 00:51:05.295
Fish, Tim Duffy and Smith John Wiesman, Daniel

821
00:51:05.304 --> 00:51:07.476
Friedman, William Buckner, Paul Georgina, L Lo

822
00:51:07.666 --> 00:51:12.978
A Georges, Theo Chris Williams and Peter W David

823
00:51:12.989 --> 00:51:15.610
Williams the Costa Anton Erickson Charles Murray, Alex Shower

824
00:51:15.789 --> 00:51:20.389
, Marie Martinez, Coralie Chevalier, Bangalore atheists,

825
00:51:20.398 --> 00:51:24.478
Larry D Lee Junior, Old Harri Bon Starry Michael

826
00:51:24.489 --> 00:51:28.675
Bailey. Then Sperber, Robert Grass is a Jeff

827
00:51:28.684 --> 00:51:31.954
mcmahon, Jake Zul Barnabas Radix Mark Temple, Thomas

828
00:51:31.965 --> 00:51:36.394
Dubner, Luke Neeson, Chris to Kimberley Johnson,

829
00:51:36.603 --> 00:51:40.273
Benjamin GALT, Jessica Nowicki Linda Brandon, Nicholas Carlson

830
00:51:40.494 --> 00:51:45.394
, Ismael Bensley Man, George Kiss Valentin Steinman,

831
00:51:45.635 --> 00:51:51.090
Per Rowley, Kate Von Goler, Alexander Robert Liam

832
00:51:51.099 --> 00:51:55.820
Donaway br Masoud Ali Mohammadi Perpendicular, Jonas Herner,

833
00:51:57.128 --> 00:52:00.949
Ursula. Good enough, Gregory Hastings David Pins of

834
00:52:00.489 --> 00:52:06.188
Sean Nelson and Mike Levin. A special thanks to

835
00:52:06.199 --> 00:52:08.219
my producers is our web, Jim Frank Luca Stefi

836
00:52:08.579 --> 00:52:13.329
, Tom Ween, Bernard Yugi Cortes Dixon Benedict Muller

837
00:52:13.340 --> 00:52:15.750
Thomas Trumble, Catherine and Patrick Tobin, John Carl

838
00:52:15.760 --> 00:52:19.969
Montenegro, Alick Ortiz and Nick Golden. And to

839
00:52:19.978 --> 00:52:22.389
my executive producers, Matthew Lavender, Sergi, Adriano

840
00:52:22.398 --> 00:52:25.750
Bogdan Knits and Rosie. Thank you for all

